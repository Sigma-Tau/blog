[
  {
    "objectID": "posts/2022/2022-03-16-polars.html",
    "href": "posts/2022/2022-03-16-polars.html",
    "title": "Polars",
    "section": "",
    "text": "Polars es una librería de DataFrames increíblemente rápida implementada en Rust utilizando Arrow Columnar Format de Apache como modelo de memoria.\n\nLazy | eager execution\nMulti-threaded\nSIMD (Single Instruction, Multiple Data)\nQuery optimization\nPowerful expression API\nRust | Python | …\n\nEsta sección tiene como objetivos presentarle Polars a través de ejemplos y comparándolo con otras soluciones.\n\nNota: Si usted no esta familiarizado con la manipulación de datos en Python, se recomienda partir leyendo sobre la librería de Pandas. También, se deja como referencia el curso de Manipulación de Datos.\n\n\n\n\n\n\nPara instalar Polars, necesitará usar la línea de comando. Si ha instalado Anaconda, puede usar:\nconda install -c conda-forge polars\nDe lo contrario, puede instalar con pip:\npip install polars\n\nNota: Todos los binarios están preconstruidos para Python v3.6+.\n\n\n\n\n\nPolars es muy rápido y, de hecho, es una de las mejores soluciones disponibles. Tomemos como referencia db-benchmark de h2oai. Esta página tiene como objetivo comparar varias herramientas similares a bases de datos populares en la ciencia de datos de código abierto. Se ejecuta regularmente con las últimas versiones de estos paquetes y se actualiza automáticamente.\nTambién se incluye la sintaxis que se cronometra junto con el tiempo. De esta manera, puede ver de inmediato si está realizando estas tareas o no, y si las diferencias de tiempo le importan o no. Una diferencia de 10x puede ser irrelevante si eso es solo 1s frente a 0,1s en el tamaño de sus datos.\nA modo de ejemplo, veamos algunos ejemplos de performances de distintas librerías para ejecutar distintos tipos de tareas sobre datasets con distintos tamaños. Para el caso de tareas básicas sobre un dataset de 50 GB, Polars supera a librerías espacializadas en distribución de Dataframes como Spark (143 segundos vs 568 segundos). Por otro lado, librerías conocidas en Python como Pandas o Dask se tiene el problema de out of memory.\n\n\n\n\nPolars tiene un poderoso concepto llamado expresiones. Las expresiones polares se pueden usar en varios contextos y son un mapeo funcional de Fn(Series) -&gt; Series, lo que significa que tienen Series como entrada y Series como salida. Al observar esta definición funcional, podemos ver que la salida de un Expr también puede servir como entrada de un Expr.\nEso puede sonar un poco extraño, así que vamos a dar un ejemplo.\nLa siguiente es una expresión:\npl.col(\"foo\").sort().head(2)\nEl fragmento anterior dice seleccionar la columna \"foo\", luego ordenar esta columna y luego tomar los primeros 2 valores de la salida ordenada. El poder de las expresiones es que cada expresión produce una nueva expresión y que se pueden canalizar juntas. Puede ejecutar una expresión pasándola en uno de los contextos de ejecución polares. Aquí ejecutamos dos expresiones ejecutando df.select:\ndf.select([\n     pl.col(\"foo\").sort().head(2),\n     pl.col(\"barra\").filter(pl.col(\"foo\") == 1).sum()\n])\nTodas las expresiones se ejecutan en paralelo. (Tenga en cuenta que dentro de una expresión puede haber más paralelización).\n\n\nEn esta sección veremos algunos ejemplos, pero primero vamos a crear un conjunto de datos:\n\nimport polars as pl\nimport numpy as np\n\n\nnp.random.seed(12)\n\ndf = pl.DataFrame(\n    {\n        \"nrs\": [1, 2, 3, None, 5],\n        \"names\": [\"foo\", \"ham\", \"spam\", \"egg\", None],\n        \"random\": np.random.rand(5),\n        \"groups\": [\"A\", \"A\", \"B\", \"C\", \"B\"],\n    }\n)\nprint(df)\n\nshape: (5, 4)\n┌──────┬───────┬──────────┬────────┐\n│ nrs  ┆ names ┆ random   ┆ groups │\n│ ---  ┆ ---   ┆ ---      ┆ ---    │\n│ i64  ┆ str   ┆ f64      ┆ str    │\n╞══════╪═══════╪══════════╪════════╡\n│ 1    ┆ foo   ┆ 0.154163 ┆ A      │\n├╌╌╌╌╌╌┼╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌┤\n│ 2    ┆ ham   ┆ 0.74     ┆ A      │\n├╌╌╌╌╌╌┼╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌┤\n│ 3    ┆ spam  ┆ 0.263315 ┆ B      │\n├╌╌╌╌╌╌┼╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌┤\n│ null ┆ egg   ┆ 0.533739 ┆ C      │\n├╌╌╌╌╌╌┼╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌┤\n│ 5    ┆ null  ┆ 0.014575 ┆ B      │\n└──────┴───────┴──────────┴────────┘\n\n\nPuedes hacer mucho con las expresiones, veamos algunos ejemplos:\n\n\nPodemos contar los valores únicos en una columna. Tenga en cuenta que estamos creando el mismo resultado de diferentes maneras. Para no tener nombres de columna duplicados en el DataFrame, usamos una expresión de alias, que cambia el nombre de una expresión.\n\nout = df.select(\n    [\n        pl.col(\"names\").n_unique().alias(\"unique_names_1\"),\n        pl.col(\"names\").unique().count().alias(\"unique_names_2\"),\n    ]\n)\nprint(out)\n\nshape: (1, 2)\n┌────────────────┬────────────────┐\n│ unique_names_1 ┆ unique_names_2 │\n│ ---            ┆ ---            │\n│ u32            ┆ u32            │\n╞════════════════╪════════════════╡\n│ 5              ┆ 5              │\n└────────────────┴────────────────┘\n\n\n\n\n\n\nPodemos hacer varias agregaciones. A continuación mostramos algunas de ellas, pero hay más, como median, mean, first, etc.\n\nout = df.select(\n    [\n        pl.sum(\"random\").alias(\"sum\"),\n        pl.min(\"random\").alias(\"min\"),\n        pl.max(\"random\").alias(\"max\"),\n        pl.col(\"random\").max().alias(\"other_max\"),\n        pl.std(\"random\").alias(\"std dev\"),\n        pl.var(\"random\").alias(\"variance\"),\n    ]\n)\nprint(out)\n\nshape: (1, 6)\n┌──────────┬──────────┬──────┬───────────┬──────────┬──────────┐\n│ sum      ┆ min      ┆ max  ┆ other_max ┆ std dev  ┆ variance │\n│ ---      ┆ ---      ┆ ---  ┆ ---       ┆ ---      ┆ ---      │\n│ f64      ┆ f64      ┆ f64  ┆ f64       ┆ f64      ┆ f64      │\n╞══════════╪══════════╪══════╪═══════════╪══════════╪══════════╡\n│ 1.705842 ┆ 0.014575 ┆ 0.74 ┆ 0.74      ┆ 0.293209 ┆ 0.085971 │\n└──────────┴──────────┴──────┴───────────┴──────────┴──────────┘\n\n\n\n\n\nTambién podemos hacer cosas bastante complejas. En el siguiente fragmento, contamos todos los nombres que terminan con la cadena \"am\".\n\nout = df.select(\n    [\n        pl.col(\"names\").filter(pl.col(\"names\").str.contains(r\"am$\")).count(),\n    ]\n)\nprint(out)\n\nshape: (1, 1)\n┌───────┐\n│ names │\n│ ---   │\n│ u32   │\n╞═══════╡\n│ 2     │\n└───────┘\n\n\n\n\n\nEn el ejemplo a continuación, usamos un condicional para crear una nueva expresión when -&gt; then -&gt; otherwise.\nLa función when() requiere una expresión de predicado (y, por lo tanto, conduce a una serie booleana), luego espera una expresión que se usará en caso de que el predicado se evalúe como verdadero y, de lo contrario, espera una expresión que se usará en caso de que el predicado se evalúe.\nTenga en cuenta que puede pasar cualquier expresión, o simplemente expresiones base como pl.col(\"foo\"), pl.lit(3), pl.lit(\"bar\"), etc.\nFinalmente, multiplicamos esto con el resultado de una expresión de suma.\n\nout = df.select(\n    [\n        pl.when(pl.col(\"random\") &gt; 0.5).then(0).otherwise(pl.col(\"random\")) * pl.sum(\"nrs\"),\n    ]\n)\nprint(out)\n\nshape: (5, 1)\n┌──────────┐\n│ literal  │\n│ ---      │\n│ f64      │\n╞══════════╡\n│ 1.695791 │\n├╌╌╌╌╌╌╌╌╌╌┤\n│ 0.0      │\n├╌╌╌╌╌╌╌╌╌╌┤\n│ 2.896465 │\n├╌╌╌╌╌╌╌╌╌╌┤\n│ 0.0      │\n├╌╌╌╌╌╌╌╌╌╌┤\n│ 0.160325 │\n└──────────┘\n\n\n\n\n\nUna expresión polar también puede hacer un GROUPBY, AGGREGATION y JOIN implícitos en una sola expresión.\nEn los ejemplos a continuación, hacemos un GROUPBY sobre \"groups\" y AGREGATE SUM de \"random\", y en la siguiente expresión GROUPBY OVER \"names\" y AGREGATE una lista de \"random\". Estas funciones de ventana se pueden combinar con otras expresiones y son una forma eficaz de determinar estadísticas de grupo. Vea más expresiones en el siguiente link.\n\nout = df[\n    [\n        pl.col(\"*\"),  # select all\n        pl.col(\"random\").sum().over(\"groups\").alias(\"sum[random]/groups\"),\n        pl.col(\"random\").list().over(\"names\").alias(\"random/name\"),\n    ]\n]\nprint(out)\n\nshape: (5, 6)\n┌──────┬───────┬──────────┬────────┬────────────────────┬─────────────┐\n│ nrs  ┆ names ┆ random   ┆ groups ┆ sum[random]/groups ┆ random/name │\n│ ---  ┆ ---   ┆ ---      ┆ ---    ┆ ---                ┆ ---         │\n│ i64  ┆ str   ┆ f64      ┆ str    ┆ f64                ┆ list [f64]  │\n╞══════╪═══════╪══════════╪════════╪════════════════════╪═════════════╡\n│ 1    ┆ foo   ┆ 0.154163 ┆ A      ┆ 0.894213           ┆ [0.154163]  │\n├╌╌╌╌╌╌┼╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌┤\n│ 2    ┆ ham   ┆ 0.74     ┆ A      ┆ 0.894213           ┆ [0.74]      │\n├╌╌╌╌╌╌┼╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌┤\n│ 3    ┆ spam  ┆ 0.263315 ┆ B      ┆ 0.2778             ┆ [0.263315]  │\n├╌╌╌╌╌╌┼╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌┤\n│ null ┆ egg   ┆ 0.533739 ┆ C      ┆ 0.533739           ┆ [0.533739]  │\n├╌╌╌╌╌╌┼╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌┤\n│ 5    ┆ null  ┆ 0.014575 ┆ B      ┆ 0.2778             ┆ [0.014575]  │\n└──────┴───────┴──────────┴────────┴────────────────────┴─────────────┘\n\n\n\n\n\n\n\n\nUna de las formas más eficientes de procesar datos tabulares es paralelizar su procesamiento a través del enfoque “dividir-aplicar-combinar”. Esta operación es el núcleo de la implementación del agrupamiento de Polars, lo que le permite lograr operaciones ultrarrápidas. Más específicamente, las fases de “división” y “aplicación” se ejecutan de forma multiproceso.\nUna operación de agrupación simple se toma a continuación como ejemplo para ilustrar este enfoque:\n\nPara las operaciones hash realizadas durante la fase de “división”, Polars utiliza un enfoque sin bloqueo de subprocesos múltiples que se ilustra en el siguiente esquema:\n\n¡Esta paralelización permite que las operaciones de agrupación y unión (por ejemplo) sean increíblemente rápidas!\n\n\n\nTodos hemos escuchado que Python es lento y “no escala”. Además de la sobrecarga de ejecutar el código de bytes “lento”, Python debe permanecer dentro de las restricciones del Global interpreter lock (GIL). Esto significa que si se usa la operación lambda o una función de Python personalizada para aplicar durante una fase de paralelización, la velocidad de Polars se limita al ejecutar el código de Python, lo que evita que varios subprocesos ejecuten la función.\nTodo esto se siente terriblemente limitante, especialmente porque a menudo necesitamos esos lambda en un paso .groupby(), por ejemplo. Este enfoque aún es compatible con Polars, pero teniendo en cuenta el código de bytes Y el precio GIL deben pagarse.\nPara mitigar esto, Polars implementa una poderosa sintaxis definida no solo en su lazy, sino también en su uso eager.\n\n\n\nEn la introducción de la página anterior, discutimos que el uso de funciones personalizadas de Python eliminaba la paralelización y que podemos usar las expresiones de la API diferida para mitigar esto. Echemos un vistazo a lo que eso significa.\nComencemos con el conjunto de datos simple del congreso de EE. UU.\n\nimport polars as pl\n\ndataset = pl.read_csv(\"legislators-current.csv\")\ndataset = dataset.with_column(pl.col(\"birthday\").str.strptime(pl.Date))\nprint(dataset.head())\n\nshape: (5, 34)\n┌───────────┬────────────┬─────────────┬────────┬─────┬────────────────┬────────────────────┬──────────┬────────────────┐\n│ last_name ┆ first_name ┆ middle_name ┆ suffix ┆ ... ┆ ballotpedia_id ┆ washington_post_id ┆ icpsr_id ┆ wikipedia_id   │\n│ ---       ┆ ---        ┆ ---         ┆ ---    ┆     ┆ ---            ┆ ---                ┆ ---      ┆ ---            │\n│ str       ┆ str        ┆ str         ┆ str    ┆     ┆ str            ┆ str                ┆ i64      ┆ str            │\n╞═══════════╪════════════╪═════════════╪════════╪═════╪════════════════╪════════════════════╪══════════╪════════════════╡\n│ Brown     ┆ Sherrod    ┆ null        ┆ null   ┆ ... ┆ Sherrod Brown  ┆ null               ┆ 29389    ┆ Sherrod Brown  │\n├╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌┼╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤\n│ Cantwell  ┆ Maria      ┆ null        ┆ null   ┆ ... ┆ Maria Cantwell ┆ null               ┆ 39310    ┆ Maria Cantwell │\n├╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌┼╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤\n│ Cardin    ┆ Benjamin   ┆ L.          ┆ null   ┆ ... ┆ Ben Cardin     ┆ null               ┆ 15408    ┆ Ben Cardin     │\n├╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌┼╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤\n│ Carper    ┆ Thomas     ┆ Richard     ┆ null   ┆ ... ┆ Tom Carper     ┆ null               ┆ 15015    ┆ Tom Carper     │\n├╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌┼╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤\n│ Casey     ┆ Robert     ┆ P.          ┆ Jr.    ┆ ... ┆ Bob Casey, Jr. ┆ null               ┆ 40703    ┆ Bob Casey Jr.  │\n└───────────┴────────────┴─────────────┴────────┴─────┴────────────────┴────────────────────┴──────────┴────────────────┘\n\n\n\n\n\nPuede combinar fácilmente diferentes agregaciones agregando varias expresiones en una lista. No hay un límite superior en el número de agregaciones que puede hacer y puede hacer cualquier combinación que desee. En el fragmento a continuación, hacemos las siguientes agregaciones:\nPor grupo \"first_name\":\n\ncuente el número de filas en el grupo:\n\nforma abreviada: pl.count(\"party\")\nforma completa: pl.col(\"party\").count()\n\nagregue el grupo de valores de género a una lista:\n\nforma completa: pl.col(\"gender\").list()\n\nobtenga el primer valor de la columna \"last_name\" en el grupo:\n\nforma abreviada: pl.primero(\"last_name\")\nforma completa: pl.col(\"last_name\").first()\n\n\nAdemás de la agregación, clasificamos inmediatamente el resultado y lo limitamos a los 5 principales para que tengamos un buen resumen general.\n\nq = (\n    dataset.lazy()\n    .groupby(\"first_name\")\n    .agg(\n        [\n            pl.count(),\n            pl.col(\"gender\").list(),\n            pl.first(\"last_name\"),\n        ]\n    )\n    .sort(\"count\", reverse=True)\n    .limit(5)\n)\n\ndf = q.collect()\nprint(df)\n\nshape: (5, 4)\n┌────────────┬───────┬─────────────────────┬───────────┐\n│ first_name ┆ count ┆ gender              ┆ last_name │\n│ ---        ┆ ---   ┆ ---                 ┆ ---       │\n│ str        ┆ u32   ┆ list [str]          ┆ str       │\n╞════════════╪═══════╪═════════════════════╪═══════════╡\n│ John       ┆ 19    ┆ [\"M\", \"M\", ... \"M\"] ┆ Barrasso  │\n├╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌┤\n│ Mike       ┆ 13    ┆ [\"M\", \"M\", ... \"M\"] ┆ Kelly     │\n├╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌┤\n│ Michael    ┆ 11    ┆ [\"M\", \"M\", ... \"M\"] ┆ Bennet    │\n├╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌┤\n│ David      ┆ 11    ┆ [\"M\", \"M\", ... \"M\"] ┆ Cicilline │\n├╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌┤\n│ James      ┆ 9     ┆ [\"M\", \"M\", ... \"M\"] ┆ Inhofe    │\n└────────────┴───────┴─────────────────────┴───────────┘\n\n\n\n\n\nOk, eso fue bastante fácil, ¿verdad? Subamos un nivel. Digamos que queremos saber cuántos delegados de un “estado” (state) son administración “Democrat” o “Republican”. Podríamos consultarlo directamente en la agregación sin la necesidad de lambda o arreglar el DataFrame.\n\nq = (\n    dataset.lazy()\n    .groupby(\"state\")\n    .agg(\n        [\n            (pl.col(\"party\") == \"Democrat\").sum().alias(\"demo\"),\n            (pl.col(\"party\") == \"Republican\").sum().alias(\"repu\"),\n        ]\n    )\n    .sort(\"demo\", reverse=True)\n    .limit(5)\n)\n\ndf = q.collect()\nprint(df)\n\nshape: (5, 3)\n┌───────┬──────┬──────┐\n│ state ┆ demo ┆ repu │\n│ ---   ┆ ---  ┆ ---  │\n│ str   ┆ u32  ┆ u32  │\n╞═══════╪══════╪══════╡\n│ CA    ┆ 44   ┆ 10   │\n├╌╌╌╌╌╌╌┼╌╌╌╌╌╌┼╌╌╌╌╌╌┤\n│ NY    ┆ 21   ┆ 8    │\n├╌╌╌╌╌╌╌┼╌╌╌╌╌╌┼╌╌╌╌╌╌┤\n│ IL    ┆ 15   ┆ 5    │\n├╌╌╌╌╌╌╌┼╌╌╌╌╌╌┼╌╌╌╌╌╌┤\n│ TX    ┆ 13   ┆ 25   │\n├╌╌╌╌╌╌╌┼╌╌╌╌╌╌┼╌╌╌╌╌╌┤\n│ NJ    ┆ 12   ┆ 2    │\n└───────┴──────┴──────┘\n\n\nPor supuesto, también se podría hacer algo similar con un GROUPBY anidado, pero eso no me permitiría mostrar estas características agradables. 😉\n\nq = (\n    dataset.lazy()\n    .groupby([\"state\", \"party\"])\n    .agg([pl.count(\"party\").alias(\"count\")])\n    .filter((pl.col(\"party\") == \"Democrat\") | (pl.col(\"party\") == \"Republican\"))\n    .sort(\"count\", reverse=True)\n    .limit(5)\n)\n\ndf = q.collect()\nprint(df)\n\nshape: (5, 3)\n┌───────┬────────────┬───────┐\n│ state ┆ party      ┆ count │\n│ ---   ┆ ---        ┆ ---   │\n│ str   ┆ str        ┆ u32   │\n╞═══════╪════════════╪═══════╡\n│ CA    ┆ Democrat   ┆ 44    │\n├╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┤\n│ TX    ┆ Republican ┆ 25    │\n├╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┤\n│ NY    ┆ Democrat   ┆ 21    │\n├╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┤\n│ FL    ┆ Republican ┆ 18    │\n├╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┤\n│ IL    ┆ Democrat   ┆ 15    │\n└───────┴────────────┴───────┘\n\n\n\n\n\nTambién podemos filtrar los grupos. Digamos que queremos calcular una media por grupo, pero no queremos incluir todos los valores de ese grupo y tampoco queremos filtrar las filas del DataFrame (porque necesitamos esas filas para otra agregación).\nEn el siguiente ejemplo, mostramos cómo se puede hacer eso. Tenga en cuenta que podemos hacer funciones de Python para mayor claridad. Estas funciones no nos cuestan nada. Esto se debe a que solo creamos Polars expression, no aplicamos una función personalizada sobre Series durante el tiempo de ejecución de la consulta.\n\nfrom datetime import date\n\ndef compute_age() -&gt; pl.Expr:\n    return date(2021, 1, 1).year - pl.col(\"birthday\").dt.year()\n\n\ndef avg_birthday(gender: str) -&gt; pl.Expr:\n    return compute_age().filter(pl.col(\"gender\") == gender).mean().alias(f\"avg {gender} birthday\")\n\n\nq = (\n    dataset.lazy()\n    .groupby([\"state\"])\n    .agg(\n        [\n            avg_birthday(\"M\"),\n            avg_birthday(\"F\"),\n            (pl.col(\"gender\") == \"M\").sum().alias(\"# male\"),\n            (pl.col(\"gender\") == \"F\").sum().alias(\"# female\"),\n        ]\n    )\n    .limit(5)\n)\n\ndf = q.collect()\nprint(df)\n\nshape: (5, 5)\n┌───────┬────────────────┬────────────────┬────────┬──────────┐\n│ state ┆ avg M birthday ┆ avg F birthday ┆ # male ┆ # female │\n│ ---   ┆ ---            ┆ ---            ┆ ---    ┆ ---      │\n│ str   ┆ f64            ┆ f64            ┆ u32    ┆ u32      │\n╞═══════╪════════════════╪════════════════╪════════╪══════════╡\n│ MS    ┆ 60.0           ┆ 62.0           ┆ 5      ┆ 1        │\n├╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┤\n│ NV    ┆ 55.5           ┆ 61.75          ┆ 2      ┆ 4        │\n├╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┤\n│ KS    ┆ 54.2           ┆ 41.0           ┆ 5      ┆ 1        │\n├╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┤\n│ IN    ┆ 55.0           ┆ 50.5           ┆ 9      ┆ 2        │\n├╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┤\n│ IL    ┆ 60.923077      ┆ 58.428571      ┆ 13     ┆ 7        │\n└───────┴────────────────┴────────────────┴────────┴──────────┘\n\n\n\n\n\nA menudo veo que se ordena un DataFrame con el único propósito de ordenar durante la operación GROUPBY. Digamos que queremos obtener los nombres de los políticos más antiguos y más jóvenes (no es que todavía estén vivos) por estado, podríamos ORDENAR y AGRUPAR.\n\ndef get_person() -&gt; pl.Expr:\n    return pl.col(\"first_name\") + pl.lit(\" \") + pl.col(\"last_name\")\n\n\nq = (\n    dataset.lazy()\n    .sort(\"birthday\")\n    .groupby([\"state\"])\n    .agg(\n        [\n            get_person().first().alias(\"youngest\"),\n            get_person().last().alias(\"oldest\"),\n        ]\n    )\n    .limit(5)\n)\n\ndf = q.collect()\n\nprint(df)\n\nshape: (5, 3)\n┌───────┬──────────────────────────┬──────────────────────────┐\n│ state ┆ youngest                 ┆ oldest                   │\n│ ---   ┆ ---                      ┆ ---                      │\n│ str   ┆ str                      ┆ str                      │\n╞═══════╪══════════════════════════╪══════════════════════════╡\n│ PR    ┆ Jenniffer González-Colón ┆ Jenniffer González-Colón │\n├╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤\n│ ND    ┆ John Hoeven              ┆ Kelly Armstrong          │\n├╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤\n│ KY    ┆ Harold Rogers            ┆ Garland Barr             │\n├╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤\n│ NM    ┆ Teresa Leger Fernandez   ┆ Melanie Stansbury        │\n├╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤\n│ OR    ┆ Peter DeFazio            ┆ Jeff Merkley             │\n└───────┴──────────────────────────┴──────────────────────────┘\n\n\n\n\n\n\n\nPolars - User Guide\nPolars - Github"
  },
  {
    "objectID": "posts/2022/2022-03-16-polars.html#introducción",
    "href": "posts/2022/2022-03-16-polars.html#introducción",
    "title": "Polars",
    "section": "",
    "text": "Polars es una librería de DataFrames increíblemente rápida implementada en Rust utilizando Arrow Columnar Format de Apache como modelo de memoria.\n\nLazy | eager execution\nMulti-threaded\nSIMD (Single Instruction, Multiple Data)\nQuery optimization\nPowerful expression API\nRust | Python | …\n\nEsta sección tiene como objetivos presentarle Polars a través de ejemplos y comparándolo con otras soluciones.\n\nNota: Si usted no esta familiarizado con la manipulación de datos en Python, se recomienda partir leyendo sobre la librería de Pandas. También, se deja como referencia el curso de Manipulación de Datos."
  },
  {
    "objectID": "posts/2022/2022-03-16-polars.html#primeros-pasos",
    "href": "posts/2022/2022-03-16-polars.html#primeros-pasos",
    "title": "Polars",
    "section": "",
    "text": "Para instalar Polars, necesitará usar la línea de comando. Si ha instalado Anaconda, puede usar:\nconda install -c conda-forge polars\nDe lo contrario, puede instalar con pip:\npip install polars\n\nNota: Todos los binarios están preconstruidos para Python v3.6+."
  },
  {
    "objectID": "posts/2022/2022-03-16-polars.html#rendimiento",
    "href": "posts/2022/2022-03-16-polars.html#rendimiento",
    "title": "Polars",
    "section": "",
    "text": "Polars es muy rápido y, de hecho, es una de las mejores soluciones disponibles. Tomemos como referencia db-benchmark de h2oai. Esta página tiene como objetivo comparar varias herramientas similares a bases de datos populares en la ciencia de datos de código abierto. Se ejecuta regularmente con las últimas versiones de estos paquetes y se actualiza automáticamente.\nTambién se incluye la sintaxis que se cronometra junto con el tiempo. De esta manera, puede ver de inmediato si está realizando estas tareas o no, y si las diferencias de tiempo le importan o no. Una diferencia de 10x puede ser irrelevante si eso es solo 1s frente a 0,1s en el tamaño de sus datos.\nA modo de ejemplo, veamos algunos ejemplos de performances de distintas librerías para ejecutar distintos tipos de tareas sobre datasets con distintos tamaños. Para el caso de tareas básicas sobre un dataset de 50 GB, Polars supera a librerías espacializadas en distribución de Dataframes como Spark (143 segundos vs 568 segundos). Por otro lado, librerías conocidas en Python como Pandas o Dask se tiene el problema de out of memory."
  },
  {
    "objectID": "posts/2022/2022-03-16-polars.html#expresiones-en-polars",
    "href": "posts/2022/2022-03-16-polars.html#expresiones-en-polars",
    "title": "Polars",
    "section": "",
    "text": "Polars tiene un poderoso concepto llamado expresiones. Las expresiones polares se pueden usar en varios contextos y son un mapeo funcional de Fn(Series) -&gt; Series, lo que significa que tienen Series como entrada y Series como salida. Al observar esta definición funcional, podemos ver que la salida de un Expr también puede servir como entrada de un Expr.\nEso puede sonar un poco extraño, así que vamos a dar un ejemplo.\nLa siguiente es una expresión:\npl.col(\"foo\").sort().head(2)\nEl fragmento anterior dice seleccionar la columna \"foo\", luego ordenar esta columna y luego tomar los primeros 2 valores de la salida ordenada. El poder de las expresiones es que cada expresión produce una nueva expresión y que se pueden canalizar juntas. Puede ejecutar una expresión pasándola en uno de los contextos de ejecución polares. Aquí ejecutamos dos expresiones ejecutando df.select:\ndf.select([\n     pl.col(\"foo\").sort().head(2),\n     pl.col(\"barra\").filter(pl.col(\"foo\") == 1).sum()\n])\nTodas las expresiones se ejecutan en paralelo. (Tenga en cuenta que dentro de una expresión puede haber más paralelización).\n\n\nEn esta sección veremos algunos ejemplos, pero primero vamos a crear un conjunto de datos:\n\nimport polars as pl\nimport numpy as np\n\n\nnp.random.seed(12)\n\ndf = pl.DataFrame(\n    {\n        \"nrs\": [1, 2, 3, None, 5],\n        \"names\": [\"foo\", \"ham\", \"spam\", \"egg\", None],\n        \"random\": np.random.rand(5),\n        \"groups\": [\"A\", \"A\", \"B\", \"C\", \"B\"],\n    }\n)\nprint(df)\n\nshape: (5, 4)\n┌──────┬───────┬──────────┬────────┐\n│ nrs  ┆ names ┆ random   ┆ groups │\n│ ---  ┆ ---   ┆ ---      ┆ ---    │\n│ i64  ┆ str   ┆ f64      ┆ str    │\n╞══════╪═══════╪══════════╪════════╡\n│ 1    ┆ foo   ┆ 0.154163 ┆ A      │\n├╌╌╌╌╌╌┼╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌┤\n│ 2    ┆ ham   ┆ 0.74     ┆ A      │\n├╌╌╌╌╌╌┼╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌┤\n│ 3    ┆ spam  ┆ 0.263315 ┆ B      │\n├╌╌╌╌╌╌┼╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌┤\n│ null ┆ egg   ┆ 0.533739 ┆ C      │\n├╌╌╌╌╌╌┼╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌┤\n│ 5    ┆ null  ┆ 0.014575 ┆ B      │\n└──────┴───────┴──────────┴────────┘\n\n\nPuedes hacer mucho con las expresiones, veamos algunos ejemplos:\n\n\nPodemos contar los valores únicos en una columna. Tenga en cuenta que estamos creando el mismo resultado de diferentes maneras. Para no tener nombres de columna duplicados en el DataFrame, usamos una expresión de alias, que cambia el nombre de una expresión.\n\nout = df.select(\n    [\n        pl.col(\"names\").n_unique().alias(\"unique_names_1\"),\n        pl.col(\"names\").unique().count().alias(\"unique_names_2\"),\n    ]\n)\nprint(out)\n\nshape: (1, 2)\n┌────────────────┬────────────────┐\n│ unique_names_1 ┆ unique_names_2 │\n│ ---            ┆ ---            │\n│ u32            ┆ u32            │\n╞════════════════╪════════════════╡\n│ 5              ┆ 5              │\n└────────────────┴────────────────┘\n\n\n\n\n\n\nPodemos hacer varias agregaciones. A continuación mostramos algunas de ellas, pero hay más, como median, mean, first, etc.\n\nout = df.select(\n    [\n        pl.sum(\"random\").alias(\"sum\"),\n        pl.min(\"random\").alias(\"min\"),\n        pl.max(\"random\").alias(\"max\"),\n        pl.col(\"random\").max().alias(\"other_max\"),\n        pl.std(\"random\").alias(\"std dev\"),\n        pl.var(\"random\").alias(\"variance\"),\n    ]\n)\nprint(out)\n\nshape: (1, 6)\n┌──────────┬──────────┬──────┬───────────┬──────────┬──────────┐\n│ sum      ┆ min      ┆ max  ┆ other_max ┆ std dev  ┆ variance │\n│ ---      ┆ ---      ┆ ---  ┆ ---       ┆ ---      ┆ ---      │\n│ f64      ┆ f64      ┆ f64  ┆ f64       ┆ f64      ┆ f64      │\n╞══════════╪══════════╪══════╪═══════════╪══════════╪══════════╡\n│ 1.705842 ┆ 0.014575 ┆ 0.74 ┆ 0.74      ┆ 0.293209 ┆ 0.085971 │\n└──────────┴──────────┴──────┴───────────┴──────────┴──────────┘\n\n\n\n\n\nTambién podemos hacer cosas bastante complejas. En el siguiente fragmento, contamos todos los nombres que terminan con la cadena \"am\".\n\nout = df.select(\n    [\n        pl.col(\"names\").filter(pl.col(\"names\").str.contains(r\"am$\")).count(),\n    ]\n)\nprint(out)\n\nshape: (1, 1)\n┌───────┐\n│ names │\n│ ---   │\n│ u32   │\n╞═══════╡\n│ 2     │\n└───────┘\n\n\n\n\n\nEn el ejemplo a continuación, usamos un condicional para crear una nueva expresión when -&gt; then -&gt; otherwise.\nLa función when() requiere una expresión de predicado (y, por lo tanto, conduce a una serie booleana), luego espera una expresión que se usará en caso de que el predicado se evalúe como verdadero y, de lo contrario, espera una expresión que se usará en caso de que el predicado se evalúe.\nTenga en cuenta que puede pasar cualquier expresión, o simplemente expresiones base como pl.col(\"foo\"), pl.lit(3), pl.lit(\"bar\"), etc.\nFinalmente, multiplicamos esto con el resultado de una expresión de suma.\n\nout = df.select(\n    [\n        pl.when(pl.col(\"random\") &gt; 0.5).then(0).otherwise(pl.col(\"random\")) * pl.sum(\"nrs\"),\n    ]\n)\nprint(out)\n\nshape: (5, 1)\n┌──────────┐\n│ literal  │\n│ ---      │\n│ f64      │\n╞══════════╡\n│ 1.695791 │\n├╌╌╌╌╌╌╌╌╌╌┤\n│ 0.0      │\n├╌╌╌╌╌╌╌╌╌╌┤\n│ 2.896465 │\n├╌╌╌╌╌╌╌╌╌╌┤\n│ 0.0      │\n├╌╌╌╌╌╌╌╌╌╌┤\n│ 0.160325 │\n└──────────┘\n\n\n\n\n\nUna expresión polar también puede hacer un GROUPBY, AGGREGATION y JOIN implícitos en una sola expresión.\nEn los ejemplos a continuación, hacemos un GROUPBY sobre \"groups\" y AGREGATE SUM de \"random\", y en la siguiente expresión GROUPBY OVER \"names\" y AGREGATE una lista de \"random\". Estas funciones de ventana se pueden combinar con otras expresiones y son una forma eficaz de determinar estadísticas de grupo. Vea más expresiones en el siguiente link.\n\nout = df[\n    [\n        pl.col(\"*\"),  # select all\n        pl.col(\"random\").sum().over(\"groups\").alias(\"sum[random]/groups\"),\n        pl.col(\"random\").list().over(\"names\").alias(\"random/name\"),\n    ]\n]\nprint(out)\n\nshape: (5, 6)\n┌──────┬───────┬──────────┬────────┬────────────────────┬─────────────┐\n│ nrs  ┆ names ┆ random   ┆ groups ┆ sum[random]/groups ┆ random/name │\n│ ---  ┆ ---   ┆ ---      ┆ ---    ┆ ---                ┆ ---         │\n│ i64  ┆ str   ┆ f64      ┆ str    ┆ f64                ┆ list [f64]  │\n╞══════╪═══════╪══════════╪════════╪════════════════════╪═════════════╡\n│ 1    ┆ foo   ┆ 0.154163 ┆ A      ┆ 0.894213           ┆ [0.154163]  │\n├╌╌╌╌╌╌┼╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌┤\n│ 2    ┆ ham   ┆ 0.74     ┆ A      ┆ 0.894213           ┆ [0.74]      │\n├╌╌╌╌╌╌┼╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌┤\n│ 3    ┆ spam  ┆ 0.263315 ┆ B      ┆ 0.2778             ┆ [0.263315]  │\n├╌╌╌╌╌╌┼╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌┤\n│ null ┆ egg   ┆ 0.533739 ┆ C      ┆ 0.533739           ┆ [0.533739]  │\n├╌╌╌╌╌╌┼╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌┤\n│ 5    ┆ null  ┆ 0.014575 ┆ B      ┆ 0.2778             ┆ [0.014575]  │\n└──────┴───────┴──────────┴────────┴────────────────────┴─────────────┘"
  },
  {
    "objectID": "posts/2022/2022-03-16-polars.html#groupby",
    "href": "posts/2022/2022-03-16-polars.html#groupby",
    "title": "Polars",
    "section": "",
    "text": "Una de las formas más eficientes de procesar datos tabulares es paralelizar su procesamiento a través del enfoque “dividir-aplicar-combinar”. Esta operación es el núcleo de la implementación del agrupamiento de Polars, lo que le permite lograr operaciones ultrarrápidas. Más específicamente, las fases de “división” y “aplicación” se ejecutan de forma multiproceso.\nUna operación de agrupación simple se toma a continuación como ejemplo para ilustrar este enfoque:\n\nPara las operaciones hash realizadas durante la fase de “división”, Polars utiliza un enfoque sin bloqueo de subprocesos múltiples que se ilustra en el siguiente esquema:\n\n¡Esta paralelización permite que las operaciones de agrupación y unión (por ejemplo) sean increíblemente rápidas!\n\n\n\nTodos hemos escuchado que Python es lento y “no escala”. Además de la sobrecarga de ejecutar el código de bytes “lento”, Python debe permanecer dentro de las restricciones del Global interpreter lock (GIL). Esto significa que si se usa la operación lambda o una función de Python personalizada para aplicar durante una fase de paralelización, la velocidad de Polars se limita al ejecutar el código de Python, lo que evita que varios subprocesos ejecuten la función.\nTodo esto se siente terriblemente limitante, especialmente porque a menudo necesitamos esos lambda en un paso .groupby(), por ejemplo. Este enfoque aún es compatible con Polars, pero teniendo en cuenta el código de bytes Y el precio GIL deben pagarse.\nPara mitigar esto, Polars implementa una poderosa sintaxis definida no solo en su lazy, sino también en su uso eager.\n\n\n\nEn la introducción de la página anterior, discutimos que el uso de funciones personalizadas de Python eliminaba la paralelización y que podemos usar las expresiones de la API diferida para mitigar esto. Echemos un vistazo a lo que eso significa.\nComencemos con el conjunto de datos simple del congreso de EE. UU.\n\nimport polars as pl\n\ndataset = pl.read_csv(\"legislators-current.csv\")\ndataset = dataset.with_column(pl.col(\"birthday\").str.strptime(pl.Date))\nprint(dataset.head())\n\nshape: (5, 34)\n┌───────────┬────────────┬─────────────┬────────┬─────┬────────────────┬────────────────────┬──────────┬────────────────┐\n│ last_name ┆ first_name ┆ middle_name ┆ suffix ┆ ... ┆ ballotpedia_id ┆ washington_post_id ┆ icpsr_id ┆ wikipedia_id   │\n│ ---       ┆ ---        ┆ ---         ┆ ---    ┆     ┆ ---            ┆ ---                ┆ ---      ┆ ---            │\n│ str       ┆ str        ┆ str         ┆ str    ┆     ┆ str            ┆ str                ┆ i64      ┆ str            │\n╞═══════════╪════════════╪═════════════╪════════╪═════╪════════════════╪════════════════════╪══════════╪════════════════╡\n│ Brown     ┆ Sherrod    ┆ null        ┆ null   ┆ ... ┆ Sherrod Brown  ┆ null               ┆ 29389    ┆ Sherrod Brown  │\n├╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌┼╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤\n│ Cantwell  ┆ Maria      ┆ null        ┆ null   ┆ ... ┆ Maria Cantwell ┆ null               ┆ 39310    ┆ Maria Cantwell │\n├╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌┼╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤\n│ Cardin    ┆ Benjamin   ┆ L.          ┆ null   ┆ ... ┆ Ben Cardin     ┆ null               ┆ 15408    ┆ Ben Cardin     │\n├╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌┼╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤\n│ Carper    ┆ Thomas     ┆ Richard     ┆ null   ┆ ... ┆ Tom Carper     ┆ null               ┆ 15015    ┆ Tom Carper     │\n├╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌┼╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤\n│ Casey     ┆ Robert     ┆ P.          ┆ Jr.    ┆ ... ┆ Bob Casey, Jr. ┆ null               ┆ 40703    ┆ Bob Casey Jr.  │\n└───────────┴────────────┴─────────────┴────────┴─────┴────────────────┴────────────────────┴──────────┴────────────────┘\n\n\n\n\n\nPuede combinar fácilmente diferentes agregaciones agregando varias expresiones en una lista. No hay un límite superior en el número de agregaciones que puede hacer y puede hacer cualquier combinación que desee. En el fragmento a continuación, hacemos las siguientes agregaciones:\nPor grupo \"first_name\":\n\ncuente el número de filas en el grupo:\n\nforma abreviada: pl.count(\"party\")\nforma completa: pl.col(\"party\").count()\n\nagregue el grupo de valores de género a una lista:\n\nforma completa: pl.col(\"gender\").list()\n\nobtenga el primer valor de la columna \"last_name\" en el grupo:\n\nforma abreviada: pl.primero(\"last_name\")\nforma completa: pl.col(\"last_name\").first()\n\n\nAdemás de la agregación, clasificamos inmediatamente el resultado y lo limitamos a los 5 principales para que tengamos un buen resumen general.\n\nq = (\n    dataset.lazy()\n    .groupby(\"first_name\")\n    .agg(\n        [\n            pl.count(),\n            pl.col(\"gender\").list(),\n            pl.first(\"last_name\"),\n        ]\n    )\n    .sort(\"count\", reverse=True)\n    .limit(5)\n)\n\ndf = q.collect()\nprint(df)\n\nshape: (5, 4)\n┌────────────┬───────┬─────────────────────┬───────────┐\n│ first_name ┆ count ┆ gender              ┆ last_name │\n│ ---        ┆ ---   ┆ ---                 ┆ ---       │\n│ str        ┆ u32   ┆ list [str]          ┆ str       │\n╞════════════╪═══════╪═════════════════════╪═══════════╡\n│ John       ┆ 19    ┆ [\"M\", \"M\", ... \"M\"] ┆ Barrasso  │\n├╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌┤\n│ Mike       ┆ 13    ┆ [\"M\", \"M\", ... \"M\"] ┆ Kelly     │\n├╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌┤\n│ Michael    ┆ 11    ┆ [\"M\", \"M\", ... \"M\"] ┆ Bennet    │\n├╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌┤\n│ David      ┆ 11    ┆ [\"M\", \"M\", ... \"M\"] ┆ Cicilline │\n├╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌┤\n│ James      ┆ 9     ┆ [\"M\", \"M\", ... \"M\"] ┆ Inhofe    │\n└────────────┴───────┴─────────────────────┴───────────┘\n\n\n\n\n\nOk, eso fue bastante fácil, ¿verdad? Subamos un nivel. Digamos que queremos saber cuántos delegados de un “estado” (state) son administración “Democrat” o “Republican”. Podríamos consultarlo directamente en la agregación sin la necesidad de lambda o arreglar el DataFrame.\n\nq = (\n    dataset.lazy()\n    .groupby(\"state\")\n    .agg(\n        [\n            (pl.col(\"party\") == \"Democrat\").sum().alias(\"demo\"),\n            (pl.col(\"party\") == \"Republican\").sum().alias(\"repu\"),\n        ]\n    )\n    .sort(\"demo\", reverse=True)\n    .limit(5)\n)\n\ndf = q.collect()\nprint(df)\n\nshape: (5, 3)\n┌───────┬──────┬──────┐\n│ state ┆ demo ┆ repu │\n│ ---   ┆ ---  ┆ ---  │\n│ str   ┆ u32  ┆ u32  │\n╞═══════╪══════╪══════╡\n│ CA    ┆ 44   ┆ 10   │\n├╌╌╌╌╌╌╌┼╌╌╌╌╌╌┼╌╌╌╌╌╌┤\n│ NY    ┆ 21   ┆ 8    │\n├╌╌╌╌╌╌╌┼╌╌╌╌╌╌┼╌╌╌╌╌╌┤\n│ IL    ┆ 15   ┆ 5    │\n├╌╌╌╌╌╌╌┼╌╌╌╌╌╌┼╌╌╌╌╌╌┤\n│ TX    ┆ 13   ┆ 25   │\n├╌╌╌╌╌╌╌┼╌╌╌╌╌╌┼╌╌╌╌╌╌┤\n│ NJ    ┆ 12   ┆ 2    │\n└───────┴──────┴──────┘\n\n\nPor supuesto, también se podría hacer algo similar con un GROUPBY anidado, pero eso no me permitiría mostrar estas características agradables. 😉\n\nq = (\n    dataset.lazy()\n    .groupby([\"state\", \"party\"])\n    .agg([pl.count(\"party\").alias(\"count\")])\n    .filter((pl.col(\"party\") == \"Democrat\") | (pl.col(\"party\") == \"Republican\"))\n    .sort(\"count\", reverse=True)\n    .limit(5)\n)\n\ndf = q.collect()\nprint(df)\n\nshape: (5, 3)\n┌───────┬────────────┬───────┐\n│ state ┆ party      ┆ count │\n│ ---   ┆ ---        ┆ ---   │\n│ str   ┆ str        ┆ u32   │\n╞═══════╪════════════╪═══════╡\n│ CA    ┆ Democrat   ┆ 44    │\n├╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┤\n│ TX    ┆ Republican ┆ 25    │\n├╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┤\n│ NY    ┆ Democrat   ┆ 21    │\n├╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┤\n│ FL    ┆ Republican ┆ 18    │\n├╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┤\n│ IL    ┆ Democrat   ┆ 15    │\n└───────┴────────────┴───────┘\n\n\n\n\n\nTambién podemos filtrar los grupos. Digamos que queremos calcular una media por grupo, pero no queremos incluir todos los valores de ese grupo y tampoco queremos filtrar las filas del DataFrame (porque necesitamos esas filas para otra agregación).\nEn el siguiente ejemplo, mostramos cómo se puede hacer eso. Tenga en cuenta que podemos hacer funciones de Python para mayor claridad. Estas funciones no nos cuestan nada. Esto se debe a que solo creamos Polars expression, no aplicamos una función personalizada sobre Series durante el tiempo de ejecución de la consulta.\n\nfrom datetime import date\n\ndef compute_age() -&gt; pl.Expr:\n    return date(2021, 1, 1).year - pl.col(\"birthday\").dt.year()\n\n\ndef avg_birthday(gender: str) -&gt; pl.Expr:\n    return compute_age().filter(pl.col(\"gender\") == gender).mean().alias(f\"avg {gender} birthday\")\n\n\nq = (\n    dataset.lazy()\n    .groupby([\"state\"])\n    .agg(\n        [\n            avg_birthday(\"M\"),\n            avg_birthday(\"F\"),\n            (pl.col(\"gender\") == \"M\").sum().alias(\"# male\"),\n            (pl.col(\"gender\") == \"F\").sum().alias(\"# female\"),\n        ]\n    )\n    .limit(5)\n)\n\ndf = q.collect()\nprint(df)\n\nshape: (5, 5)\n┌───────┬────────────────┬────────────────┬────────┬──────────┐\n│ state ┆ avg M birthday ┆ avg F birthday ┆ # male ┆ # female │\n│ ---   ┆ ---            ┆ ---            ┆ ---    ┆ ---      │\n│ str   ┆ f64            ┆ f64            ┆ u32    ┆ u32      │\n╞═══════╪════════════════╪════════════════╪════════╪══════════╡\n│ MS    ┆ 60.0           ┆ 62.0           ┆ 5      ┆ 1        │\n├╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┤\n│ NV    ┆ 55.5           ┆ 61.75          ┆ 2      ┆ 4        │\n├╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┤\n│ KS    ┆ 54.2           ┆ 41.0           ┆ 5      ┆ 1        │\n├╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┤\n│ IN    ┆ 55.0           ┆ 50.5           ┆ 9      ┆ 2        │\n├╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┤\n│ IL    ┆ 60.923077      ┆ 58.428571      ┆ 13     ┆ 7        │\n└───────┴────────────────┴────────────────┴────────┴──────────┘\n\n\n\n\n\nA menudo veo que se ordena un DataFrame con el único propósito de ordenar durante la operación GROUPBY. Digamos que queremos obtener los nombres de los políticos más antiguos y más jóvenes (no es que todavía estén vivos) por estado, podríamos ORDENAR y AGRUPAR.\n\ndef get_person() -&gt; pl.Expr:\n    return pl.col(\"first_name\") + pl.lit(\" \") + pl.col(\"last_name\")\n\n\nq = (\n    dataset.lazy()\n    .sort(\"birthday\")\n    .groupby([\"state\"])\n    .agg(\n        [\n            get_person().first().alias(\"youngest\"),\n            get_person().last().alias(\"oldest\"),\n        ]\n    )\n    .limit(5)\n)\n\ndf = q.collect()\n\nprint(df)\n\nshape: (5, 3)\n┌───────┬──────────────────────────┬──────────────────────────┐\n│ state ┆ youngest                 ┆ oldest                   │\n│ ---   ┆ ---                      ┆ ---                      │\n│ str   ┆ str                      ┆ str                      │\n╞═══════╪══════════════════════════╪══════════════════════════╡\n│ PR    ┆ Jenniffer González-Colón ┆ Jenniffer González-Colón │\n├╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤\n│ ND    ┆ John Hoeven              ┆ Kelly Armstrong          │\n├╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤\n│ KY    ┆ Harold Rogers            ┆ Garland Barr             │\n├╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤\n│ NM    ┆ Teresa Leger Fernandez   ┆ Melanie Stansbury        │\n├╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤\n│ OR    ┆ Peter DeFazio            ┆ Jeff Merkley             │\n└───────┴──────────────────────────┴──────────────────────────┘"
  },
  {
    "objectID": "posts/2022/2022-03-16-polars.html#referencias",
    "href": "posts/2022/2022-03-16-polars.html#referencias",
    "title": "Polars",
    "section": "",
    "text": "Polars - User Guide\nPolars - Github"
  },
  {
    "objectID": "posts/2022/2022-10-12-causal_impact.html",
    "href": "posts/2022/2022-10-12-causal_impact.html",
    "title": "Causal Impact",
    "section": "",
    "text": "El paquete CausalImpact creado por Google estima el impacto de una intervención en una serie temporal. Por ejemplo, ¿cómo afecta una nueva función en una aplicación el tiempo de los usuarios en la aplicación?\nEn este tutorial, hablaremos sobre cómo usar el paquete de Python CausalImpact para hacer inferencias causales de series de tiempo. Aprenderás: * ¿Cómo establecer los períodos previo y posterior para el análisis de impacto causal? * ¿Cómo realizar inferencias causales sobre datos de series temporales? * ¿Cómo resumir los resultados del análisis de causalidad y crear un informe? * ¿Cuáles son las diferencias entre los paquetes python y R para CausalImpact?\n\n\n\nEn primer lugar, instalemos pycausalimpac para el análisis causal de series de tiempo.\n\n# Install python version of causal impact\n#!pip install pycausalimpact\n\nUna vez completada la instalación, podemos importar las bibliotecas. * pandas, numpy y datetime se importan para el procesamiento de datos. * ArmaProcess se importa para la creación de datos de series temporales sintéticas. * matplotlib y seaborn son para visualización. * CausalImpact es para la estimación de los efectos del tratamiento de series de tiempo.\n\n# Data processing\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime\n\n# Create synthetic time-series data\nfrom statsmodels.tsa.arima_process import ArmaProcess\n\n# Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Causal impact\nfrom causalimpact import CausalImpact\n\n\n\n\nCrearemos un conjunto de datos de series de tiempo sintético para el análisis de impacto causal. El beneficio de usar un conjunto de datos sintético es que podemos validar la precisión de los resultados del modelo.\nEl paquete CausalImpact requiere dos tipos de series temporales: * Una serie temporal de respuesta que se ve directamente afectada por la intervención. * Y una o más series temporales de control que no se ven afectadas por la intervención.\nLa idea es construir un modelo de serie de tiempo para predecir el resultado contrafáctico. En otras palabras, el modelo utilizará la serie temporal de control para predecir cuál habría sido el resultado de la serie temporal de respuesta si no hubiera habido intervención.\nEn este ejemplo, creamos una variable de serie temporal de respuesta y una variable de serie temporal de control. * Para que el conjunto de datos sea reproducible, se establece una semilla aleatoria al comienzo del código. * Luego se crea un proceso de promedio móvil autorregresivo (ARMA). La parte autorregresiva (AR) tiene dos coeficientes 0,95 y 0,05, y la parte de media móvil (MA) tiene dos coeficientes 0,6 y 0,3. * Después de crear el proceso de media móvil autorregresiva (ARMA), se generan 500 muestras a partir del proceso. * La variable de serie temporal de control X se crea añadiendo un valor constante de 10 a los valores generados. * La variable de serie temporal de respuesta y es una función de la variable de serie temporal de control X. Es igual a 2 veces X más un valor aleatorio. * La intervención ocurre en el índice de 300, y el verdadero impacto causal es 10.\n\n# Set up a seed for reproducibility\nnp.random.seed(42)\n\n# Autoregressive coefficients\narparams = np.array([.95, .05])\n\n# Moving average coefficients\nmaparams = np.array([.6, .3])\n\n# Create a ARMA process\narma_process = ArmaProcess.from_coeffs(arparams, maparams)\n\n# Create the control time-series\nX = 10 + arma_process.generate_sample(nsample=500)\n\n# Create the response time-series\ny = 2 * X + np.random.normal(size=500)\n\n# Add the true causal impact\ny[300:] += 10\n\nUna serie de tiempo generalmente tiene una variable de tiempo que indica la frecuencia de los datos recopilados. Creamos 500 fechas a partir del 1 de enero de 2021 usando la función pandas date_range, lo que indica que el conjunto de datos tiene datos diarios.\nDespués de eso, se crea un marco de datos de pandas con la variable de control X, la variable de respuesta es y y las dates como índice.\n\n# Create dates\ndates = pd.date_range('2021-01-01', freq='D', periods=500)\n\n# Create dataframe\ndf = pd.DataFrame({'dates': dates, 'y': y, 'X': X}, columns=['dates', 'y', 'X'])\n\n# Set dates as index\ndf.set_index('dates', inplace=True)\n\n# Take a look at the data\ndf.head()\n\n\n\n\n\n\n\n\n\ny\nX\n\n\ndates\n\n\n\n\n\n\n2021-01-01\n21.919606\n10.496714\n\n\n2021-01-02\n23.172702\n10.631643\n\n\n2021-01-03\n21.278713\n11.338640\n\n\n2021-01-04\n26.909878\n13.173454\n\n\n2021-01-05\n27.260727\n13.955685\n\n\n\n\n\n\n\n\n\n\n\nEstableceremos los periodos de pre y post intervención. Usando df.index, podemos ver que la fecha de inicio de la serie temporal es 2021-01-01, la fecha de finalización de la serie temporal es 2022-05-15 y la fecha de inicio del tratamiento es 2021- 10-28.\n\n# Print out the time series start date\nprint(f'The time-series start date is :{df.index.min()}')\n\n# Print out the time series end date\nprint(f'The time-series end date is :{df.index.max()}')\n\n# Print out the intervention start date\nprint(f'The treatment start date is :{df.index[300]}')\n\nThe time-series start date is :2021-01-01 00:00:00\nThe time-series end date is :2022-05-15 00:00:00\nThe treatment start date is :2021-10-28 00:00:00\n\n\nA continuación, visualicemos los datos de la serie temporal.\n\n# Visualize data using seaborn\nsns.set(rc={'figure.figsize':(12,8)})\nsns.lineplot(x=df.index, y=df['X'])\nsns.lineplot(x=df.index, y=df['y'])\nplt.axvline(x= df.index[300], color='red')\nplt.legend(labels = ['X', 'y'])\nplt.show()\n\n\n\n\n\n\n\n\nEn el gráfico, la línea azul es la serie temporal de control, la línea naranja es la serie temporal de respuesta y la línea vertical roja representa la fecha de inicio de la intervención.\nPodemos ver que antes de la intervención, las series temporales de control y respuesta tienen valores similares. Después de la intervención, la serie de tiempo de respuesta tiene consistentemente valores más altos que la serie de tiempo de control.\nEl paquete CausalImpact de python requiere las entradas de los períodos anterior y posterior en un formato de lista. El primer elemento de la lista es el índice inicial y el último elemento de la lista es el índice final.\nLa fecha de inicio de la intervención es 2021-10-28, por lo que el período previo finaliza en 2021-10-27.\n\n# Set pre-period\npre_period = [str(df.index.min())[:10], str(df.index[299])[:10]]\n\n# Set post-period\npost_period = [str(df.index[300])[:10], str(df.index.max())[:10]]\n\n# Print out the values\nprint(f'The pre-period is {pre_period}')\nprint(f'The post-period is {post_period}')\n\nThe pre-period is ['2021-01-01', '2021-10-27']\nThe post-period is ['2021-10-28', '2022-05-15']\n\n\n\n\n\nCalcularemos la diferencia bruta entre los períodos previo y posterior.\nPodemos ver que el promedio diario previo al tratamiento es -1,64, el promedio diario posterior al tratamiento es 50,08 y la diferencia bruta entre el tratamiento previo y posterior es 51,7, que es mucho mayor que el verdadero impacto causal de 10.\nSin análisis de causalidad, sobreestimaremos el impacto causal.\n\n# Calculate the pre-daily average\npre_daily_avg = df['y'][:300].mean()\n\n# Calculate the post-daily average\npost_daily_avg = df['y'][300:].mean()\n\n# Print out the results\nprint(f'The pre-treatment daily average is {pre_daily_avg}.')\nprint(f'The post-treatment daily average is {post_daily_avg}.')\nprint(f'The raw difference between the pre and the post treatment is {post_daily_avg - pre_daily_avg}.')\n\nThe pre-treatment daily average is -1.6403416947312546.\nThe post-treatment daily average is 50.08461262581729.\nThe raw difference between the pre and the post treatment is 51.72495432054855.\n\n\n\n\n\nejecutaremos el análisis de impacto causal sobre la serie temporal.\nEl análisis de causalidad tiene dos supuestos: * Supuesto 1: Hay una o más series temporales de control que están altamente correlacionadas con la variable de respuesta, pero que no se ven afectadas por la intervención. La violación de esta suposición puede dar lugar a conclusiones erróneas sobre la existencia, la dirección o la magnitud del efecto del tratamiento. * Supuesto 2: La correlación entre el control y la serie temporal de respuesta es la misma para antes y después de la intervención.\nLos datos de series de tiempo sintéticos que creamos satisfacen las dos suposiciones.\nEl paquete CausalImpact de python tiene una función llamada CausalImpact que implementa un modelo de serie de tiempo estructural bayesiano (BSTS) en el backend. Tiene tres entradas requeridas: * data toma el nombre del dataframe de python. * pre_period toma los valores de índice inicial y final para el período previo a la intervención. * post_period toma los valores de índice inicial y final para el período posterior a la intervención.\nDespués de guardar el objeto de salida en una variable llamada impact, podemos ejecutar impact.plot() para visualizar los resultados.\n\n# Causal impact model\nimpact = CausalImpact(data=df, pre_period=pre_period, post_period=post_period)\n\n# Visualization\nimpact.plot()\n\nplt.show()\n\nC:\\Users\\franc\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\basic-tools-a_KeXlN7-py3.8\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n  self._init_dates(dates, freq)\nC:\\Users\\franc\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\basic-tools-a_KeXlN7-py3.8\\lib\\site-packages\\statsmodels\\base\\optimizer.py:17: FutureWarning: Keyword arguments have been passed to the optimizer that have no effect. The list of allowed keyword arguments for method lbfgs is: m, pgtol, factr, maxfun, epsilon, approx_grad, bounds, loglike_and_score. The list of unsupported keyword arguments passed include: nseasons, standardize. After release 0.14, this will raise.\n  warnings.warn(\nC:\\Users\\franc\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\basic-tools-a_KeXlN7-py3.8\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n  self._init_dates(dates, freq)\n\n\n\n\n\n\n\n\n\nLa visualización consta de tres gráficos: * El primer gráfico traza los valores contrafactuales pronosticados y los valores reales para el período posterior. * El segundo gráfico representa los efectos puntuales, que son las diferencias entre los valores reales y los previstos. Podemos ver que los valores de los efectos de puntos anteriores al período están alrededor de 0, y los valores de los efectos de puntos posteriores al período están alrededor del impacto real de 10. * El tercer gráfico traza el efecto acumulativo, que es la suma acumulativa de los efectos de puntos del segundo gráfico.\n\n\n\nResumiremos el impacto causal de la intervención para la serie temporal.\nEl resumen de impact.summary() nos dice que: * El promedio posterior a la intervención real es 50,08 y el promedio posterior a la intervención pronosticado es 40,3. * El efecto causal absoluto es 10,06, que está muy cerca del verdadero impacto de 10 y mucho mejor que la diferencia bruta de 51,7. * El efecto causal relativo es del 25,12%. * La probabilidad posterior de un efecto causal es del 100%, lo que demuestra que el modelo tiene mucha confianza en que existe el impacto causal.\n\n# Causal impact summary\nprint(impact.summary())\n\nPosterior Inference {Causal Impact}\n                          Average            Cumulative\nActual                    50.08              10016.92\nPrediction (s.d.)         40.03 (1.2)        8005.58 (239.36)\n95% CI                    [37.64, 42.33]     [7527.6, 8465.89]\n\nAbsolute effect (s.d.)    10.06 (1.2)        2011.34 (239.36)\n95% CI                    [7.76, 12.45]      [1551.03, 2489.32]\n\nRelative effect (s.d.)    25.12% (2.99%)     25.12% (2.99%)\n95% CI                    [19.37%, 31.09%]   [19.37%, 31.09%]\n\nPosterior tail-area probability p: 0.0\nPosterior prob. of a causal effect: 100.0%\n\nFor more details run the command: print(impact.summary('report'))\n\n\nPodemos imprimir la versión del informe del resumen usando la opción output='report'.\n\n# Causal impact report\nprint(impact.summary(output='report'))\n\nAnalysis report {CausalImpact}\n\n\nDuring the post-intervention period, the response variable had\nan average value of approx. 50.08. By contrast, in the absence of an\nintervention, we would have expected an average response of 40.03.\nThe 95% interval of this counterfactual prediction is [37.64, 42.33].\nSubtracting this prediction from the observed response yields\nan estimate of the causal effect the intervention had on the\nresponse variable. This effect is 10.06 with a 95% interval of\n[7.76, 12.45]. For a discussion of the significance of this effect,\nsee below.\n\n\nSumming up the individual data points during the post-intervention\nperiod (which can only sometimes be meaningfully interpreted), the\nresponse variable had an overall value of 10016.92.\nBy contrast, had the intervention not taken place, we would have expected\na sum of 8005.58. The 95% interval of this prediction is [7527.6, 8465.89].\n\n\nThe above results are given in terms of absolute numbers. In relative\nterms, the response variable showed an increase of +25.12%. The 95%\ninterval of this percentage is [19.37%, 31.09%].\n\n\nThis means that the positive effect observed during the intervention\nperiod is statistically significant and unlikely to be due to random\nfluctuations. It should be noted, however, that the question of whether\nthis increase also bears substantive significance can only be answered\nby comparing the absolute effect (10.06) to the original goal\nof the underlying intervention.\n\n\nThe probability of obtaining this effect by chance is very small\n(Bayesian one-sided tail-area probability p = 0.0).\nThis means the causal effect can be considered statistically\nsignificant.\n\n\n\n\n\nHablaremos sobre las diferencias del paquete CausalImpact de Google entre Python y R.\nEl paquete python fue portado desde el paquete R, por lo que la mayoría de las veces los dos paquetes producen resultados similares, pero a veces producen resultados diferentes.\nLas diferencias son causadas por suposiciones para inicializaciones previas, el proceso de optimización y los algoritmos de implementación.\nLa documentación del paquete pycausalimpact recomienda encarecidamente establecer prior_level_sd en Ninguno, lo que permitirá que statsmodel realice la optimización para el componente anterior en el nivel local.\nEn base a esta sugerencia, se crea una versión con la opción prior_level_sd=None.\n\n# Causal impact model without prior level sd\nimpact_no_prior_level_sd = CausalImpact(df, pre_period, post_period, prior_level_sd=None)\n\n# Plot the results\nimpact_no_prior_level_sd.plot()\n\nC:\\Users\\franc\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\basic-tools-a_KeXlN7-py3.8\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n  self._init_dates(dates, freq)\nC:\\Users\\franc\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\basic-tools-a_KeXlN7-py3.8\\lib\\site-packages\\statsmodels\\base\\optimizer.py:17: FutureWarning: Keyword arguments have been passed to the optimizer that have no effect. The list of allowed keyword arguments for method lbfgs is: m, pgtol, factr, maxfun, epsilon, approx_grad, bounds, loglike_and_score. The list of unsupported keyword arguments passed include: nseasons, standardize, prior_level_sd. After release 0.14, this will raise.\n  warnings.warn(\nC:\\Users\\franc\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\basic-tools-a_KeXlN7-py3.8\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n  self._init_dates(dates, freq)\n\n\n\n\n\n\n\n\n\nPodemos ver que los valores de estimación puntual son similares, pero las desviaciones estándar son más pequeñas para la estimación.\n\n# Print out the summary\nprint(impact_no_prior_level_sd.summary())\n\nPosterior Inference {Causal Impact}\n                          Average            Cumulative\nActual                    50.08              10016.92\nPrediction (s.d.)         39.84 (0.09)       7967.47 (18.41)\n95% CI                    [39.65, 40.01]     [7929.85, 8001.99]\n\nAbsolute effect (s.d.)    10.25 (0.09)       2049.45 (18.41)\n95% CI                    [10.07, 10.44]     [2014.93, 2087.08]\n\nRelative effect (s.d.)    25.72% (0.23%)     25.72% (0.23%)\n95% CI                    [25.29%, 26.19%]   [25.29%, 26.19%]\n\nPosterior tail-area probability p: 0.0\nPosterior prob. of a causal effect: 100.0%\n\nFor more details run the command: print(impact.summary('report'))\n\n\n\n\n\nPara aprender a ajustar los hiperparámetros del modelo de impacto causal de series temporales con el paquete CausalImpact de python, consulte eltutorial Hyperparameter Tuning for Time Series Causal Impact Analysis in Python\n\n\n\n\nInferring causal impact using Bayesian structural time-series models.\nTime Series Causal Impact Analysis in Python | Machine Learning.\nInferring the effect of an event using CausalImpact by Kay Brodersen."
  },
  {
    "objectID": "posts/2022/2022-10-12-causal_impact.html#introducción",
    "href": "posts/2022/2022-10-12-causal_impact.html#introducción",
    "title": "Causal Impact",
    "section": "",
    "text": "El paquete CausalImpact creado por Google estima el impacto de una intervención en una serie temporal. Por ejemplo, ¿cómo afecta una nueva función en una aplicación el tiempo de los usuarios en la aplicación?\nEn este tutorial, hablaremos sobre cómo usar el paquete de Python CausalImpact para hacer inferencias causales de series de tiempo. Aprenderás: * ¿Cómo establecer los períodos previo y posterior para el análisis de impacto causal? * ¿Cómo realizar inferencias causales sobre datos de series temporales? * ¿Cómo resumir los resultados del análisis de causalidad y crear un informe? * ¿Cuáles son las diferencias entre los paquetes python y R para CausalImpact?"
  },
  {
    "objectID": "posts/2022/2022-10-12-causal_impact.html#importar-librerías",
    "href": "posts/2022/2022-10-12-causal_impact.html#importar-librerías",
    "title": "Causal Impact",
    "section": "",
    "text": "En primer lugar, instalemos pycausalimpac para el análisis causal de series de tiempo.\n\n# Install python version of causal impact\n#!pip install pycausalimpact\n\nUna vez completada la instalación, podemos importar las bibliotecas. * pandas, numpy y datetime se importan para el procesamiento de datos. * ArmaProcess se importa para la creación de datos de series temporales sintéticas. * matplotlib y seaborn son para visualización. * CausalImpact es para la estimación de los efectos del tratamiento de series de tiempo.\n\n# Data processing\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime\n\n# Create synthetic time-series data\nfrom statsmodels.tsa.arima_process import ArmaProcess\n\n# Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Causal impact\nfrom causalimpact import CausalImpact"
  },
  {
    "objectID": "posts/2022/2022-10-12-causal_impact.html#crear-el-conjunto-de-datos",
    "href": "posts/2022/2022-10-12-causal_impact.html#crear-el-conjunto-de-datos",
    "title": "Causal Impact",
    "section": "",
    "text": "Crearemos un conjunto de datos de series de tiempo sintético para el análisis de impacto causal. El beneficio de usar un conjunto de datos sintético es que podemos validar la precisión de los resultados del modelo.\nEl paquete CausalImpact requiere dos tipos de series temporales: * Una serie temporal de respuesta que se ve directamente afectada por la intervención. * Y una o más series temporales de control que no se ven afectadas por la intervención.\nLa idea es construir un modelo de serie de tiempo para predecir el resultado contrafáctico. En otras palabras, el modelo utilizará la serie temporal de control para predecir cuál habría sido el resultado de la serie temporal de respuesta si no hubiera habido intervención.\nEn este ejemplo, creamos una variable de serie temporal de respuesta y una variable de serie temporal de control. * Para que el conjunto de datos sea reproducible, se establece una semilla aleatoria al comienzo del código. * Luego se crea un proceso de promedio móvil autorregresivo (ARMA). La parte autorregresiva (AR) tiene dos coeficientes 0,95 y 0,05, y la parte de media móvil (MA) tiene dos coeficientes 0,6 y 0,3. * Después de crear el proceso de media móvil autorregresiva (ARMA), se generan 500 muestras a partir del proceso. * La variable de serie temporal de control X se crea añadiendo un valor constante de 10 a los valores generados. * La variable de serie temporal de respuesta y es una función de la variable de serie temporal de control X. Es igual a 2 veces X más un valor aleatorio. * La intervención ocurre en el índice de 300, y el verdadero impacto causal es 10.\n\n# Set up a seed for reproducibility\nnp.random.seed(42)\n\n# Autoregressive coefficients\narparams = np.array([.95, .05])\n\n# Moving average coefficients\nmaparams = np.array([.6, .3])\n\n# Create a ARMA process\narma_process = ArmaProcess.from_coeffs(arparams, maparams)\n\n# Create the control time-series\nX = 10 + arma_process.generate_sample(nsample=500)\n\n# Create the response time-series\ny = 2 * X + np.random.normal(size=500)\n\n# Add the true causal impact\ny[300:] += 10\n\nUna serie de tiempo generalmente tiene una variable de tiempo que indica la frecuencia de los datos recopilados. Creamos 500 fechas a partir del 1 de enero de 2021 usando la función pandas date_range, lo que indica que el conjunto de datos tiene datos diarios.\nDespués de eso, se crea un marco de datos de pandas con la variable de control X, la variable de respuesta es y y las dates como índice.\n\n# Create dates\ndates = pd.date_range('2021-01-01', freq='D', periods=500)\n\n# Create dataframe\ndf = pd.DataFrame({'dates': dates, 'y': y, 'X': X}, columns=['dates', 'y', 'X'])\n\n# Set dates as index\ndf.set_index('dates', inplace=True)\n\n# Take a look at the data\ndf.head()\n\n\n\n\n\n\n\n\n\ny\nX\n\n\ndates\n\n\n\n\n\n\n2021-01-01\n21.919606\n10.496714\n\n\n2021-01-02\n23.172702\n10.631643\n\n\n2021-01-03\n21.278713\n11.338640\n\n\n2021-01-04\n26.909878\n13.173454\n\n\n2021-01-05\n27.260727\n13.955685"
  },
  {
    "objectID": "posts/2022/2022-10-12-causal_impact.html#períodos-anteriores-y-posteriores",
    "href": "posts/2022/2022-10-12-causal_impact.html#períodos-anteriores-y-posteriores",
    "title": "Causal Impact",
    "section": "",
    "text": "Estableceremos los periodos de pre y post intervención. Usando df.index, podemos ver que la fecha de inicio de la serie temporal es 2021-01-01, la fecha de finalización de la serie temporal es 2022-05-15 y la fecha de inicio del tratamiento es 2021- 10-28.\n\n# Print out the time series start date\nprint(f'The time-series start date is :{df.index.min()}')\n\n# Print out the time series end date\nprint(f'The time-series end date is :{df.index.max()}')\n\n# Print out the intervention start date\nprint(f'The treatment start date is :{df.index[300]}')\n\nThe time-series start date is :2021-01-01 00:00:00\nThe time-series end date is :2022-05-15 00:00:00\nThe treatment start date is :2021-10-28 00:00:00\n\n\nA continuación, visualicemos los datos de la serie temporal.\n\n# Visualize data using seaborn\nsns.set(rc={'figure.figsize':(12,8)})\nsns.lineplot(x=df.index, y=df['X'])\nsns.lineplot(x=df.index, y=df['y'])\nplt.axvline(x= df.index[300], color='red')\nplt.legend(labels = ['X', 'y'])\nplt.show()\n\n\n\n\n\n\n\n\nEn el gráfico, la línea azul es la serie temporal de control, la línea naranja es la serie temporal de respuesta y la línea vertical roja representa la fecha de inicio de la intervención.\nPodemos ver que antes de la intervención, las series temporales de control y respuesta tienen valores similares. Después de la intervención, la serie de tiempo de respuesta tiene consistentemente valores más altos que la serie de tiempo de control.\nEl paquete CausalImpact de python requiere las entradas de los períodos anterior y posterior en un formato de lista. El primer elemento de la lista es el índice inicial y el último elemento de la lista es el índice final.\nLa fecha de inicio de la intervención es 2021-10-28, por lo que el período previo finaliza en 2021-10-27.\n\n# Set pre-period\npre_period = [str(df.index.min())[:10], str(df.index[299])[:10]]\n\n# Set post-period\npost_period = [str(df.index[300])[:10], str(df.index.max())[:10]]\n\n# Print out the values\nprint(f'The pre-period is {pre_period}')\nprint(f'The post-period is {post_period}')\n\nThe pre-period is ['2021-01-01', '2021-10-27']\nThe post-period is ['2021-10-28', '2022-05-15']"
  },
  {
    "objectID": "posts/2022/2022-10-12-causal_impact.html#diferencias-sin-procesar",
    "href": "posts/2022/2022-10-12-causal_impact.html#diferencias-sin-procesar",
    "title": "Causal Impact",
    "section": "",
    "text": "Calcularemos la diferencia bruta entre los períodos previo y posterior.\nPodemos ver que el promedio diario previo al tratamiento es -1,64, el promedio diario posterior al tratamiento es 50,08 y la diferencia bruta entre el tratamiento previo y posterior es 51,7, que es mucho mayor que el verdadero impacto causal de 10.\nSin análisis de causalidad, sobreestimaremos el impacto causal.\n\n# Calculate the pre-daily average\npre_daily_avg = df['y'][:300].mean()\n\n# Calculate the post-daily average\npost_daily_avg = df['y'][300:].mean()\n\n# Print out the results\nprint(f'The pre-treatment daily average is {pre_daily_avg}.')\nprint(f'The post-treatment daily average is {post_daily_avg}.')\nprint(f'The raw difference between the pre and the post treatment is {post_daily_avg - pre_daily_avg}.')\n\nThe pre-treatment daily average is -1.6403416947312546.\nThe post-treatment daily average is 50.08461262581729.\nThe raw difference between the pre and the post treatment is 51.72495432054855."
  },
  {
    "objectID": "posts/2022/2022-10-12-causal_impact.html#causal-impact-en-series-de-tiempo",
    "href": "posts/2022/2022-10-12-causal_impact.html#causal-impact-en-series-de-tiempo",
    "title": "Causal Impact",
    "section": "",
    "text": "ejecutaremos el análisis de impacto causal sobre la serie temporal.\nEl análisis de causalidad tiene dos supuestos: * Supuesto 1: Hay una o más series temporales de control que están altamente correlacionadas con la variable de respuesta, pero que no se ven afectadas por la intervención. La violación de esta suposición puede dar lugar a conclusiones erróneas sobre la existencia, la dirección o la magnitud del efecto del tratamiento. * Supuesto 2: La correlación entre el control y la serie temporal de respuesta es la misma para antes y después de la intervención.\nLos datos de series de tiempo sintéticos que creamos satisfacen las dos suposiciones.\nEl paquete CausalImpact de python tiene una función llamada CausalImpact que implementa un modelo de serie de tiempo estructural bayesiano (BSTS) en el backend. Tiene tres entradas requeridas: * data toma el nombre del dataframe de python. * pre_period toma los valores de índice inicial y final para el período previo a la intervención. * post_period toma los valores de índice inicial y final para el período posterior a la intervención.\nDespués de guardar el objeto de salida en una variable llamada impact, podemos ejecutar impact.plot() para visualizar los resultados.\n\n# Causal impact model\nimpact = CausalImpact(data=df, pre_period=pre_period, post_period=post_period)\n\n# Visualization\nimpact.plot()\n\nplt.show()\n\nC:\\Users\\franc\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\basic-tools-a_KeXlN7-py3.8\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n  self._init_dates(dates, freq)\nC:\\Users\\franc\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\basic-tools-a_KeXlN7-py3.8\\lib\\site-packages\\statsmodels\\base\\optimizer.py:17: FutureWarning: Keyword arguments have been passed to the optimizer that have no effect. The list of allowed keyword arguments for method lbfgs is: m, pgtol, factr, maxfun, epsilon, approx_grad, bounds, loglike_and_score. The list of unsupported keyword arguments passed include: nseasons, standardize. After release 0.14, this will raise.\n  warnings.warn(\nC:\\Users\\franc\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\basic-tools-a_KeXlN7-py3.8\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n  self._init_dates(dates, freq)\n\n\n\n\n\n\n\n\n\nLa visualización consta de tres gráficos: * El primer gráfico traza los valores contrafactuales pronosticados y los valores reales para el período posterior. * El segundo gráfico representa los efectos puntuales, que son las diferencias entre los valores reales y los previstos. Podemos ver que los valores de los efectos de puntos anteriores al período están alrededor de 0, y los valores de los efectos de puntos posteriores al período están alrededor del impacto real de 10. * El tercer gráfico traza el efecto acumulativo, que es la suma acumulativa de los efectos de puntos del segundo gráfico."
  },
  {
    "objectID": "posts/2022/2022-10-12-causal_impact.html#resumen-causal-impact",
    "href": "posts/2022/2022-10-12-causal_impact.html#resumen-causal-impact",
    "title": "Causal Impact",
    "section": "",
    "text": "Resumiremos el impacto causal de la intervención para la serie temporal.\nEl resumen de impact.summary() nos dice que: * El promedio posterior a la intervención real es 50,08 y el promedio posterior a la intervención pronosticado es 40,3. * El efecto causal absoluto es 10,06, que está muy cerca del verdadero impacto de 10 y mucho mejor que la diferencia bruta de 51,7. * El efecto causal relativo es del 25,12%. * La probabilidad posterior de un efecto causal es del 100%, lo que demuestra que el modelo tiene mucha confianza en que existe el impacto causal.\n\n# Causal impact summary\nprint(impact.summary())\n\nPosterior Inference {Causal Impact}\n                          Average            Cumulative\nActual                    50.08              10016.92\nPrediction (s.d.)         40.03 (1.2)        8005.58 (239.36)\n95% CI                    [37.64, 42.33]     [7527.6, 8465.89]\n\nAbsolute effect (s.d.)    10.06 (1.2)        2011.34 (239.36)\n95% CI                    [7.76, 12.45]      [1551.03, 2489.32]\n\nRelative effect (s.d.)    25.12% (2.99%)     25.12% (2.99%)\n95% CI                    [19.37%, 31.09%]   [19.37%, 31.09%]\n\nPosterior tail-area probability p: 0.0\nPosterior prob. of a causal effect: 100.0%\n\nFor more details run the command: print(impact.summary('report'))\n\n\nPodemos imprimir la versión del informe del resumen usando la opción output='report'.\n\n# Causal impact report\nprint(impact.summary(output='report'))\n\nAnalysis report {CausalImpact}\n\n\nDuring the post-intervention period, the response variable had\nan average value of approx. 50.08. By contrast, in the absence of an\nintervention, we would have expected an average response of 40.03.\nThe 95% interval of this counterfactual prediction is [37.64, 42.33].\nSubtracting this prediction from the observed response yields\nan estimate of the causal effect the intervention had on the\nresponse variable. This effect is 10.06 with a 95% interval of\n[7.76, 12.45]. For a discussion of the significance of this effect,\nsee below.\n\n\nSumming up the individual data points during the post-intervention\nperiod (which can only sometimes be meaningfully interpreted), the\nresponse variable had an overall value of 10016.92.\nBy contrast, had the intervention not taken place, we would have expected\na sum of 8005.58. The 95% interval of this prediction is [7527.6, 8465.89].\n\n\nThe above results are given in terms of absolute numbers. In relative\nterms, the response variable showed an increase of +25.12%. The 95%\ninterval of this percentage is [19.37%, 31.09%].\n\n\nThis means that the positive effect observed during the intervention\nperiod is statistically significant and unlikely to be due to random\nfluctuations. It should be noted, however, that the question of whether\nthis increase also bears substantive significance can only be answered\nby comparing the absolute effect (10.06) to the original goal\nof the underlying intervention.\n\n\nThe probability of obtaining this effect by chance is very small\n(Bayesian one-sided tail-area probability p = 0.0).\nThis means the causal effect can be considered statistically\nsignificant."
  },
  {
    "objectID": "posts/2022/2022-10-12-causal_impact.html#causalimpact-python-vs-r",
    "href": "posts/2022/2022-10-12-causal_impact.html#causalimpact-python-vs-r",
    "title": "Causal Impact",
    "section": "",
    "text": "Hablaremos sobre las diferencias del paquete CausalImpact de Google entre Python y R.\nEl paquete python fue portado desde el paquete R, por lo que la mayoría de las veces los dos paquetes producen resultados similares, pero a veces producen resultados diferentes.\nLas diferencias son causadas por suposiciones para inicializaciones previas, el proceso de optimización y los algoritmos de implementación.\nLa documentación del paquete pycausalimpact recomienda encarecidamente establecer prior_level_sd en Ninguno, lo que permitirá que statsmodel realice la optimización para el componente anterior en el nivel local.\nEn base a esta sugerencia, se crea una versión con la opción prior_level_sd=None.\n\n# Causal impact model without prior level sd\nimpact_no_prior_level_sd = CausalImpact(df, pre_period, post_period, prior_level_sd=None)\n\n# Plot the results\nimpact_no_prior_level_sd.plot()\n\nC:\\Users\\franc\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\basic-tools-a_KeXlN7-py3.8\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n  self._init_dates(dates, freq)\nC:\\Users\\franc\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\basic-tools-a_KeXlN7-py3.8\\lib\\site-packages\\statsmodels\\base\\optimizer.py:17: FutureWarning: Keyword arguments have been passed to the optimizer that have no effect. The list of allowed keyword arguments for method lbfgs is: m, pgtol, factr, maxfun, epsilon, approx_grad, bounds, loglike_and_score. The list of unsupported keyword arguments passed include: nseasons, standardize, prior_level_sd. After release 0.14, this will raise.\n  warnings.warn(\nC:\\Users\\franc\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\basic-tools-a_KeXlN7-py3.8\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n  self._init_dates(dates, freq)\n\n\n\n\n\n\n\n\n\nPodemos ver que los valores de estimación puntual son similares, pero las desviaciones estándar son más pequeñas para la estimación.\n\n# Print out the summary\nprint(impact_no_prior_level_sd.summary())\n\nPosterior Inference {Causal Impact}\n                          Average            Cumulative\nActual                    50.08              10016.92\nPrediction (s.d.)         39.84 (0.09)       7967.47 (18.41)\n95% CI                    [39.65, 40.01]     [7929.85, 8001.99]\n\nAbsolute effect (s.d.)    10.25 (0.09)       2049.45 (18.41)\n95% CI                    [10.07, 10.44]     [2014.93, 2087.08]\n\nRelative effect (s.d.)    25.72% (0.23%)     25.72% (0.23%)\n95% CI                    [25.29%, 26.19%]   [25.29%, 26.19%]\n\nPosterior tail-area probability p: 0.0\nPosterior prob. of a causal effect: 100.0%\n\nFor more details run the command: print(impact.summary('report'))"
  },
  {
    "objectID": "posts/2022/2022-10-12-causal_impact.html#ajuste-de-hiperparámetros",
    "href": "posts/2022/2022-10-12-causal_impact.html#ajuste-de-hiperparámetros",
    "title": "Causal Impact",
    "section": "",
    "text": "Para aprender a ajustar los hiperparámetros del modelo de impacto causal de series temporales con el paquete CausalImpact de python, consulte eltutorial Hyperparameter Tuning for Time Series Causal Impact Analysis in Python"
  },
  {
    "objectID": "posts/2022/2022-10-12-causal_impact.html#referencias",
    "href": "posts/2022/2022-10-12-causal_impact.html#referencias",
    "title": "Causal Impact",
    "section": "",
    "text": "Inferring causal impact using Bayesian structural time-series models.\nTime Series Causal Impact Analysis in Python | Machine Learning.\nInferring the effect of an event using CausalImpact by Kay Brodersen."
  },
  {
    "objectID": "posts/2021/2021-08-31-buenas_practicas.html",
    "href": "posts/2021/2021-08-31-buenas_practicas.html",
    "title": "Buenas Prácticas - Python",
    "section": "",
    "text": "Una pregunta que surgue a menudo cuando uno se encuentra programando es saber cuál es la forma correcta de programar. La respuesta es que no existe la forma correcta de programar (ya sea en Python o cualquier otro lenguaje), sin embargo, existen estandares dentro del mundo de la programación, con el fin de hacer el código más legible, sencillo de entender y ayudar a encontrar posibles errores.\nEn esta sección se mostrará algunos conceptos sencillos que te ayudarán a mejorar tus skills en el desarrollo de software (con Python).\n\n\n\nEl PEP8 es un estilo de codificación que proporciona convenciones de codificación para el código Python que comprende la biblioteca estándar en la distribución principal de Python.\nAlgunos aspectos importantes:\n\nEl PEP8 y el PEP 257 (Docstring Conventions) fueron adaptados del ensayo original de la Guía de estilo Python de Guido, con algunas adiciones de la guía de estilo de Barry.\nEsta guía de estilo evoluciona con el tiempo a medida que se identifican convenciones adicionales y las convenciones pasadas se vuelven obsoletas debido a cambios en el propio lenguaje.\nMuchos proyectos tienen sus propias pautas de estilo de codificación. En caso de conflicto, dichas guías específicas del proyecto tienen prioridad para ese proyecto.\n\nBasados en el PEP8 y algunas buenas prácticas del diseño de software, veamos ejemplo para poder escribir de mejor forma nuestros códigos.\n\n\nCuando sea posible, define variables con nombres que tengan algún sentido o que puedas identificar fácilmente, no importa que sean más largas. Por ejemplo, en un programa podríamos escribir:\n\na = 10.  \nb = 3.5 \nprint(f\"El area es {a*b}\" )\n\nEl area es 35.0\n\n\npero, ¿qué significan a y b? lo sabemos por el comentario (bien hecho), pero si más adelante nos encontramos con esas variables, tendremos que recordar cual es cual. Es mejor usar nombres con significado:\n\naltura = 10.  \nbase = 3.5 \nprint(f\"El area es {a*b}\" )\n\nEl area es 35.0\n\n\n\n\n\nLas líneas de codigo no deben ser muy largas, como mucho 72 caracteres. Si se tiene una línea larga, se puede cortar con una barra invertida (\\) y continuar en la siguiente línea:\n\nprint(\"Esta es una frase muy larga, se puede cortar con un \\\n       y seguir en la línea inferior.\")\n\nEsta es una frase muy larga, se puede cortar con un        y seguir en la línea inferior.\n\n\n\n\n\nLos comentarios son muy importantes al escribir un programa. Describen lo que está sucediendo dentro de un programa, para que una persona que mira el código fuente no tenga dificultades para descifrarlo.\n\n#\n# esto es un comentario\nprint('Hola')\n\nHola\n\n\nTambién podemos tener comentarios multilíneas:\n\n#\n# Este es un comentario largo\n# y se extiende\n# a varias líneas\n\n\n\n\nLas importaciones generalmente deben estar en líneas separadas:\n\n#\n# no:\nimport sys, os\n\n\n#\n# si:\nimport os\nimport sys\n\n\n\n\nExisten varias formas de hacer comparaciones de objetos (principalmente en el uso del bucle if), acá se dejan alguna recomendaciones:\n# no\nif greeting == True:\n\n# no\nif greeting is True:\n# si\nif greeting:\n\n\n\nDentro de paréntesis, corchetes o llaves, no dejar espacios inmediatamente dentro de ellos:\n\n#\n# no\nlista_01 = [1, 2, 3,4, 5, 6,7, 8, 9,]\n\n\n#\n# si \nlista_01 = [\n    1, 2, 3,\n    4, 5, 6,\n    7, 8, 9, \n]\n\nAunque en Python se pueden hacer varias declaraciones en una línea, se recomienda hacer sólo una en cada línea:\n\n#\n# no\na = 10; b = 20\n\n\n#\n# si\na = 10\nb = 20  \n\nCuando se trabaja con lista, conjuntos y/o tuplas se recomienda poner en cada línea sus argumentos.\n\n#\n# no\nlista = [(1, 'hola'),(2, 'mundo'),]  \n\n\n#\n# si\nlista = [\n    (1, 'hola'),\n    (2, 'mundo'),\n]\n\nLo anterior se puede extender para funciones con muchos argumentos\n\n#\n# no\ndef funcion_01(x1,x2,x3,x4):\n    print(x1,x2,x3,x4)\n    \ndef funcion_02(\n    x1,x2,x3,x4):\n    print(x1,x2,x3,x4)\n\n\n#\n# si\ndef funcion_01(x1,x2,\n               x3,x4):\n    \n    print(x1,x2,x3,x4)\n    \ndef funcion_02(\n        x1,x2,\n        x3,x4):\n    \n    print(x1,x2,x3,x4)\n\n\n\n\nUn tema interesante es corresponde a la identación respecto a los operadores binarios, acá se muestra la forma correcta de hacerlo:\n# no\nincome = (gross_wages +\n          taxable_interest +\n          (dividends - qualified_dividends) -\n          ira_deduction -\n          student_loan_interest)\n# si\nincome = (gross_wages\n          + taxable_interest\n          + (dividends - qualified_dividends)\n          - ira_deduction\n          - student_loan_interest)\n\n\n\nAunque combinar iterables con elementos de control de flujo para manipular listas es muy sencillo con Python, hay métodos específicos más eficientes para hacer lo mismo. Pensemos el fitrado de datos de una lista:\n\n#\n# Seleccionar los números positivos\nnumeros = [-3, 2, 1, -8, -2, 7]\npositivos = []\nfor i in numeros:\n    if i &gt; 0:\n        positivos.append(i)\n        \nprint(f\"positivos: {positivos}\")\n\npositivos: [2, 1, 7]\n\n\nAunque técnicamente es correcto, es más eficiente hacer List Comprehension:\n\n#\n# comprension de lista\nnumeros = [-3, 2, 1, -8, -2, 7]\npositivos = [i for i in numeros if i &gt; 0] # List Comprehension\nprint(f\"positivos: {positivos}\")\n\npositivos: [2, 1, 7]\n\n\n\n\n\nCuando se ocupa try/except, es necesario especificar el tipo de error que se está cometiendo.\n\n#\n# importar librerias\nimport sys\n\n\n#\n# no\ntry:\n    r = 1/0\nexcept:\n    print(\"Oops! ocurrio un\",sys.exc_info()[0])\n\nOops! ocurrio un &lt;class 'ZeroDivisionError'&gt;\n\n\n\n#\n# si\ntry:\n    r = 1/0\nexcept ZeroDivisionError:\n    print(\"Oops! ocurrio un\",sys.exc_info()[0])\n\nOops! ocurrio un &lt;class 'ZeroDivisionError'&gt;\n\n\n\n\n\nSiempre es mejor definir las variables dentro de una función y no dejar variables globales.\n\n#\n# no\nvalor = 5\n\ndef funcion_01(variable):\n    return 2*variable + valor\n\n\nfuncion_01(2)\n\n9\n\n\n\n#\n# si\ndef funcion_01(variable,valor):\n    return 2*variable + valor\n\n\nfuncion_01(2,5)\n\n9\n\n\n\n\n\nCon Python 3 se puede especificar el tipo de parámetro y el tipo de retorno de una función (usando la notación PEP484 y PEP526. Se definen dos conceptos claves:\n\nEscritura dinámica: no se especifican los atributos de los inputs ni de los ouputs\nEscritura estática: se especifican los atributos de los inputs y los ouputs\n\n\n#\n# escritura dinámica\ndef suma(x,y):\n    return x+y\n\n\nprint(suma(1,2))\n\n3\n\n\n\n#\n# escritura estatica\ndef suma(x:float,\n         y:float)-&gt;float:\n    return x+y\n\n\nprint(suma(1,2))\n\n3\n\n\nPara la escritura estática, si bien se especifica el tipo de atributo (tanto de los inputs o outputs), la función puede recibir otros tipos de atributos.\n\nprint(suma(\"hola\",\" mundo\"))\n\nhola mundo\n\n\nPara validar los tipos de datos son los correctos, se deben ocupar librerías especializadas en la validación de datos (por ejemplo: pydantic).\n\n\n\nExisten librerías que pueden ayudar a corregir errores de escrituras en tú código (también conocido como Análisis Estático), por ejemplo:\n\nblack: El formateador de código inflexible.\nflake8: La herramienta para aplicar la guía de estilo PEP8.\nmypy: Mypy es un verificador de tipo estático para Python 3.\n\n\n\n\n\nCasi tan importante como la escritura de código, es su correcta documentación, una parte fundamental de cualquier programa que a menudo se infravalora o simplemente se ignora. Aparte de los comentarios entre el código explicando cómo funciona, el elemento básico de documentación de Python es el Docstring o cadena de documentación, que ya hemos visto. Simplemente es una cadena de texto con triple comillas que se coloca justo después de la definición de función o clase que sirve de documentación a ese elemento.\n\ndef potencia(x, y):\n    \"\"\"\n    Calcula la potencia arbitraria de un numero\n    \"\"\"\n    return x**y\n\n\n# Acceso a la documentación\npotencia.__doc__\n\n'\\n    Calcula la potencia arbitraria de un numero\\n    '\n\n\n\n# Acceso a la documentación\nhelp(potencia)\n\nHelp on function potencia in module __main__:\n\npotencia(x, y)\n    Calcula la potencia arbitraria de un numero\n\n\n\nLo correcto es detallar lo mejor posible en el Docstring qué hace y cómo se usa la función o clase y los parámetros que necesita. Se recomienda usar el estilo de documentación del software de documentación sphinx, que emplea reStructuredText como lenguaje de marcado.\nVeamos un ejemplo de una función bien documentada:\n\ndef potencia(x, y):\n    \"\"\"\n    Calcula la potencia arbitraria de un numero\n\n    :param x: base\n    :param y: exponente\n    :return:  potencia de un numero\n    :ejemplos:\n    \n    &gt;&gt;&gt; potencia(2, 1)\n    2\n    &gt;&gt;&gt; potencia(3, 2)\n    9\n    \"\"\"\n\n    return x**y\n\n\n# Acceso a la documentación\npotencia.__doc__\n\n'\\n    Calcula la potencia arbitraria de un numero\\n\\n    :param x: base\\n    :param y: exponente\\n    :return:  potencia de un numero\\n    :ejemplos:\\n    \\n    &gt;&gt;&gt; potencia(2, 1)\\n    2\\n    &gt;&gt;&gt; potencia(3, 2)\\n    9\\n    '\n\n\n\n# Acceso a la documentación\nhelp(potencia)\n\nHelp on function potencia in module __main__:\n\npotencia(x, y)\n    Calcula la potencia arbitraria de un numero\n    \n    :param x: base\n    :param y: exponente\n    :return:  potencia de un numero\n    :ejemplos:\n    \n    &gt;&gt;&gt; potencia(2, 1)\n    2\n    &gt;&gt;&gt; potencia(3, 2)\n    9\n\n\n\nExisten varias formas de documentar tus funciones, las principales encontradas en la literatura son: * Google docstrings: forma de documentación recomendada por Google.. * reStructured Text: estándar oficial de documentación de Python; No es apto para principiantes, pero tiene muchas funciones. * NumPy/SciPy docstrings: combinación de NumPy de reStructured y Google Docstrings.\n\n\n\nEl Zen de Python te dará la guía para decidir sobre que hacer con tu código, no te dice como lo debes escribir, sino como debes pensar si estas programando en Python.\nPrincipios importantes:\n\nExplícito es mejor que implícito: Que no se asuma nada, asegúrate que las cosas sean.\nSimple es mejor que complejo: Evita código complejo, código espagueti o que hace mas cosas para poder hacer una simple tarea.\nPlano es mejor que anidado: Si tu código tiene mas de 3 niveles de identación, deberías mover parte de ese código a una función.\nLos errores nunca deberían pasar silenciosamente: No uses un Try/Except sin definir que tipo de error vas a cachar, viene de la mano con Explicito es mejor que implícito.\nSi la implementación es difícil de explicar, es mala idea.\n\nTambién, podemos ver el mensaje original del zen de python, ejecutando la siguiente linea de comando.\n\nimport this\n\nThe Zen of Python, by Tim Peters\n\nBeautiful is better than ugly.\nExplicit is better than implicit.\nSimple is better than complex.\nComplex is better than complicated.\nFlat is better than nested.\nSparse is better than dense.\nReadability counts.\nSpecial cases aren't special enough to break the rules.\nAlthough practicality beats purity.\nErrors should never pass silently.\nUnless explicitly silenced.\nIn the face of ambiguity, refuse the temptation to guess.\nThere should be one-- and preferably only one --obvious way to do it.\nAlthough that way may not be obvious at first unless you're Dutch.\nNow is better than never.\nAlthough never is often better than *right* now.\nIf the implementation is hard to explain, it's a bad idea.\nIf the implementation is easy to explain, it may be a good idea.\nNamespaces are one honking great idea -- let's do more of those!\n\n\n\n\n\nLos consejos que se presentan son de mucha utilidad si usted quiere llevar sus conociminetos de programación al siguiente nivel, sin embargo, el contenido de cada uno amerita un curso por si solo. Se deja recomienda al lector seguir profundizando en estos temas.\n\n\nPython al ser multiparadigma, nos da una amplia gama de posibilidades de diseñar nuestros códigos. Dentro de estos se destacan:\n\nProgramación orientada a objetos (OOP)\nProgramación funcional\n\nCuándo ocupar uno o la otra, va a depender de cómo queremos abordar una determinada problemática, puesto que en la mayoría de los casos, se puede pasar de un paradigma a o otro (incluso mezclarlos de ser necesario).\n\n\n\nEn ingeniería de software, SOLID (Single responsibility, Open-closed, Liskov substitution, Interface segregation and Dependency inversion) es un acrónimo mnemónico introducido por Robert C. Martin a comienzos de la década del 2000 que representa cinco principios básicos de la programación orientada a objetos y el diseño. Cuando estos principios se aplican en conjunto es más probable que un desarrollador cree un sistema que sea fácil de mantener y ampliar con el tiempo.\nEn el siguiente link se deja una guía para poder entender estos conceptos en python.\n\n\n\nLos patrones de diseño son la base para la búsqueda de soluciones a problemas comunes en el desarrollo de software y otros ámbitos referentes al diseño de interacción o interfaces.\n\nUn patrón de diseño es una solución a un problema de diseño.\n\nSe destacan tres tipos de patrones de diseños:\n\nComportamiento\nCreacionales\nEstructurales\n\nEn el siguiente link se deja una guía para poder entender estos conceptos en python.\n\n\n\n\n\nThe Clean Coder: A Code Of Conduct For Professional Programmers Robert C. Martin (2011)\nClean Code: A Handbook of Agile Software - Robert C. Martin (2009).\nWorking effectively with legacy code Michael C. Feathers (2004)\nRefactoring Martin Fowler (1999)\nThe Pragmatic Programmer Thomas Hunt (1999)"
  },
  {
    "objectID": "posts/2021/2021-08-31-buenas_practicas.html#introducción",
    "href": "posts/2021/2021-08-31-buenas_practicas.html#introducción",
    "title": "Buenas Prácticas - Python",
    "section": "",
    "text": "Una pregunta que surgue a menudo cuando uno se encuentra programando es saber cuál es la forma correcta de programar. La respuesta es que no existe la forma correcta de programar (ya sea en Python o cualquier otro lenguaje), sin embargo, existen estandares dentro del mundo de la programación, con el fin de hacer el código más legible, sencillo de entender y ayudar a encontrar posibles errores.\nEn esta sección se mostrará algunos conceptos sencillos que te ayudarán a mejorar tus skills en el desarrollo de software (con Python)."
  },
  {
    "objectID": "posts/2021/2021-08-31-buenas_practicas.html#estilo-de-codificación-pep8",
    "href": "posts/2021/2021-08-31-buenas_practicas.html#estilo-de-codificación-pep8",
    "title": "Buenas Prácticas - Python",
    "section": "",
    "text": "El PEP8 es un estilo de codificación que proporciona convenciones de codificación para el código Python que comprende la biblioteca estándar en la distribución principal de Python.\nAlgunos aspectos importantes:\n\nEl PEP8 y el PEP 257 (Docstring Conventions) fueron adaptados del ensayo original de la Guía de estilo Python de Guido, con algunas adiciones de la guía de estilo de Barry.\nEsta guía de estilo evoluciona con el tiempo a medida que se identifican convenciones adicionales y las convenciones pasadas se vuelven obsoletas debido a cambios en el propio lenguaje.\nMuchos proyectos tienen sus propias pautas de estilo de codificación. En caso de conflicto, dichas guías específicas del proyecto tienen prioridad para ese proyecto.\n\nBasados en el PEP8 y algunas buenas prácticas del diseño de software, veamos ejemplo para poder escribir de mejor forma nuestros códigos.\n\n\nCuando sea posible, define variables con nombres que tengan algún sentido o que puedas identificar fácilmente, no importa que sean más largas. Por ejemplo, en un programa podríamos escribir:\n\na = 10.  \nb = 3.5 \nprint(f\"El area es {a*b}\" )\n\nEl area es 35.0\n\n\npero, ¿qué significan a y b? lo sabemos por el comentario (bien hecho), pero si más adelante nos encontramos con esas variables, tendremos que recordar cual es cual. Es mejor usar nombres con significado:\n\naltura = 10.  \nbase = 3.5 \nprint(f\"El area es {a*b}\" )\n\nEl area es 35.0\n\n\n\n\n\nLas líneas de codigo no deben ser muy largas, como mucho 72 caracteres. Si se tiene una línea larga, se puede cortar con una barra invertida (\\) y continuar en la siguiente línea:\n\nprint(\"Esta es una frase muy larga, se puede cortar con un \\\n       y seguir en la línea inferior.\")\n\nEsta es una frase muy larga, se puede cortar con un        y seguir en la línea inferior.\n\n\n\n\n\nLos comentarios son muy importantes al escribir un programa. Describen lo que está sucediendo dentro de un programa, para que una persona que mira el código fuente no tenga dificultades para descifrarlo.\n\n#\n# esto es un comentario\nprint('Hola')\n\nHola\n\n\nTambién podemos tener comentarios multilíneas:\n\n#\n# Este es un comentario largo\n# y se extiende\n# a varias líneas\n\n\n\n\nLas importaciones generalmente deben estar en líneas separadas:\n\n#\n# no:\nimport sys, os\n\n\n#\n# si:\nimport os\nimport sys\n\n\n\n\nExisten varias formas de hacer comparaciones de objetos (principalmente en el uso del bucle if), acá se dejan alguna recomendaciones:\n# no\nif greeting == True:\n\n# no\nif greeting is True:\n# si\nif greeting:\n\n\n\nDentro de paréntesis, corchetes o llaves, no dejar espacios inmediatamente dentro de ellos:\n\n#\n# no\nlista_01 = [1, 2, 3,4, 5, 6,7, 8, 9,]\n\n\n#\n# si \nlista_01 = [\n    1, 2, 3,\n    4, 5, 6,\n    7, 8, 9, \n]\n\nAunque en Python se pueden hacer varias declaraciones en una línea, se recomienda hacer sólo una en cada línea:\n\n#\n# no\na = 10; b = 20\n\n\n#\n# si\na = 10\nb = 20  \n\nCuando se trabaja con lista, conjuntos y/o tuplas se recomienda poner en cada línea sus argumentos.\n\n#\n# no\nlista = [(1, 'hola'),(2, 'mundo'),]  \n\n\n#\n# si\nlista = [\n    (1, 'hola'),\n    (2, 'mundo'),\n]\n\nLo anterior se puede extender para funciones con muchos argumentos\n\n#\n# no\ndef funcion_01(x1,x2,x3,x4):\n    print(x1,x2,x3,x4)\n    \ndef funcion_02(\n    x1,x2,x3,x4):\n    print(x1,x2,x3,x4)\n\n\n#\n# si\ndef funcion_01(x1,x2,\n               x3,x4):\n    \n    print(x1,x2,x3,x4)\n    \ndef funcion_02(\n        x1,x2,\n        x3,x4):\n    \n    print(x1,x2,x3,x4)\n\n\n\n\nUn tema interesante es corresponde a la identación respecto a los operadores binarios, acá se muestra la forma correcta de hacerlo:\n# no\nincome = (gross_wages +\n          taxable_interest +\n          (dividends - qualified_dividends) -\n          ira_deduction -\n          student_loan_interest)\n# si\nincome = (gross_wages\n          + taxable_interest\n          + (dividends - qualified_dividends)\n          - ira_deduction\n          - student_loan_interest)\n\n\n\nAunque combinar iterables con elementos de control de flujo para manipular listas es muy sencillo con Python, hay métodos específicos más eficientes para hacer lo mismo. Pensemos el fitrado de datos de una lista:\n\n#\n# Seleccionar los números positivos\nnumeros = [-3, 2, 1, -8, -2, 7]\npositivos = []\nfor i in numeros:\n    if i &gt; 0:\n        positivos.append(i)\n        \nprint(f\"positivos: {positivos}\")\n\npositivos: [2, 1, 7]\n\n\nAunque técnicamente es correcto, es más eficiente hacer List Comprehension:\n\n#\n# comprension de lista\nnumeros = [-3, 2, 1, -8, -2, 7]\npositivos = [i for i in numeros if i &gt; 0] # List Comprehension\nprint(f\"positivos: {positivos}\")\n\npositivos: [2, 1, 7]\n\n\n\n\n\nCuando se ocupa try/except, es necesario especificar el tipo de error que se está cometiendo.\n\n#\n# importar librerias\nimport sys\n\n\n#\n# no\ntry:\n    r = 1/0\nexcept:\n    print(\"Oops! ocurrio un\",sys.exc_info()[0])\n\nOops! ocurrio un &lt;class 'ZeroDivisionError'&gt;\n\n\n\n#\n# si\ntry:\n    r = 1/0\nexcept ZeroDivisionError:\n    print(\"Oops! ocurrio un\",sys.exc_info()[0])\n\nOops! ocurrio un &lt;class 'ZeroDivisionError'&gt;\n\n\n\n\n\nSiempre es mejor definir las variables dentro de una función y no dejar variables globales.\n\n#\n# no\nvalor = 5\n\ndef funcion_01(variable):\n    return 2*variable + valor\n\n\nfuncion_01(2)\n\n9\n\n\n\n#\n# si\ndef funcion_01(variable,valor):\n    return 2*variable + valor\n\n\nfuncion_01(2,5)\n\n9\n\n\n\n\n\nCon Python 3 se puede especificar el tipo de parámetro y el tipo de retorno de una función (usando la notación PEP484 y PEP526. Se definen dos conceptos claves:\n\nEscritura dinámica: no se especifican los atributos de los inputs ni de los ouputs\nEscritura estática: se especifican los atributos de los inputs y los ouputs\n\n\n#\n# escritura dinámica\ndef suma(x,y):\n    return x+y\n\n\nprint(suma(1,2))\n\n3\n\n\n\n#\n# escritura estatica\ndef suma(x:float,\n         y:float)-&gt;float:\n    return x+y\n\n\nprint(suma(1,2))\n\n3\n\n\nPara la escritura estática, si bien se especifica el tipo de atributo (tanto de los inputs o outputs), la función puede recibir otros tipos de atributos.\n\nprint(suma(\"hola\",\" mundo\"))\n\nhola mundo\n\n\nPara validar los tipos de datos son los correctos, se deben ocupar librerías especializadas en la validación de datos (por ejemplo: pydantic).\n\n\n\nExisten librerías que pueden ayudar a corregir errores de escrituras en tú código (también conocido como Análisis Estático), por ejemplo:\n\nblack: El formateador de código inflexible.\nflake8: La herramienta para aplicar la guía de estilo PEP8.\nmypy: Mypy es un verificador de tipo estático para Python 3."
  },
  {
    "objectID": "posts/2021/2021-08-31-buenas_practicas.html#documentación",
    "href": "posts/2021/2021-08-31-buenas_practicas.html#documentación",
    "title": "Buenas Prácticas - Python",
    "section": "",
    "text": "Casi tan importante como la escritura de código, es su correcta documentación, una parte fundamental de cualquier programa que a menudo se infravalora o simplemente se ignora. Aparte de los comentarios entre el código explicando cómo funciona, el elemento básico de documentación de Python es el Docstring o cadena de documentación, que ya hemos visto. Simplemente es una cadena de texto con triple comillas que se coloca justo después de la definición de función o clase que sirve de documentación a ese elemento.\n\ndef potencia(x, y):\n    \"\"\"\n    Calcula la potencia arbitraria de un numero\n    \"\"\"\n    return x**y\n\n\n# Acceso a la documentación\npotencia.__doc__\n\n'\\n    Calcula la potencia arbitraria de un numero\\n    '\n\n\n\n# Acceso a la documentación\nhelp(potencia)\n\nHelp on function potencia in module __main__:\n\npotencia(x, y)\n    Calcula la potencia arbitraria de un numero\n\n\n\nLo correcto es detallar lo mejor posible en el Docstring qué hace y cómo se usa la función o clase y los parámetros que necesita. Se recomienda usar el estilo de documentación del software de documentación sphinx, que emplea reStructuredText como lenguaje de marcado.\nVeamos un ejemplo de una función bien documentada:\n\ndef potencia(x, y):\n    \"\"\"\n    Calcula la potencia arbitraria de un numero\n\n    :param x: base\n    :param y: exponente\n    :return:  potencia de un numero\n    :ejemplos:\n    \n    &gt;&gt;&gt; potencia(2, 1)\n    2\n    &gt;&gt;&gt; potencia(3, 2)\n    9\n    \"\"\"\n\n    return x**y\n\n\n# Acceso a la documentación\npotencia.__doc__\n\n'\\n    Calcula la potencia arbitraria de un numero\\n\\n    :param x: base\\n    :param y: exponente\\n    :return:  potencia de un numero\\n    :ejemplos:\\n    \\n    &gt;&gt;&gt; potencia(2, 1)\\n    2\\n    &gt;&gt;&gt; potencia(3, 2)\\n    9\\n    '\n\n\n\n# Acceso a la documentación\nhelp(potencia)\n\nHelp on function potencia in module __main__:\n\npotencia(x, y)\n    Calcula la potencia arbitraria de un numero\n    \n    :param x: base\n    :param y: exponente\n    :return:  potencia de un numero\n    :ejemplos:\n    \n    &gt;&gt;&gt; potencia(2, 1)\n    2\n    &gt;&gt;&gt; potencia(3, 2)\n    9\n\n\n\nExisten varias formas de documentar tus funciones, las principales encontradas en la literatura son: * Google docstrings: forma de documentación recomendada por Google.. * reStructured Text: estándar oficial de documentación de Python; No es apto para principiantes, pero tiene muchas funciones. * NumPy/SciPy docstrings: combinación de NumPy de reStructured y Google Docstrings."
  },
  {
    "objectID": "posts/2021/2021-08-31-buenas_practicas.html#zen-de-python",
    "href": "posts/2021/2021-08-31-buenas_practicas.html#zen-de-python",
    "title": "Buenas Prácticas - Python",
    "section": "",
    "text": "El Zen de Python te dará la guía para decidir sobre que hacer con tu código, no te dice como lo debes escribir, sino como debes pensar si estas programando en Python.\nPrincipios importantes:\n\nExplícito es mejor que implícito: Que no se asuma nada, asegúrate que las cosas sean.\nSimple es mejor que complejo: Evita código complejo, código espagueti o que hace mas cosas para poder hacer una simple tarea.\nPlano es mejor que anidado: Si tu código tiene mas de 3 niveles de identación, deberías mover parte de ese código a una función.\nLos errores nunca deberían pasar silenciosamente: No uses un Try/Except sin definir que tipo de error vas a cachar, viene de la mano con Explicito es mejor que implícito.\nSi la implementación es difícil de explicar, es mala idea.\n\nTambién, podemos ver el mensaje original del zen de python, ejecutando la siguiente linea de comando.\n\nimport this\n\nThe Zen of Python, by Tim Peters\n\nBeautiful is better than ugly.\nExplicit is better than implicit.\nSimple is better than complex.\nComplex is better than complicated.\nFlat is better than nested.\nSparse is better than dense.\nReadability counts.\nSpecial cases aren't special enough to break the rules.\nAlthough practicality beats purity.\nErrors should never pass silently.\nUnless explicitly silenced.\nIn the face of ambiguity, refuse the temptation to guess.\nThere should be one-- and preferably only one --obvious way to do it.\nAlthough that way may not be obvious at first unless you're Dutch.\nNow is better than never.\nAlthough never is often better than *right* now.\nIf the implementation is hard to explain, it's a bad idea.\nIf the implementation is easy to explain, it may be a good idea.\nNamespaces are one honking great idea -- let's do more of those!"
  },
  {
    "objectID": "posts/2021/2021-08-31-buenas_practicas.html#más-consejos",
    "href": "posts/2021/2021-08-31-buenas_practicas.html#más-consejos",
    "title": "Buenas Prácticas - Python",
    "section": "",
    "text": "Los consejos que se presentan son de mucha utilidad si usted quiere llevar sus conociminetos de programación al siguiente nivel, sin embargo, el contenido de cada uno amerita un curso por si solo. Se deja recomienda al lector seguir profundizando en estos temas.\n\n\nPython al ser multiparadigma, nos da una amplia gama de posibilidades de diseñar nuestros códigos. Dentro de estos se destacan:\n\nProgramación orientada a objetos (OOP)\nProgramación funcional\n\nCuándo ocupar uno o la otra, va a depender de cómo queremos abordar una determinada problemática, puesto que en la mayoría de los casos, se puede pasar de un paradigma a o otro (incluso mezclarlos de ser necesario).\n\n\n\nEn ingeniería de software, SOLID (Single responsibility, Open-closed, Liskov substitution, Interface segregation and Dependency inversion) es un acrónimo mnemónico introducido por Robert C. Martin a comienzos de la década del 2000 que representa cinco principios básicos de la programación orientada a objetos y el diseño. Cuando estos principios se aplican en conjunto es más probable que un desarrollador cree un sistema que sea fácil de mantener y ampliar con el tiempo.\nEn el siguiente link se deja una guía para poder entender estos conceptos en python.\n\n\n\nLos patrones de diseño son la base para la búsqueda de soluciones a problemas comunes en el desarrollo de software y otros ámbitos referentes al diseño de interacción o interfaces.\n\nUn patrón de diseño es una solución a un problema de diseño.\n\nSe destacan tres tipos de patrones de diseños:\n\nComportamiento\nCreacionales\nEstructurales\n\nEn el siguiente link se deja una guía para poder entender estos conceptos en python."
  },
  {
    "objectID": "posts/2021/2021-08-31-buenas_practicas.html#referencias",
    "href": "posts/2021/2021-08-31-buenas_practicas.html#referencias",
    "title": "Buenas Prácticas - Python",
    "section": "",
    "text": "The Clean Coder: A Code Of Conduct For Professional Programmers Robert C. Martin (2011)\nClean Code: A Handbook of Agile Software - Robert C. Martin (2009).\nWorking effectively with legacy code Michael C. Feathers (2004)\nRefactoring Martin Fowler (1999)\nThe Pragmatic Programmer Thomas Hunt (1999)"
  },
  {
    "objectID": "posts/2021/basic-analysis-impact-on-digital-learning.html",
    "href": "posts/2021/basic-analysis-impact-on-digital-learning.html",
    "title": "Impact on Digital Learning",
    "section": "",
    "text": "Main objective is understand of the best way the challenge LearnPlatform COVID-19 Impact on Digital Learning proposed by Kaggle.\nThe steps to follow are:\n\nOverview of the Dataset: Understanding the datasets available.\nPreprocessing: Preprocessing of the datasets available.\nEDA: Exploratory data analysis using visualization tools in Python.\n\n\nNote: My analysis is inspired by several of the notebooks that different profiles have uploaded to the challenge, so some graphics or images belong to these authors. The most important ones will be found in the references. On the other hand, my project is available in Jupyter Book, click in the following link.\n\n\n\nThe objective of this section is to be able to read and give an interpretation to each one of the available datasets, analyzing column by column. For each dataset we will make a brief description:\n\nFile: File name (.csv).\nShape: Dimensionality of datasets.\nDescription: Basic description of the dataset.\nTop 5 rows: Show first 5 rows + explanation for some columns.\nSummary: Summary of datasets.\n\n::: {#d55eb0e5 .cell _kg_hide-input=‘true’ execution=‘{“iopub.execute_input”:“2021-08-24T21:59:13.410181Z”,“iopub.status.busy”:“2021-08-24T21:59:13.316042Z”,“iopub.status.idle”:“2021-08-24T21:59:41.807446Z”,“shell.execute_reply”:“2021-08-24T21:59:41.806324Z”,“shell.execute_reply.started”:“2021-08-24T21:48:09.998997Z”}’ papermill=‘{“duration”:28.514116,“end_time”:“2021-08-24T21:59:41.807642”,“exception”:false,“start_time”:“2021-08-24T21:59:13.293526”,“status”:“completed”}’ tags=‘[“hide-input”]’ execution_count=1}\n# libraries\nimport glob\nimport re\n\n\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\nfrom wordcloud import WordCloud, STOPWORDS\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\n# read data\n\n## products dataset\npath = '../input/learnplatform-covid19-impact-on-digital-learning/'\nproducts_df = pd.read_csv(path + \"products_info.csv\")\nproducts_df.columns = [x.lower().replace(' ','_') for x in products_df.columns]\n\n## districts dataset\ndistricts_df = pd.read_csv(path +\"districts_info.csv\")\n#districts_df.state = districts_df.state.replace('District Of Columbia','District of Columbia')\n\n## engagement dataset\npath = '../input/learnplatform-covid19-impact-on-digital-learning/engagement_data/' \nall_files = glob.glob(path + \"/*.csv\")\n\nli = []\n\nfor filename in all_files:\n    df = pd.read_csv(filename, index_col=None, header=0)\n    district_id = filename.split(\"/\")[-1].split(\".\")[0]\n    df[\"district_id\"] = district_id\n    li.append(df)\nengagement_df = pd.concat(li)\nengagement_df = engagement_df.reset_index(drop=True)\n\n# summary\n\ndf_list = [\n    districts_df,\n    products_df,\n    engagement_df\n]\n\ndf_name = [\n    'districts_df',\n    'products_df',\n    'engagement_df'\n]\n\ncols = [\n    'dataframe',\n    'column', \n    'dtype', \n    'Non-Null Count', \n    'Null Count',\n    'unique'\n    \n]\n\nframes=[]\n\n\nfor i in range(len(df_list)):\n    df = df_list[i].copy()\n    a = df.dtypes.reset_index().rename(columns = {'index':'column',0:'dtype'})\n    b = df.count().reset_index().rename(columns = {'index':'column',0:'Non-Null Count'})\n    c = df.isnull().sum().reset_index().rename(columns = {'index':'column',0:'Null Count'})\n    temp = a.merge(b,on = 'column').merge(c,on = 'column')\n    \n    dct = {col: len(df[col].unique()) for col in df.columns}\n    df_unique = pd.DataFrame({\n    'column':dct.keys(),\n    'unique':dct.values(),\n    })\n    temp = temp.merge(df_unique,on = 'column')\n    temp['dataframe'] = df_name[i]\n    frames.append(temp)\n:::\n\n\n\nFile: districts_info.csv.\nShape: \\(233\\) rows \\(\\times\\) \\(7\\) columns.\nDescription: file contains information about each school district.\nTop 5 rows::\n\n\n\nSummary:\n\n\n\n\n\n\nFile: products_info.csv\nShape: \\(372\\) rows \\(\\times\\) \\(6\\) columns.\nDescription: for each school district, there is an additional file that contains the engagement for each tool for everyday in 2020.\nTop 5 rows:: \nSummary:\n\n\n\n\n\n\nFile: engagement_data/*.csv.\nShape: \\(22324190\\) rows \\(\\times\\) \\(5\\) columns.\nDescription: file contains information about each school district. The files can be joined by the key columns district_id and lp_id.\nTop 5 rows:: \nSummary:\n\n\n\n\n\n\nPreprocessing is an important step in any analytics competition. It helps you to handle your data more efficiently. However, please note that the way I preprocess the data may not be suited for your analysis purposes. Therefore, before you begin preprocessing your data, think about which data you would like to keep and/or modify and which data is not relevant for your analysis.\n\none-hot encoding the product sectors\nsplitting up the primary essential function into main and sub category\n\n\nNote: Preprocessing varies if you see other notebooks of this challenge. The processing will depend on the understanding of each of the datasets and the extra information that you may have.\n\n\n::: {#0578f18e .cell _kg_hide-input=‘true’ execution=‘{“iopub.execute_input”:“2021-08-24T21:59:42.315509Z”,“iopub.status.busy”:“2021-08-24T21:59:42.314913Z”,“iopub.status.idle”:“2021-08-24T22:00:00.669027Z”,“shell.execute_reply”:“2021-08-24T22:00:00.669504Z”,“shell.execute_reply.started”:“2021-08-24T21:48:44.902736Z”}’ papermill=‘{“duration”:18.384236,“end_time”:“2021-08-24T22:00:00.669700”,“exception”:false,“start_time”:“2021-08-24T21:59:42.285464”,“status”:“completed”}’ tags=‘[“hide-input”]’ execution_count=2}\n# products_df\n\nproducts_df['primary_function_main'] = products_df['primary_essential_function'].apply(lambda x: x.split(' - ')[0] if x == x else x)\nproducts_df['primary_function_sub'] = products_df['primary_essential_function'].apply(lambda x: x.split(' - ')[1] if x == x else x)\n\n# Synchronize similar values\nproducts_df['primary_function_sub'] = products_df['primary_function_sub'].replace({'Sites, Resources & References' : 'Sites, Resources & Reference'})\n#products_df.drop(\"Primary Essential Function\", axis=1, inplace=True)\n\ntemp_sectors = products_df['sector(s)'].str.get_dummies(sep=\"; \")\ntemp_sectors.columns = [f\"sector_{re.sub(' ', '', c)}\" for c in temp_sectors.columns]\nproducts_df = products_df.join(temp_sectors)\n#products_df.drop(\"Sector(s)\", axis=1, inplace=True)\n\n#del temp_sectors\n\n# engagement_df\n\nengagement_df['time'] = pd.to_datetime(engagement_df['time'])\n\ntemp = engagement_df[['time']].drop_duplicates('time')\ntemp['week'] = temp['time'].apply(lambda x: x.isocalendar()[1])\nengagement_df = engagement_df.merge(temp,on ='time')\n\nengagement_df['lp_id'] = engagement_df['lp_id'].fillna(-1).astype(int)\nengagement_df['district_id'] = engagement_df['district_id'].fillna(-1).astype(int)\n\nengagement_df_mix = engagement_df.merge(\n    districts_df[['district_id','state']],\n    on = 'district_id'\n)\nengagement_df_mix = engagement_df_mix.merge(\n    products_df[['lp_id','product_name','sector_Corporate', 'sector_HigherEd','sector_PreK-12']],\n    on = 'lp_id'\n)\n:::\n\n\n\nExploratory data analysis is the most important part of the challenge, since this will make the difference between the winner and the other participants. You should keep in mind that your visualizations must be able to simply and easily summarize the datasets. Also, it is hoped that the proposed visualizations can help to understand behaviors that are not easy to analyze with a simple table.\nVisualizations will be made in matplotlib, seaborn y plotly. Based on the article by Diverging Color Maps for Scientific Visualization (Expanded) - Kenneth Moreland, we will occupy Grays scale next to the technique: dark text on a light background.\n\nNote: Visualizations made on this notebook are static. You can use different tools to be able to make dynamic visualizations (Altair, plotly, etc.). You can also perform tools like Streamlit to make Dashboards. On the other hand, if you fully understand python visualization tools and have knowledge of HTML/CSS, you can make beautiful notebook presentations like this one.\n\n\n\nFirst of all, I am interested how diverse the available school districts are. As you can see in below plot, the available data does not cover all the states in the U.S. . The states with the most available school districts are CT (30) and UT (29) while there are also states with only one school district (FL, TN, NY, AZ).\n::: {#74a2ee97 .cell _kg_hide-input=‘true’ execution=‘{“iopub.execute_input”:“2021-08-24T22:00:00.788039Z”,“iopub.status.busy”:“2021-08-24T22:00:00.787339Z”,“iopub.status.idle”:“2021-08-24T22:00:00.944794Z”,“shell.execute_reply”:“2021-08-24T22:00:00.944141Z”,“shell.execute_reply.started”:“2021-08-24T21:49:15.148181Z”}’ papermill=‘{“duration”:0.179153,“end_time”:“2021-08-24T22:00:00.944932”,“exception”:false,“start_time”:“2021-08-24T22:00:00.765779”,“status”:“completed”}’ tags=‘[“hide-input”]’ execution_count=3}\n# map plot: districts\n\nus_state_abbrev = {\n    'Alabama': 'AL',\n    'Alaska': 'AK',\n    'American Samoa': 'AS',\n    'Arizona': 'AZ',\n    'Arkansas': 'AR',\n    'California': 'CA',\n    'Colorado': 'CO',\n    'Connecticut': 'CT',\n    'Delaware': 'DE',\n    'District Of Columbia': 'DC',\n    'Florida': 'FL',\n    'Georgia': 'GA',\n    'Guam': 'GU',\n    'Hawaii': 'HI',\n    'Idaho': 'ID',\n    'Illinois': 'IL',\n    'Indiana': 'IN',\n    'Iowa': 'IA',\n    'Kansas': 'KS',\n    'Kentucky': 'KY',\n    'Louisiana': 'LA',\n    'Maine': 'ME',\n    'Maryland': 'MD',\n    'Massachusetts': 'MA',\n    'Michigan': 'MI',\n    'Minnesota': 'MN',\n    'Mississippi': 'MS',\n    'Missouri': 'MO',\n    'Montana': 'MT',\n    'Nebraska': 'NE',\n    'Nevada': 'NV',\n    'New Hampshire': 'NH',\n    'New Jersey': 'NJ',\n    'New Mexico': 'NM',\n    'New York': 'NY',\n    'North Carolina': 'NC',\n    'North Dakota': 'ND',\n    'Northern Mariana Islands':'MP',\n    'Ohio': 'OH',\n    'Oklahoma': 'OK',\n    'Oregon': 'OR',\n    'Pennsylvania': 'PA',\n    'Puerto Rico': 'PR',\n    'Rhode Island': 'RI',\n    'South Carolina': 'SC',\n    'South Dakota': 'SD',\n    'Tennessee': 'TN',\n    'Texas': 'TX',\n    'Utah': 'UT',\n    'Vermont': 'VT',\n    'Virgin Islands': 'VI',\n    'Virginia': 'VA',\n    'Washington': 'WA',\n    'West Virginia': 'WV',\n    'Wisconsin': 'WI',\n    'Wyoming': 'WY'\n}\n\ndistricts_df['state_abbrev'] = districts_df['state'].replace(us_state_abbrev)\ndistricts_info_by_state = districts_df['state_abbrev'].value_counts().to_frame().reset_index(drop=False)\ndistricts_info_by_state.columns = ['state_abbrev', 'num_districts']\n\ntemp = pd.DataFrame({\n    'state_abbrev':us_state_abbrev.values(),\n})\n\ntemp = temp.merge(districts_info_by_state,on='state_abbrev',how='left').fillna(0)\ntemp['num_districts'] = temp['num_districts'].astype(int)\n\nfig = go.Figure()\nlayout = dict(\n    title_text = \"Number of Available School Districts per State\",\n    title_font_color=\"black\",\n    geo_scope='usa',\n)    \n\n\nfig.add_trace(\n    go.Choropleth(\n        locations=temp.state_abbrev,\n        zmax=1,\n        z = temp.num_districts,\n        locationmode = 'USA-states', # set of locations match entries in `locations`\n        marker_line_color='black',\n        geo='geo',\n        colorscale=px.colors.sequential.Greys, \n        \n    )\n)\n            \nfig.update_layout(layout)   \nfig.show()\n\n                                                \n\n:::\n::: {#7eac1448 .cell _kg_hide-input=‘true’ execution=‘{“iopub.execute_input”:“2021-08-24T22:00:01.006598Z”,“iopub.status.busy”:“2021-08-24T22:00:01.005538Z”,“iopub.status.idle”:“2021-08-24T22:00:01.460650Z”,“shell.execute_reply”:“2021-08-24T22:00:01.461172Z”,“shell.execute_reply.started”:“2021-08-24T21:49:15.338218Z”}’ papermill=‘{“duration”:0.494452,“end_time”:“2021-08-24T22:00:01.461333”,“exception”:false,“start_time”:“2021-08-24T22:00:00.966881”,“status”:“completed”}’ tags=‘[“hide-input”]’ execution_count=4}\n# bar plot: districts\n\nplt.style.use('default')\nplt.figure(figsize=(14,8))\n\nplotting = sns.countplot(\n    y=\"state\",\n    data=districts_df,\n    order=districts_df.state.value_counts().index,\n    palette=\"Greys_d\",\n    linewidth=3\n)\n\nfor container in plotting.containers:\n    plotting.bar_label(container,fontsize=16)\n\n#Text\nplotting.text(x = -5, y = -4.2, s = \"State Distribution\",fontsize = 24, weight = 'bold', alpha = .90);\nplotting.text(x = -5, y = -3, s = \"Distribution of United States\",fontsize = 16, alpha = .85)\nplotting.text(x = 31.2, y = 0.08, s = 'Highest', weight = 'bold',fontsize = 14)\nplotting.text(x = 1.7, y = 22.3, s = 'Lowest', weight = 'bold',fontsize = 14)\n\nplt.yticks(fontsize=14)\nplt.xticks(fontsize=14)\n\n\n\nplt.show()\n\n\n\n\n\n\n\n:::\n\n\n\nLocales are separated into 4 categories: Suburb,Rural, City and Town, where most of the locales are concentrated in the Suburb category (104).\nFor the pct_black/hispanic variable, Rural and Town categories concentrate their entire population close to the interval $ [0,0.2 [$, while for the others sectors this percentage is varied.\nFor pctfree/reduced and pp_total_raw indicators, the distribution for each location is different, although they tend to focus on a particular interval.\n::: {#a15318df .cell _kg_hide-input=‘true’ execution=‘{“iopub.execute_input”:“2021-08-24T22:00:01.557765Z”,“iopub.status.busy”:“2021-08-24T22:00:01.556848Z”,“iopub.status.idle”:“2021-08-24T22:00:03.160447Z”,“shell.execute_reply”:“2021-08-24T22:00:03.160919Z”,“shell.execute_reply.started”:“2021-08-24T21:49:15.865498Z”}’ papermill=‘{“duration”:1.630377,“end_time”:“2021-08-24T22:00:03.161103”,“exception”:false,“start_time”:“2021-08-24T22:00:01.530726”,“status”:“completed”}’ tags=‘[“hide-input”]’ execution_count=5}\n# heatmap: districts -&gt; locale\n\ntemp = districts_df.groupby('locale').pp_total_raw.value_counts().to_frame()\ntemp.columns = ['amount']\n\ntemp = temp.reset_index(drop=False)\n\ntemp = temp.pivot(index='locale', columns='pp_total_raw')['amount']\ntemp = temp[['[4000, 6000[', '[6000, 8000[', '[8000, 10000[', '[10000, 12000[',\n       '[12000, 14000[', '[14000, 16000[', '[16000, 18000[', \n       '[18000, 20000[', '[20000, 22000[', '[22000, 24000[', ]]\n\n\ntemp1 = districts_df.groupby('locale')['pct_black/hispanic'].value_counts().to_frame()\ntemp1.columns = ['amount']\n\ntemp1 = temp1.reset_index(drop=False)\ntemp1 = temp1.pivot(index='locale', columns='pct_black/hispanic')['amount']\n\ntemp2 = districts_df.groupby('locale')['pct_free/reduced'].value_counts().to_frame()\ntemp2.columns = ['amount']\n\ntemp2 = temp2.reset_index(drop=False)\n\ntemp2 = temp2.pivot(index='locale', columns='pct_free/reduced')['amount']\n\nplt.style.use('default')\n\nfig, [[ax1, ax2], [ax3, ax4]] = plt.subplots(nrows=2, ncols=2, figsize=(24,18))\n\nsns.countplot(data=districts_df, x='locale', ax=ax1, palette='Greys_d')\nax1.text(x = -0.5, y = 120, s = \"Locale Distribution\",fontsize = 24, weight = 'bold', alpha = .90);\nax1.xaxis.set_tick_params(labelsize=16)\n\nfor container in ax1.containers:\n    ax1.bar_label(container,fontsize=16)\n\nsns.heatmap(temp1.fillna(0), annot=True,  cmap='Greys', ax=ax2,annot_kws={\"fontsize\":14})\nax2.set_title('Heatmap: locale and pct_black/hispanic',fontsize=16,loc='left')\nax2.xaxis.set_tick_params(labelsize=16)\nax2.yaxis.set_tick_params(labelsize=16)\n\nsns.heatmap(temp.fillna(0), annot=True,  cmap='Greys', ax=ax3,annot_kws={\"fontsize\":14})\nax3.set_title('Heatmap: locale and pp_total_raw',fontsize=16,loc='left')\nax3.xaxis.set_tick_params(labelsize=16)\nax3.yaxis.set_tick_params(labelsize=16)\n\nsns.heatmap(temp2.fillna(0), annot=True,  cmap='Greys', ax=ax4,annot_kws={\"fontsize\":14})\nax4.set_title('Heatmap: locale and pct_free/reduced',fontsize=16,loc='left')\nax4.xaxis.set_tick_params(labelsize=16)\nax4.yaxis.set_tick_params(labelsize=16)\n\n\nplt.show()\n\n\n\n\n\n\n\n:::\n\n\n\nSectors are separated into 3 categories: sector_Corporate, sector_HigherEd and sector_PreK-12, donde la categoría mayoritaria corresponde a sector_PreK-12 (350). On the other hand, analyzing the primary_function_main variable, all sectors are focused on theLC category. It is worth mentioning that the distribution of the other categories remains almost the same between sectors.\n::: {#84e6e3f2 .cell _kg_hide-input=‘true’ execution=‘{“iopub.execute_input”:“2021-08-24T22:00:03.301514Z”,“iopub.status.busy”:“2021-08-24T22:00:03.292826Z”,“iopub.status.idle”:“2021-08-24T22:00:03.796850Z”,“shell.execute_reply”:“2021-08-24T22:00:03.797314Z”,“shell.execute_reply.started”:“2021-08-24T21:49:17.574316Z”}’ papermill=‘{“duration”:0.551066,“end_time”:“2021-08-24T22:00:03.797481”,“exception”:false,“start_time”:“2021-08-24T22:00:03.246415”,“status”:“completed”}’ tags=‘[“hide-input”]’ execution_count=6}\nplt.style.use('default')\nnames = ['sector_Corporate', 'sector_HigherEd','sector_PreK-12']\ncounts = [products_df[x].sum() for x in names]\n\ntemp_bar = pd.DataFrame({\n    'sector':names,\n    'count':counts\n}).sort_values('count',ascending = False)\n\ntemp = products_df.groupby('primary_function_main')[names].sum()\n\n#fig, [ax1, ax2 ]= plt.subplots(nrows=1, ncols=2, figsize=(12,6))\nplt.figure(figsize=(18,18))\n\nplt.subplot(3,2,1)\nax = sns.barplot(x=\"sector\", y=\"count\", data=temp_bar,palette ='Greys_d')\nfor container in ax.containers:\n    ax.bar_label(container,fontsize=12)\n\nplt.subplot(3,2,2)\n\nsns.heatmap(temp.T, annot=True,  cmap='Greys',annot_kws={\"fontsize\":10},fmt='g')\n\nplt.text(x = -6, y = -0.25, s = \"Sectors Distribution\",fontsize = 18, weight = 'bold', alpha = .90);\n\nplt.show()\n\n\n\n\n\n\n\n:::\n\n\n\nContinuing the analysis of the primary_function_main variable, it was observed that most of these are in theLC category (77%). Within this category, its subcategory is analyzed, where the predominant subcategory is Sites, Resources & Reference (101).\n::: {#c0c96d39 .cell _kg_hide-input=‘true’ execution=‘{“iopub.execute_input”:“2021-08-24T22:00:03.924145Z”,“iopub.status.busy”:“2021-08-24T22:00:03.923467Z”,“iopub.status.idle”:“2021-08-24T22:00:04.076527Z”,“shell.execute_reply”:“2021-08-24T22:00:04.077022Z”,“shell.execute_reply.started”:“2021-08-24T21:49:18.054662Z”}’ papermill=‘{“duration”:0.190305,“end_time”:“2021-08-24T22:00:04.077195”,“exception”:false,“start_time”:“2021-08-24T22:00:03.886890”,“status”:“completed”}’ tags=‘[“hide-input”]’ execution_count=7}\n# pieplot: products\n\ncolor = [\n    'darkgray',\n    'silver',\n    'lightgray',\n    'gainsboro',\n]\n\nproducts_df[\"primary_function_main\"].value_counts().plot(\n    kind = 'pie', \n    autopct='%1d%%', \n    figsize=(6,6), \n    colors=color,\n    wedgeprops={\"edgecolor\":\"k\",'linewidth': 0.8,},\n    textprops={'color':\"black\"},\n    startangle=0)\nplt.text(x = -1.4, y = 1.1, s = \"Categories\",fontsize = 18, weight = 'bold', alpha = .90);\nplt.show()\n\n\n\n\n\n\n\n:::\n::: {#c52db360 .cell _kg_hide-input=‘true’ execution=‘{“iopub.execute_input”:“2021-08-24T22:00:04.155626Z”,“iopub.status.busy”:“2021-08-24T22:00:04.153843Z”,“iopub.status.idle”:“2021-08-24T22:00:04.430687Z”,“shell.execute_reply”:“2021-08-24T22:00:04.431156Z”,“shell.execute_reply.started”:“2021-08-24T21:49:18.238340Z”}’ papermill=‘{“duration”:0.322608,“end_time”:“2021-08-24T22:00:04.431319”,“exception”:false,“start_time”:“2021-08-24T22:00:04.108711”,“status”:“completed”}’ tags=‘[“hide-input”]’ execution_count=8}\n# pieplot: products -&gt; subcategories\n\nplt.style.use('default')\nplt.figure(figsize=(18,8))\n\ntemp = products_df[products_df.primary_function_main == 'LC']\nax = sns.countplot(\n    data=temp, \n    y='primary_function_sub',\n    order=temp.primary_function_sub.value_counts().index,\n    palette ='Greys_d'\n)\n\nfor container in ax.containers:\n    ax.bar_label(container,fontsize=16)\n\n\n#plt.title('Sub-Categories in Primary Function LC')\nplt.text(x = -50, y = -0.8, \n         s = \"Sub-Categories in Primary Function LC\",fontsize = 24, weight = 'bold', alpha = .90);\n\nplt.text(x = 105, y =0.08, s = 'Highest', weight = 'bold',fontsize=16)\nplt.text(x = 7, y = 6, s = 'Lowest', weight = 'bold',fontsize=16)\nplt.yticks(fontsize=16)\nplt.xticks(fontsize=16)\n\nplt.show()\n\n\n\n\n\n\n\n:::\n\n\n\nAfter understanding the functionality of each of the tools, it is necessary to understand the distribution of the tools. The first thing is to study the distribution of the providers of the products we have, where:\n\n258 providers have 1 occurrences.\n18 providers have 2 occurrences.\n9 providers have 3 occurrences.\n2 providers have 4 occurrences.\n2 providers have 6 occurrences.\n1 provider have 30 occurrences.\n\nBased on this, only the top 15 providers will be displayed.\n::: {#8cf15baa .cell _kg_hide-input=‘true’ execution=‘{“iopub.execute_input”:“2021-08-24T22:00:04.610677Z”,“iopub.status.busy”:“2021-08-24T22:00:04.586018Z”,“iopub.status.idle”:“2021-08-24T22:00:04.945261Z”,“shell.execute_reply”:“2021-08-24T22:00:04.945715Z”,“shell.execute_reply.started”:“2021-08-24T21:49:18.515334Z”}’ papermill=‘{“duration”:0.414758,“end_time”:“2021-08-24T22:00:04.945874”,“exception”:false,“start_time”:“2021-08-24T22:00:04.531116”,“status”:“completed”}’ tags=‘[“hide-input”]’ execution_count=9}\ndct = {\n    'Savvas Learning Company | Formerly Pearson K12 Learning': 'Savvas Learning Company'\n}\n\ntemp = products_df['provider/company_name'].value_counts().reset_index()\ntemp.columns = ['provider/company_name','count']\ntemp = temp.replace( {\n    'Savvas Learning Company | Formerly Pearson K12 Learning': 'Savvas Learning Company'\n})\n\nn = 15\ntemp = temp.sort_values('count',ascending = False).head(n)\n\nplt.style.use('default')\nplt.figure(figsize=(18,8))\n\nax = sns.barplot(\n    data=temp, \n    y='provider/company_name',\n    x='count',\n    palette ='Greys_d'\n)\n\nfor container in ax.containers:\n    ax.bar_label(container,fontsize=15)\n    \nplt.text(x = -7, y = -1, \n         s = f\"Top {n} provider/company name\",fontsize = 20, weight = 'bold', alpha = .90);\n   \nplt.text(x = 31, y =0.08, s = 'Highest', weight = 'bold',fontsize=16)\nplt.text(x = 3, y = 14.2, s = 'Lowest', weight = 'bold',fontsize=16)\nplt.yticks(fontsize=16)\n\n\nplt.yticks(fontsize=16)\nplt.xticks(fontsize=16)\nplt.show()\n\n\n\n\n\n\n\n:::\nWith regard to products, there are about 372 different products.\nWe can make a word cloud to be able to analyze in a different way, words by themselves that are repeated the most in the product_name variable.\n::: {#53d5558a .cell _kg_hide-input=‘true’ execution=‘{“iopub.execute_input”:“2021-08-24T22:00:05.106485Z”,“iopub.status.busy”:“2021-08-24T22:00:05.104433Z”,“iopub.status.idle”:“2021-08-24T22:00:06.068287Z”,“shell.execute_reply”:“2021-08-24T22:00:06.068956Z”,“shell.execute_reply.started”:“2021-08-24T21:49:18.935625Z”}’ papermill=‘{“duration”:1.012728,“end_time”:“2021-08-24T22:00:06.069135”,“exception”:false,“start_time”:“2021-08-24T22:00:05.056407”,“status”:“completed”}’ tags=‘[“hide-input”]’ execution_count=10}\ncloud = WordCloud(\n    width=1080,\n    height=270,\n    colormap='Greys',\n    background_color='white'\n    ).generate(\" \".join(products_df['product_name'].astype(str)))\n\nplt.figure(figsize=(22, 10))\nplt.imshow(cloud)\nplt.axis('off');\n\n\n\n\n\n\n\n:::\nTo understand more in detail the use of these products, we will analyze the use of these products with respect to the variable engagement_index. The first graph is related to the average engagement_index (per student) for the year 2020, where the first 15 products will be displayed.\nAn important fact is that 362 products have an average of less than 1!.\n::: {#4c2d24bf .cell _kg_hide-input=‘true’ execution=‘{“iopub.execute_input”:“2021-08-24T22:00:06.276148Z”,“iopub.status.busy”:“2021-08-24T22:00:06.275229Z”,“iopub.status.idle”:“2021-08-24T22:00:07.642119Z”,“shell.execute_reply”:“2021-08-24T22:00:07.642568Z”,“shell.execute_reply.started”:“2021-08-24T21:49:19.888566Z”}’ papermill=‘{“duration”:1.424686,“end_time”:“2021-08-24T22:00:07.642750”,“exception”:false,“start_time”:“2021-08-24T22:00:06.218064”,“status”:“completed”}’ tags=‘[“hide-input”]’ execution_count=11}\ngroup_01 = (engagement_df_mix.groupby('product_name')['engagement_index'].mean()/1000).reset_index().sort_values('engagement_index',ascending = False)\ngroup_01['engagement_index'] = group_01['engagement_index'].apply(lambda x: round(x,2))\nless_1 = len(group_01.loc[lambda x:x['engagement_index']&lt;1])\n\n\nplt.style.use('default')\nplt.figure(figsize=(14,8))\n\nplotting = sns.barplot(\n    y=\"product_name\",\n    x = \"engagement_index\",\n    data=group_01.head(20),\n    palette=\"Greys_d\",\n\n)\n\nfor container in plotting.containers:\n    plotting.bar_label(container,fontsize=14)\n\nplt.text(x = -3.5, y = -3, \n         s = \"Mean daily page-load events in top 20 tools\",fontsize = 20, weight = 'bold', alpha = .90);\n\nplt.text(x = -3.5, y = -2, \n         s = \"per 1 student\",fontsize = 14,  alpha = .90);\n\nplt.text(x = 11, y =0.1, s = 'Highest', weight = 'bold',fontsize=14)\nplt.text(x = 1, y = 19.2, s = 'Lowest', weight = 'bold',fontsize=14)\nplt.yticks(fontsize=16)\nplt.xticks(fontsize=16)\nplt.show()\n\n\n\n\n\n\n\n:::\nLet’s study the temporal behavior (at the level of weeks) of these tools during the year 2020, where the most three used tools will be shown with different colors, while the other tools will be visualized but with the same color (in order to understand their distribution).\n\nNote: The proposed analysis can be carried out at the day level and analyzing through time series each of the tools during the year 2020.\n\n::: {#9d39e70f .cell _kg_hide-input=‘true’ execution=‘{“iopub.execute_input”:“2021-08-24T22:00:08.316803Z”,“iopub.status.busy”:“2021-08-24T22:00:08.316106Z”,“iopub.status.idle”:“2021-08-24T22:00:13.858922Z”,“shell.execute_reply”:“2021-08-24T22:00:13.859377Z”,“shell.execute_reply.started”:“2021-08-24T21:49:21.992666Z”}’ papermill=‘{“duration”:6.060783,“end_time”:“2021-08-24T22:00:13.859531”,“exception”:false,“start_time”:“2021-08-24T22:00:07.798748”,“status”:“completed”}’ tags=‘[“hide-input”]’ execution_count=12}\ncol = 'week'\n\ngroup_04  = (engagement_df_mix.groupby(['product_name',col])['engagement_index'].mean()/1000).reset_index().sort_values('engagement_index',ascending = False)\n\ng_high = group_01.head(3)['product_name']\ngroup_04_top = group_04.loc[lambda x: x.product_name.isin(g_high)]\n\nstates = group_04['product_name'].unique()\ntimes= group_04[col].unique()\n\nindex = pd.MultiIndex.from_product([states,times], names = [\"product_name\", col])\n\ndf_complete = pd.DataFrame(index = index).reset_index().fillna(0)\n\ngroup_04 = df_complete.merge(group_04,on = ['product_name',col],how='left').fillna(0)\n\nn = 3\ng_high = group_04.groupby('product_name')['engagement_index'].sum().sort_values(ascending=False).head(n).index.to_list()\n\n\ncolors = [    \n    'lightgray', \n    'dimgray', \n    'black', \n    'firebrick', \n    'darkred']\npalette_01 = {x:'lavender' for x in group_04['product_name'].unique() if x not in g_high}\npalette_02 = {g_high[i]:colors[i] for i in range(n)}\n\nplt.style.use('default')\nplt.figure(figsize=(20,6))\n\n\nsns.lineplot(\n    data=group_04.loc[lambda x: ~x.product_name.isin(g_high)], \n    x=col, \n    y=\"engagement_index\", \n    hue='product_name',\n    legend = False,\n    palette=palette_01,\n    linewidth = 1.\n\n    )\n\nsns.lineplot(\n    data=group_04.loc[lambda x: x.product_name.isin(g_high)], \n    x=col, \n    y=\"engagement_index\", \n    hue='product_name',\n    palette=palette_02,\n    linewidth = 1.\n\n    )\n\n\nplt.text(x = -2, y =23.7, s = 'Mean daily page-load events in top 3 tools', weight = 'bold',fontsize=14)\nplt.text(x = -2, y =22.3, s = 'by products and time, per 1 student',fontsize=12)\n\n\nplt.text(x = 12, y =20.7, s = '1,000 cases of COVID', weight = 'bold',fontsize=8)\nplt.text(x = 37, y =20.7, s = '1st September', weight = 'bold',fontsize=8)\n\n\nplt.axvline(x = 11, color = 'black', linestyle='--',linewidth = 0.5)\nplt.axvline(x = 36, color = 'black', linestyle='--',linewidth = 0.5)\nplt.show()\n\n\n\n\n\n\n\n:::\nNow, we can understand the engagement index for the most important tools about districts, where the districts of * Wisconsin ,  Missouri * and * Virginia * have the highest engagement index among the three most used tools.\n::: {#d1e1ba17 .cell _kg_hide-input=‘true’ execution=‘{“iopub.execute_input”:“2021-08-24T22:00:14.637147Z”,“iopub.status.busy”:“2021-08-24T22:00:14.636457Z”,“iopub.status.idle”:“2021-08-24T22:00:17.454954Z”,“shell.execute_reply”:“2021-08-24T22:00:17.455400Z”,“shell.execute_reply.started”:“2021-08-24T21:49:29.563655Z”}’ papermill=‘{“duration”:3.421243,“end_time”:“2021-08-24T22:00:17.455572”,“exception”:false,“start_time”:“2021-08-24T22:00:14.034329”,“status”:“completed”}’ tags=‘[“hide-input”]’ execution_count=13}\ngroup_02 = (engagement_df_mix.groupby(['state','product_name'])['engagement_index'].mean()/1000)\\\n            .reset_index().sort_values('engagement_index',ascending = False).fillna(0)\n\ngripo_02_top = group_02.loc[lambda x: x.product_name.isin(g_high)]\ngripo_02_top['engagement_index'] = gripo_02_top['engagement_index'].apply(lambda x: round(x,2))\n#gripo_02_top = gripo_02_top.loc[lambda x: x['engagement_index']&gt;0]\n\n\nplt.style.use('default')\n\ng = sns.FacetGrid(gripo_02_top,hue='product_name',col = 'product_name',height=4, col_wrap= 3  )\ng.map(sns.barplot, \"engagement_index\",\"state\", palette=\"Greys_d\",)\n\ng.fig.set_size_inches(15, 8)\ng.fig.subplots_adjust(top=0.81, right=0.86)\n\naxes = g.axes.flatten()\nfor ax in axes:\n    for container in ax.containers:\n        ax.bar_label(container,fontsize=8)\n\n\nplt.text(x = -50, y = -4, s = \"Mean daily page-load events in top 3 tools\",fontsize = 16, weight = 'bold', alpha = .90);\nplt.text(x = -50, y = -3, s = \"by state and products, per 1 student\",fontsize = 14,  alpha = .90);\nplt.show()\n\n\n\n\n\n\n\n:::\n\n\n\n\n\nDepending on what you want to achieve you might want to carefully preselect districts. Note that we approach in this notebook might not necessarily suit your individual purposes.\nWhen looking at digital learning, you might want to spend sometime in figuring out which districts actually applied digital learning\n\n\n\n\n\nDiverging Color Maps for Scientific Visualization (Expanded) - Kenneth Moreland\nKaggle Competitions:\n\nEnthusiast to Data Professional - What changes?\nHow To Approach Analytics Challenges\nMost popular tools in 2020 Digital Learning"
  },
  {
    "objectID": "posts/2021/basic-analysis-impact-on-digital-learning.html#overview-of-the-dataset",
    "href": "posts/2021/basic-analysis-impact-on-digital-learning.html#overview-of-the-dataset",
    "title": "Impact on Digital Learning",
    "section": "",
    "text": "The objective of this section is to be able to read and give an interpretation to each one of the available datasets, analyzing column by column. For each dataset we will make a brief description:\n\nFile: File name (.csv).\nShape: Dimensionality of datasets.\nDescription: Basic description of the dataset.\nTop 5 rows: Show first 5 rows + explanation for some columns.\nSummary: Summary of datasets.\n\n::: {#d55eb0e5 .cell _kg_hide-input=‘true’ execution=‘{“iopub.execute_input”:“2021-08-24T21:59:13.410181Z”,“iopub.status.busy”:“2021-08-24T21:59:13.316042Z”,“iopub.status.idle”:“2021-08-24T21:59:41.807446Z”,“shell.execute_reply”:“2021-08-24T21:59:41.806324Z”,“shell.execute_reply.started”:“2021-08-24T21:48:09.998997Z”}’ papermill=‘{“duration”:28.514116,“end_time”:“2021-08-24T21:59:41.807642”,“exception”:false,“start_time”:“2021-08-24T21:59:13.293526”,“status”:“completed”}’ tags=‘[“hide-input”]’ execution_count=1}\n# libraries\nimport glob\nimport re\n\n\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\nfrom wordcloud import WordCloud, STOPWORDS\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\n# read data\n\n## products dataset\npath = '../input/learnplatform-covid19-impact-on-digital-learning/'\nproducts_df = pd.read_csv(path + \"products_info.csv\")\nproducts_df.columns = [x.lower().replace(' ','_') for x in products_df.columns]\n\n## districts dataset\ndistricts_df = pd.read_csv(path +\"districts_info.csv\")\n#districts_df.state = districts_df.state.replace('District Of Columbia','District of Columbia')\n\n## engagement dataset\npath = '../input/learnplatform-covid19-impact-on-digital-learning/engagement_data/' \nall_files = glob.glob(path + \"/*.csv\")\n\nli = []\n\nfor filename in all_files:\n    df = pd.read_csv(filename, index_col=None, header=0)\n    district_id = filename.split(\"/\")[-1].split(\".\")[0]\n    df[\"district_id\"] = district_id\n    li.append(df)\nengagement_df = pd.concat(li)\nengagement_df = engagement_df.reset_index(drop=True)\n\n# summary\n\ndf_list = [\n    districts_df,\n    products_df,\n    engagement_df\n]\n\ndf_name = [\n    'districts_df',\n    'products_df',\n    'engagement_df'\n]\n\ncols = [\n    'dataframe',\n    'column', \n    'dtype', \n    'Non-Null Count', \n    'Null Count',\n    'unique'\n    \n]\n\nframes=[]\n\n\nfor i in range(len(df_list)):\n    df = df_list[i].copy()\n    a = df.dtypes.reset_index().rename(columns = {'index':'column',0:'dtype'})\n    b = df.count().reset_index().rename(columns = {'index':'column',0:'Non-Null Count'})\n    c = df.isnull().sum().reset_index().rename(columns = {'index':'column',0:'Null Count'})\n    temp = a.merge(b,on = 'column').merge(c,on = 'column')\n    \n    dct = {col: len(df[col].unique()) for col in df.columns}\n    df_unique = pd.DataFrame({\n    'column':dct.keys(),\n    'unique':dct.values(),\n    })\n    temp = temp.merge(df_unique,on = 'column')\n    temp['dataframe'] = df_name[i]\n    frames.append(temp)\n:::\n\n\n\nFile: districts_info.csv.\nShape: \\(233\\) rows \\(\\times\\) \\(7\\) columns.\nDescription: file contains information about each school district.\nTop 5 rows::\n\n\n\nSummary:\n\n\n\n\n\n\nFile: products_info.csv\nShape: \\(372\\) rows \\(\\times\\) \\(6\\) columns.\nDescription: for each school district, there is an additional file that contains the engagement for each tool for everyday in 2020.\nTop 5 rows:: \nSummary:\n\n\n\n\n\n\nFile: engagement_data/*.csv.\nShape: \\(22324190\\) rows \\(\\times\\) \\(5\\) columns.\nDescription: file contains information about each school district. The files can be joined by the key columns district_id and lp_id.\nTop 5 rows:: \nSummary:"
  },
  {
    "objectID": "posts/2021/basic-analysis-impact-on-digital-learning.html#preprocessing",
    "href": "posts/2021/basic-analysis-impact-on-digital-learning.html#preprocessing",
    "title": "Impact on Digital Learning",
    "section": "",
    "text": "Preprocessing is an important step in any analytics competition. It helps you to handle your data more efficiently. However, please note that the way I preprocess the data may not be suited for your analysis purposes. Therefore, before you begin preprocessing your data, think about which data you would like to keep and/or modify and which data is not relevant for your analysis.\n\none-hot encoding the product sectors\nsplitting up the primary essential function into main and sub category\n\n\nNote: Preprocessing varies if you see other notebooks of this challenge. The processing will depend on the understanding of each of the datasets and the extra information that you may have.\n\n\n::: {#0578f18e .cell _kg_hide-input=‘true’ execution=‘{“iopub.execute_input”:“2021-08-24T21:59:42.315509Z”,“iopub.status.busy”:“2021-08-24T21:59:42.314913Z”,“iopub.status.idle”:“2021-08-24T22:00:00.669027Z”,“shell.execute_reply”:“2021-08-24T22:00:00.669504Z”,“shell.execute_reply.started”:“2021-08-24T21:48:44.902736Z”}’ papermill=‘{“duration”:18.384236,“end_time”:“2021-08-24T22:00:00.669700”,“exception”:false,“start_time”:“2021-08-24T21:59:42.285464”,“status”:“completed”}’ tags=‘[“hide-input”]’ execution_count=2}\n# products_df\n\nproducts_df['primary_function_main'] = products_df['primary_essential_function'].apply(lambda x: x.split(' - ')[0] if x == x else x)\nproducts_df['primary_function_sub'] = products_df['primary_essential_function'].apply(lambda x: x.split(' - ')[1] if x == x else x)\n\n# Synchronize similar values\nproducts_df['primary_function_sub'] = products_df['primary_function_sub'].replace({'Sites, Resources & References' : 'Sites, Resources & Reference'})\n#products_df.drop(\"Primary Essential Function\", axis=1, inplace=True)\n\ntemp_sectors = products_df['sector(s)'].str.get_dummies(sep=\"; \")\ntemp_sectors.columns = [f\"sector_{re.sub(' ', '', c)}\" for c in temp_sectors.columns]\nproducts_df = products_df.join(temp_sectors)\n#products_df.drop(\"Sector(s)\", axis=1, inplace=True)\n\n#del temp_sectors\n\n# engagement_df\n\nengagement_df['time'] = pd.to_datetime(engagement_df['time'])\n\ntemp = engagement_df[['time']].drop_duplicates('time')\ntemp['week'] = temp['time'].apply(lambda x: x.isocalendar()[1])\nengagement_df = engagement_df.merge(temp,on ='time')\n\nengagement_df['lp_id'] = engagement_df['lp_id'].fillna(-1).astype(int)\nengagement_df['district_id'] = engagement_df['district_id'].fillna(-1).astype(int)\n\nengagement_df_mix = engagement_df.merge(\n    districts_df[['district_id','state']],\n    on = 'district_id'\n)\nengagement_df_mix = engagement_df_mix.merge(\n    products_df[['lp_id','product_name','sector_Corporate', 'sector_HigherEd','sector_PreK-12']],\n    on = 'lp_id'\n)\n:::"
  },
  {
    "objectID": "posts/2021/basic-analysis-impact-on-digital-learning.html#eda",
    "href": "posts/2021/basic-analysis-impact-on-digital-learning.html#eda",
    "title": "Impact on Digital Learning",
    "section": "",
    "text": "Exploratory data analysis is the most important part of the challenge, since this will make the difference between the winner and the other participants. You should keep in mind that your visualizations must be able to simply and easily summarize the datasets. Also, it is hoped that the proposed visualizations can help to understand behaviors that are not easy to analyze with a simple table.\nVisualizations will be made in matplotlib, seaborn y plotly. Based on the article by Diverging Color Maps for Scientific Visualization (Expanded) - Kenneth Moreland, we will occupy Grays scale next to the technique: dark text on a light background.\n\nNote: Visualizations made on this notebook are static. You can use different tools to be able to make dynamic visualizations (Altair, plotly, etc.). You can also perform tools like Streamlit to make Dashboards. On the other hand, if you fully understand python visualization tools and have knowledge of HTML/CSS, you can make beautiful notebook presentations like this one.\n\n\n\nFirst of all, I am interested how diverse the available school districts are. As you can see in below plot, the available data does not cover all the states in the U.S. . The states with the most available school districts are CT (30) and UT (29) while there are also states with only one school district (FL, TN, NY, AZ).\n::: {#74a2ee97 .cell _kg_hide-input=‘true’ execution=‘{“iopub.execute_input”:“2021-08-24T22:00:00.788039Z”,“iopub.status.busy”:“2021-08-24T22:00:00.787339Z”,“iopub.status.idle”:“2021-08-24T22:00:00.944794Z”,“shell.execute_reply”:“2021-08-24T22:00:00.944141Z”,“shell.execute_reply.started”:“2021-08-24T21:49:15.148181Z”}’ papermill=‘{“duration”:0.179153,“end_time”:“2021-08-24T22:00:00.944932”,“exception”:false,“start_time”:“2021-08-24T22:00:00.765779”,“status”:“completed”}’ tags=‘[“hide-input”]’ execution_count=3}\n# map plot: districts\n\nus_state_abbrev = {\n    'Alabama': 'AL',\n    'Alaska': 'AK',\n    'American Samoa': 'AS',\n    'Arizona': 'AZ',\n    'Arkansas': 'AR',\n    'California': 'CA',\n    'Colorado': 'CO',\n    'Connecticut': 'CT',\n    'Delaware': 'DE',\n    'District Of Columbia': 'DC',\n    'Florida': 'FL',\n    'Georgia': 'GA',\n    'Guam': 'GU',\n    'Hawaii': 'HI',\n    'Idaho': 'ID',\n    'Illinois': 'IL',\n    'Indiana': 'IN',\n    'Iowa': 'IA',\n    'Kansas': 'KS',\n    'Kentucky': 'KY',\n    'Louisiana': 'LA',\n    'Maine': 'ME',\n    'Maryland': 'MD',\n    'Massachusetts': 'MA',\n    'Michigan': 'MI',\n    'Minnesota': 'MN',\n    'Mississippi': 'MS',\n    'Missouri': 'MO',\n    'Montana': 'MT',\n    'Nebraska': 'NE',\n    'Nevada': 'NV',\n    'New Hampshire': 'NH',\n    'New Jersey': 'NJ',\n    'New Mexico': 'NM',\n    'New York': 'NY',\n    'North Carolina': 'NC',\n    'North Dakota': 'ND',\n    'Northern Mariana Islands':'MP',\n    'Ohio': 'OH',\n    'Oklahoma': 'OK',\n    'Oregon': 'OR',\n    'Pennsylvania': 'PA',\n    'Puerto Rico': 'PR',\n    'Rhode Island': 'RI',\n    'South Carolina': 'SC',\n    'South Dakota': 'SD',\n    'Tennessee': 'TN',\n    'Texas': 'TX',\n    'Utah': 'UT',\n    'Vermont': 'VT',\n    'Virgin Islands': 'VI',\n    'Virginia': 'VA',\n    'Washington': 'WA',\n    'West Virginia': 'WV',\n    'Wisconsin': 'WI',\n    'Wyoming': 'WY'\n}\n\ndistricts_df['state_abbrev'] = districts_df['state'].replace(us_state_abbrev)\ndistricts_info_by_state = districts_df['state_abbrev'].value_counts().to_frame().reset_index(drop=False)\ndistricts_info_by_state.columns = ['state_abbrev', 'num_districts']\n\ntemp = pd.DataFrame({\n    'state_abbrev':us_state_abbrev.values(),\n})\n\ntemp = temp.merge(districts_info_by_state,on='state_abbrev',how='left').fillna(0)\ntemp['num_districts'] = temp['num_districts'].astype(int)\n\nfig = go.Figure()\nlayout = dict(\n    title_text = \"Number of Available School Districts per State\",\n    title_font_color=\"black\",\n    geo_scope='usa',\n)    \n\n\nfig.add_trace(\n    go.Choropleth(\n        locations=temp.state_abbrev,\n        zmax=1,\n        z = temp.num_districts,\n        locationmode = 'USA-states', # set of locations match entries in `locations`\n        marker_line_color='black',\n        geo='geo',\n        colorscale=px.colors.sequential.Greys, \n        \n    )\n)\n            \nfig.update_layout(layout)   \nfig.show()\n\n                                                \n\n:::\n::: {#7eac1448 .cell _kg_hide-input=‘true’ execution=‘{“iopub.execute_input”:“2021-08-24T22:00:01.006598Z”,“iopub.status.busy”:“2021-08-24T22:00:01.005538Z”,“iopub.status.idle”:“2021-08-24T22:00:01.460650Z”,“shell.execute_reply”:“2021-08-24T22:00:01.461172Z”,“shell.execute_reply.started”:“2021-08-24T21:49:15.338218Z”}’ papermill=‘{“duration”:0.494452,“end_time”:“2021-08-24T22:00:01.461333”,“exception”:false,“start_time”:“2021-08-24T22:00:00.966881”,“status”:“completed”}’ tags=‘[“hide-input”]’ execution_count=4}\n# bar plot: districts\n\nplt.style.use('default')\nplt.figure(figsize=(14,8))\n\nplotting = sns.countplot(\n    y=\"state\",\n    data=districts_df,\n    order=districts_df.state.value_counts().index,\n    palette=\"Greys_d\",\n    linewidth=3\n)\n\nfor container in plotting.containers:\n    plotting.bar_label(container,fontsize=16)\n\n#Text\nplotting.text(x = -5, y = -4.2, s = \"State Distribution\",fontsize = 24, weight = 'bold', alpha = .90);\nplotting.text(x = -5, y = -3, s = \"Distribution of United States\",fontsize = 16, alpha = .85)\nplotting.text(x = 31.2, y = 0.08, s = 'Highest', weight = 'bold',fontsize = 14)\nplotting.text(x = 1.7, y = 22.3, s = 'Lowest', weight = 'bold',fontsize = 14)\n\nplt.yticks(fontsize=14)\nplt.xticks(fontsize=14)\n\n\n\nplt.show()\n\n\n\n\n\n\n\n:::\n\n\n\nLocales are separated into 4 categories: Suburb,Rural, City and Town, where most of the locales are concentrated in the Suburb category (104).\nFor the pct_black/hispanic variable, Rural and Town categories concentrate their entire population close to the interval $ [0,0.2 [$, while for the others sectors this percentage is varied.\nFor pctfree/reduced and pp_total_raw indicators, the distribution for each location is different, although they tend to focus on a particular interval.\n::: {#a15318df .cell _kg_hide-input=‘true’ execution=‘{“iopub.execute_input”:“2021-08-24T22:00:01.557765Z”,“iopub.status.busy”:“2021-08-24T22:00:01.556848Z”,“iopub.status.idle”:“2021-08-24T22:00:03.160447Z”,“shell.execute_reply”:“2021-08-24T22:00:03.160919Z”,“shell.execute_reply.started”:“2021-08-24T21:49:15.865498Z”}’ papermill=‘{“duration”:1.630377,“end_time”:“2021-08-24T22:00:03.161103”,“exception”:false,“start_time”:“2021-08-24T22:00:01.530726”,“status”:“completed”}’ tags=‘[“hide-input”]’ execution_count=5}\n# heatmap: districts -&gt; locale\n\ntemp = districts_df.groupby('locale').pp_total_raw.value_counts().to_frame()\ntemp.columns = ['amount']\n\ntemp = temp.reset_index(drop=False)\n\ntemp = temp.pivot(index='locale', columns='pp_total_raw')['amount']\ntemp = temp[['[4000, 6000[', '[6000, 8000[', '[8000, 10000[', '[10000, 12000[',\n       '[12000, 14000[', '[14000, 16000[', '[16000, 18000[', \n       '[18000, 20000[', '[20000, 22000[', '[22000, 24000[', ]]\n\n\ntemp1 = districts_df.groupby('locale')['pct_black/hispanic'].value_counts().to_frame()\ntemp1.columns = ['amount']\n\ntemp1 = temp1.reset_index(drop=False)\ntemp1 = temp1.pivot(index='locale', columns='pct_black/hispanic')['amount']\n\ntemp2 = districts_df.groupby('locale')['pct_free/reduced'].value_counts().to_frame()\ntemp2.columns = ['amount']\n\ntemp2 = temp2.reset_index(drop=False)\n\ntemp2 = temp2.pivot(index='locale', columns='pct_free/reduced')['amount']\n\nplt.style.use('default')\n\nfig, [[ax1, ax2], [ax3, ax4]] = plt.subplots(nrows=2, ncols=2, figsize=(24,18))\n\nsns.countplot(data=districts_df, x='locale', ax=ax1, palette='Greys_d')\nax1.text(x = -0.5, y = 120, s = \"Locale Distribution\",fontsize = 24, weight = 'bold', alpha = .90);\nax1.xaxis.set_tick_params(labelsize=16)\n\nfor container in ax1.containers:\n    ax1.bar_label(container,fontsize=16)\n\nsns.heatmap(temp1.fillna(0), annot=True,  cmap='Greys', ax=ax2,annot_kws={\"fontsize\":14})\nax2.set_title('Heatmap: locale and pct_black/hispanic',fontsize=16,loc='left')\nax2.xaxis.set_tick_params(labelsize=16)\nax2.yaxis.set_tick_params(labelsize=16)\n\nsns.heatmap(temp.fillna(0), annot=True,  cmap='Greys', ax=ax3,annot_kws={\"fontsize\":14})\nax3.set_title('Heatmap: locale and pp_total_raw',fontsize=16,loc='left')\nax3.xaxis.set_tick_params(labelsize=16)\nax3.yaxis.set_tick_params(labelsize=16)\n\nsns.heatmap(temp2.fillna(0), annot=True,  cmap='Greys', ax=ax4,annot_kws={\"fontsize\":14})\nax4.set_title('Heatmap: locale and pct_free/reduced',fontsize=16,loc='left')\nax4.xaxis.set_tick_params(labelsize=16)\nax4.yaxis.set_tick_params(labelsize=16)\n\n\nplt.show()\n\n\n\n\n\n\n\n:::\n\n\n\nSectors are separated into 3 categories: sector_Corporate, sector_HigherEd and sector_PreK-12, donde la categoría mayoritaria corresponde a sector_PreK-12 (350). On the other hand, analyzing the primary_function_main variable, all sectors are focused on theLC category. It is worth mentioning that the distribution of the other categories remains almost the same between sectors.\n::: {#84e6e3f2 .cell _kg_hide-input=‘true’ execution=‘{“iopub.execute_input”:“2021-08-24T22:00:03.301514Z”,“iopub.status.busy”:“2021-08-24T22:00:03.292826Z”,“iopub.status.idle”:“2021-08-24T22:00:03.796850Z”,“shell.execute_reply”:“2021-08-24T22:00:03.797314Z”,“shell.execute_reply.started”:“2021-08-24T21:49:17.574316Z”}’ papermill=‘{“duration”:0.551066,“end_time”:“2021-08-24T22:00:03.797481”,“exception”:false,“start_time”:“2021-08-24T22:00:03.246415”,“status”:“completed”}’ tags=‘[“hide-input”]’ execution_count=6}\nplt.style.use('default')\nnames = ['sector_Corporate', 'sector_HigherEd','sector_PreK-12']\ncounts = [products_df[x].sum() for x in names]\n\ntemp_bar = pd.DataFrame({\n    'sector':names,\n    'count':counts\n}).sort_values('count',ascending = False)\n\ntemp = products_df.groupby('primary_function_main')[names].sum()\n\n#fig, [ax1, ax2 ]= plt.subplots(nrows=1, ncols=2, figsize=(12,6))\nplt.figure(figsize=(18,18))\n\nplt.subplot(3,2,1)\nax = sns.barplot(x=\"sector\", y=\"count\", data=temp_bar,palette ='Greys_d')\nfor container in ax.containers:\n    ax.bar_label(container,fontsize=12)\n\nplt.subplot(3,2,2)\n\nsns.heatmap(temp.T, annot=True,  cmap='Greys',annot_kws={\"fontsize\":10},fmt='g')\n\nplt.text(x = -6, y = -0.25, s = \"Sectors Distribution\",fontsize = 18, weight = 'bold', alpha = .90);\n\nplt.show()\n\n\n\n\n\n\n\n:::\n\n\n\nContinuing the analysis of the primary_function_main variable, it was observed that most of these are in theLC category (77%). Within this category, its subcategory is analyzed, where the predominant subcategory is Sites, Resources & Reference (101).\n::: {#c0c96d39 .cell _kg_hide-input=‘true’ execution=‘{“iopub.execute_input”:“2021-08-24T22:00:03.924145Z”,“iopub.status.busy”:“2021-08-24T22:00:03.923467Z”,“iopub.status.idle”:“2021-08-24T22:00:04.076527Z”,“shell.execute_reply”:“2021-08-24T22:00:04.077022Z”,“shell.execute_reply.started”:“2021-08-24T21:49:18.054662Z”}’ papermill=‘{“duration”:0.190305,“end_time”:“2021-08-24T22:00:04.077195”,“exception”:false,“start_time”:“2021-08-24T22:00:03.886890”,“status”:“completed”}’ tags=‘[“hide-input”]’ execution_count=7}\n# pieplot: products\n\ncolor = [\n    'darkgray',\n    'silver',\n    'lightgray',\n    'gainsboro',\n]\n\nproducts_df[\"primary_function_main\"].value_counts().plot(\n    kind = 'pie', \n    autopct='%1d%%', \n    figsize=(6,6), \n    colors=color,\n    wedgeprops={\"edgecolor\":\"k\",'linewidth': 0.8,},\n    textprops={'color':\"black\"},\n    startangle=0)\nplt.text(x = -1.4, y = 1.1, s = \"Categories\",fontsize = 18, weight = 'bold', alpha = .90);\nplt.show()\n\n\n\n\n\n\n\n:::\n::: {#c52db360 .cell _kg_hide-input=‘true’ execution=‘{“iopub.execute_input”:“2021-08-24T22:00:04.155626Z”,“iopub.status.busy”:“2021-08-24T22:00:04.153843Z”,“iopub.status.idle”:“2021-08-24T22:00:04.430687Z”,“shell.execute_reply”:“2021-08-24T22:00:04.431156Z”,“shell.execute_reply.started”:“2021-08-24T21:49:18.238340Z”}’ papermill=‘{“duration”:0.322608,“end_time”:“2021-08-24T22:00:04.431319”,“exception”:false,“start_time”:“2021-08-24T22:00:04.108711”,“status”:“completed”}’ tags=‘[“hide-input”]’ execution_count=8}\n# pieplot: products -&gt; subcategories\n\nplt.style.use('default')\nplt.figure(figsize=(18,8))\n\ntemp = products_df[products_df.primary_function_main == 'LC']\nax = sns.countplot(\n    data=temp, \n    y='primary_function_sub',\n    order=temp.primary_function_sub.value_counts().index,\n    palette ='Greys_d'\n)\n\nfor container in ax.containers:\n    ax.bar_label(container,fontsize=16)\n\n\n#plt.title('Sub-Categories in Primary Function LC')\nplt.text(x = -50, y = -0.8, \n         s = \"Sub-Categories in Primary Function LC\",fontsize = 24, weight = 'bold', alpha = .90);\n\nplt.text(x = 105, y =0.08, s = 'Highest', weight = 'bold',fontsize=16)\nplt.text(x = 7, y = 6, s = 'Lowest', weight = 'bold',fontsize=16)\nplt.yticks(fontsize=16)\nplt.xticks(fontsize=16)\n\nplt.show()\n\n\n\n\n\n\n\n:::\n\n\n\nAfter understanding the functionality of each of the tools, it is necessary to understand the distribution of the tools. The first thing is to study the distribution of the providers of the products we have, where:\n\n258 providers have 1 occurrences.\n18 providers have 2 occurrences.\n9 providers have 3 occurrences.\n2 providers have 4 occurrences.\n2 providers have 6 occurrences.\n1 provider have 30 occurrences.\n\nBased on this, only the top 15 providers will be displayed.\n::: {#8cf15baa .cell _kg_hide-input=‘true’ execution=‘{“iopub.execute_input”:“2021-08-24T22:00:04.610677Z”,“iopub.status.busy”:“2021-08-24T22:00:04.586018Z”,“iopub.status.idle”:“2021-08-24T22:00:04.945261Z”,“shell.execute_reply”:“2021-08-24T22:00:04.945715Z”,“shell.execute_reply.started”:“2021-08-24T21:49:18.515334Z”}’ papermill=‘{“duration”:0.414758,“end_time”:“2021-08-24T22:00:04.945874”,“exception”:false,“start_time”:“2021-08-24T22:00:04.531116”,“status”:“completed”}’ tags=‘[“hide-input”]’ execution_count=9}\ndct = {\n    'Savvas Learning Company | Formerly Pearson K12 Learning': 'Savvas Learning Company'\n}\n\ntemp = products_df['provider/company_name'].value_counts().reset_index()\ntemp.columns = ['provider/company_name','count']\ntemp = temp.replace( {\n    'Savvas Learning Company | Formerly Pearson K12 Learning': 'Savvas Learning Company'\n})\n\nn = 15\ntemp = temp.sort_values('count',ascending = False).head(n)\n\nplt.style.use('default')\nplt.figure(figsize=(18,8))\n\nax = sns.barplot(\n    data=temp, \n    y='provider/company_name',\n    x='count',\n    palette ='Greys_d'\n)\n\nfor container in ax.containers:\n    ax.bar_label(container,fontsize=15)\n    \nplt.text(x = -7, y = -1, \n         s = f\"Top {n} provider/company name\",fontsize = 20, weight = 'bold', alpha = .90);\n   \nplt.text(x = 31, y =0.08, s = 'Highest', weight = 'bold',fontsize=16)\nplt.text(x = 3, y = 14.2, s = 'Lowest', weight = 'bold',fontsize=16)\nplt.yticks(fontsize=16)\n\n\nplt.yticks(fontsize=16)\nplt.xticks(fontsize=16)\nplt.show()\n\n\n\n\n\n\n\n:::\nWith regard to products, there are about 372 different products.\nWe can make a word cloud to be able to analyze in a different way, words by themselves that are repeated the most in the product_name variable.\n::: {#53d5558a .cell _kg_hide-input=‘true’ execution=‘{“iopub.execute_input”:“2021-08-24T22:00:05.106485Z”,“iopub.status.busy”:“2021-08-24T22:00:05.104433Z”,“iopub.status.idle”:“2021-08-24T22:00:06.068287Z”,“shell.execute_reply”:“2021-08-24T22:00:06.068956Z”,“shell.execute_reply.started”:“2021-08-24T21:49:18.935625Z”}’ papermill=‘{“duration”:1.012728,“end_time”:“2021-08-24T22:00:06.069135”,“exception”:false,“start_time”:“2021-08-24T22:00:05.056407”,“status”:“completed”}’ tags=‘[“hide-input”]’ execution_count=10}\ncloud = WordCloud(\n    width=1080,\n    height=270,\n    colormap='Greys',\n    background_color='white'\n    ).generate(\" \".join(products_df['product_name'].astype(str)))\n\nplt.figure(figsize=(22, 10))\nplt.imshow(cloud)\nplt.axis('off');\n\n\n\n\n\n\n\n:::\nTo understand more in detail the use of these products, we will analyze the use of these products with respect to the variable engagement_index. The first graph is related to the average engagement_index (per student) for the year 2020, where the first 15 products will be displayed.\nAn important fact is that 362 products have an average of less than 1!.\n::: {#4c2d24bf .cell _kg_hide-input=‘true’ execution=‘{“iopub.execute_input”:“2021-08-24T22:00:06.276148Z”,“iopub.status.busy”:“2021-08-24T22:00:06.275229Z”,“iopub.status.idle”:“2021-08-24T22:00:07.642119Z”,“shell.execute_reply”:“2021-08-24T22:00:07.642568Z”,“shell.execute_reply.started”:“2021-08-24T21:49:19.888566Z”}’ papermill=‘{“duration”:1.424686,“end_time”:“2021-08-24T22:00:07.642750”,“exception”:false,“start_time”:“2021-08-24T22:00:06.218064”,“status”:“completed”}’ tags=‘[“hide-input”]’ execution_count=11}\ngroup_01 = (engagement_df_mix.groupby('product_name')['engagement_index'].mean()/1000).reset_index().sort_values('engagement_index',ascending = False)\ngroup_01['engagement_index'] = group_01['engagement_index'].apply(lambda x: round(x,2))\nless_1 = len(group_01.loc[lambda x:x['engagement_index']&lt;1])\n\n\nplt.style.use('default')\nplt.figure(figsize=(14,8))\n\nplotting = sns.barplot(\n    y=\"product_name\",\n    x = \"engagement_index\",\n    data=group_01.head(20),\n    palette=\"Greys_d\",\n\n)\n\nfor container in plotting.containers:\n    plotting.bar_label(container,fontsize=14)\n\nplt.text(x = -3.5, y = -3, \n         s = \"Mean daily page-load events in top 20 tools\",fontsize = 20, weight = 'bold', alpha = .90);\n\nplt.text(x = -3.5, y = -2, \n         s = \"per 1 student\",fontsize = 14,  alpha = .90);\n\nplt.text(x = 11, y =0.1, s = 'Highest', weight = 'bold',fontsize=14)\nplt.text(x = 1, y = 19.2, s = 'Lowest', weight = 'bold',fontsize=14)\nplt.yticks(fontsize=16)\nplt.xticks(fontsize=16)\nplt.show()\n\n\n\n\n\n\n\n:::\nLet’s study the temporal behavior (at the level of weeks) of these tools during the year 2020, where the most three used tools will be shown with different colors, while the other tools will be visualized but with the same color (in order to understand their distribution).\n\nNote: The proposed analysis can be carried out at the day level and analyzing through time series each of the tools during the year 2020.\n\n::: {#9d39e70f .cell _kg_hide-input=‘true’ execution=‘{“iopub.execute_input”:“2021-08-24T22:00:08.316803Z”,“iopub.status.busy”:“2021-08-24T22:00:08.316106Z”,“iopub.status.idle”:“2021-08-24T22:00:13.858922Z”,“shell.execute_reply”:“2021-08-24T22:00:13.859377Z”,“shell.execute_reply.started”:“2021-08-24T21:49:21.992666Z”}’ papermill=‘{“duration”:6.060783,“end_time”:“2021-08-24T22:00:13.859531”,“exception”:false,“start_time”:“2021-08-24T22:00:07.798748”,“status”:“completed”}’ tags=‘[“hide-input”]’ execution_count=12}\ncol = 'week'\n\ngroup_04  = (engagement_df_mix.groupby(['product_name',col])['engagement_index'].mean()/1000).reset_index().sort_values('engagement_index',ascending = False)\n\ng_high = group_01.head(3)['product_name']\ngroup_04_top = group_04.loc[lambda x: x.product_name.isin(g_high)]\n\nstates = group_04['product_name'].unique()\ntimes= group_04[col].unique()\n\nindex = pd.MultiIndex.from_product([states,times], names = [\"product_name\", col])\n\ndf_complete = pd.DataFrame(index = index).reset_index().fillna(0)\n\ngroup_04 = df_complete.merge(group_04,on = ['product_name',col],how='left').fillna(0)\n\nn = 3\ng_high = group_04.groupby('product_name')['engagement_index'].sum().sort_values(ascending=False).head(n).index.to_list()\n\n\ncolors = [    \n    'lightgray', \n    'dimgray', \n    'black', \n    'firebrick', \n    'darkred']\npalette_01 = {x:'lavender' for x in group_04['product_name'].unique() if x not in g_high}\npalette_02 = {g_high[i]:colors[i] for i in range(n)}\n\nplt.style.use('default')\nplt.figure(figsize=(20,6))\n\n\nsns.lineplot(\n    data=group_04.loc[lambda x: ~x.product_name.isin(g_high)], \n    x=col, \n    y=\"engagement_index\", \n    hue='product_name',\n    legend = False,\n    palette=palette_01,\n    linewidth = 1.\n\n    )\n\nsns.lineplot(\n    data=group_04.loc[lambda x: x.product_name.isin(g_high)], \n    x=col, \n    y=\"engagement_index\", \n    hue='product_name',\n    palette=palette_02,\n    linewidth = 1.\n\n    )\n\n\nplt.text(x = -2, y =23.7, s = 'Mean daily page-load events in top 3 tools', weight = 'bold',fontsize=14)\nplt.text(x = -2, y =22.3, s = 'by products and time, per 1 student',fontsize=12)\n\n\nplt.text(x = 12, y =20.7, s = '1,000 cases of COVID', weight = 'bold',fontsize=8)\nplt.text(x = 37, y =20.7, s = '1st September', weight = 'bold',fontsize=8)\n\n\nplt.axvline(x = 11, color = 'black', linestyle='--',linewidth = 0.5)\nplt.axvline(x = 36, color = 'black', linestyle='--',linewidth = 0.5)\nplt.show()\n\n\n\n\n\n\n\n:::\nNow, we can understand the engagement index for the most important tools about districts, where the districts of * Wisconsin ,  Missouri * and * Virginia * have the highest engagement index among the three most used tools.\n::: {#d1e1ba17 .cell _kg_hide-input=‘true’ execution=‘{“iopub.execute_input”:“2021-08-24T22:00:14.637147Z”,“iopub.status.busy”:“2021-08-24T22:00:14.636457Z”,“iopub.status.idle”:“2021-08-24T22:00:17.454954Z”,“shell.execute_reply”:“2021-08-24T22:00:17.455400Z”,“shell.execute_reply.started”:“2021-08-24T21:49:29.563655Z”}’ papermill=‘{“duration”:3.421243,“end_time”:“2021-08-24T22:00:17.455572”,“exception”:false,“start_time”:“2021-08-24T22:00:14.034329”,“status”:“completed”}’ tags=‘[“hide-input”]’ execution_count=13}\ngroup_02 = (engagement_df_mix.groupby(['state','product_name'])['engagement_index'].mean()/1000)\\\n            .reset_index().sort_values('engagement_index',ascending = False).fillna(0)\n\ngripo_02_top = group_02.loc[lambda x: x.product_name.isin(g_high)]\ngripo_02_top['engagement_index'] = gripo_02_top['engagement_index'].apply(lambda x: round(x,2))\n#gripo_02_top = gripo_02_top.loc[lambda x: x['engagement_index']&gt;0]\n\n\nplt.style.use('default')\n\ng = sns.FacetGrid(gripo_02_top,hue='product_name',col = 'product_name',height=4, col_wrap= 3  )\ng.map(sns.barplot, \"engagement_index\",\"state\", palette=\"Greys_d\",)\n\ng.fig.set_size_inches(15, 8)\ng.fig.subplots_adjust(top=0.81, right=0.86)\n\naxes = g.axes.flatten()\nfor ax in axes:\n    for container in ax.containers:\n        ax.bar_label(container,fontsize=8)\n\n\nplt.text(x = -50, y = -4, s = \"Mean daily page-load events in top 3 tools\",fontsize = 16, weight = 'bold', alpha = .90);\nplt.text(x = -50, y = -3, s = \"by state and products, per 1 student\",fontsize = 14,  alpha = .90);\nplt.show()\n\n\n\n\n\n\n\n:::"
  },
  {
    "objectID": "posts/2021/basic-analysis-impact-on-digital-learning.html#summary",
    "href": "posts/2021/basic-analysis-impact-on-digital-learning.html#summary",
    "title": "Impact on Digital Learning",
    "section": "",
    "text": "Depending on what you want to achieve you might want to carefully preselect districts. Note that we approach in this notebook might not necessarily suit your individual purposes.\nWhen looking at digital learning, you might want to spend sometime in figuring out which districts actually applied digital learning"
  },
  {
    "objectID": "posts/2021/basic-analysis-impact-on-digital-learning.html#references",
    "href": "posts/2021/basic-analysis-impact-on-digital-learning.html#references",
    "title": "Impact on Digital Learning",
    "section": "",
    "text": "Diverging Color Maps for Scientific Visualization (Expanded) - Kenneth Moreland\nKaggle Competitions:\n\nEnthusiast to Data Professional - What changes?\nHow To Approach Analytics Challenges\nMost popular tools in 2020 Digital Learning"
  },
  {
    "objectID": "posts/2021/2021-07-31-jupyter.html",
    "href": "posts/2021/2021-07-31-jupyter.html",
    "title": "Jupyter Noteboook",
    "section": "",
    "text": "Jupyter Notebook, es un entorno de trabajo interactivo que permite desarrollar código en Python (por defecto, aunque permite otros lenguajes) de manera dinámica, a la vez que integrar en un mismo documento tanto bloques de código como texto, gráficas o imágenes. Es un SaaS utilizado ampliamente en análisis numérico, estadística y machine learning, entre otros campos de la informática y las matemáticas.\nPor otro lado, JupyterLab es similar a Jupyter Notebook en cuanto a sus funcionalidade, pero tiene un interfaz más interesante para los usuarios. Eventualmente Jupyter Lab reemplazará a Jupyter Notebok.\nNos centraremos en comprender aspectos básicos de cómo trabajar un archivo en jupyter notebook (extensión .ipynb).\n\n\n\n\n\nPara instalar RISE, necesitará usar la línea de comando. Si ha instalado Anaconda, puede usar:\nconda install -c conda-forge notebook\nDe lo contrario, puede instalar con pip:\npip install notebook\n\nNota: SI desea instalar JupyterLab, simplemente reemplaza notebook por jupyterlab.\n\n\n\n\n\n\n\nUna vez que haya instalado Jupyter Notebook en su computadora, estará listo para ejecutar el servidor de la computadora portátil. Puede iniciar el servidor del portátil desde la línea de comandos (usando Terminal en Mac/Linux, Símbolo del sistema en Windows) ejecutando:\njupyter notebook\n\nEsto imprimirá cierta información sobre el servidor en su terminal, incluida la URL de la aplicación web (de forma predeterminada, http://localhost:8888):\n$ jupyter notebook\n[I 08:58:24.417 NotebookApp] Serving notebooks from local directory: /Users/catherine\n[I 08:58:24.417 NotebookApp] 0 active kernels\n[I 08:58:24.417 NotebookApp] The Jupyter Notebook is running at: http://localhost:8888/\n[I 08:58:24.417 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).\nA continuación, abrirá su navegador web predeterminado a esta URL. Cuando el notebook se abra en su navegador, verá el Panel, que mostrará una lista de notebooks, archivos y subdirectorios en el directorio donde se inició el servidor.\n\nLa parte superior de la lista de notebooks se muestran rutas de navegación en las que se puede hacer clic del directorio actual.\nPara crear un nuevo notebook, haga clic en el botón New en la parte superior de la lista y seleccione el kernel del menú desplegable (como se ve a continuación). Los kernels que se enumeran dependen de lo que esté instalado en el servidor.\n\nNota: Es posible que algunos de los kernels de la siguiente captura de pantalla no existan como una opción para usted.\n\n\nUna vez seleccionado el kernel, se abrira nuestro primer notebook!.\n\n\n\n\nJupyter notebook nos ofrece el siguiente toolbox:\n\n\nFile: En él, puede crear un nuevo cuaderno o abrir uno preexistente. Aquí es también a donde iría para cambiar el nombre de un Cuaderno. Creo que el elemento de menú más interesante es la opción Guardar y Checkpoint. Esto le permite crear puntos de control a los que puede retroceder si lo necesita.\nEdit: Aquí puede cortar, copiar y pegar celdas. Aquí también es donde irías si quisieras eliminar, dividir o fusionar una celda. Puede reordenar celdas aquí también.\nView: es útil para alternar la visibilidad del encabezado y la barra de herramientas. También puede activar o desactivar los números de línea dentro de las celdas. Aquí también es donde irías si quieres meterte con la barra de herramientas de la celda.\nInsert: es solo para insertar celdas encima o debajo de la celda seleccionada actualmente.\nCell: le permite ejecutar una celda, un grupo de celdas o todas las celdas. También puede ir aquí para cambiar el tipo de celda, aunque personalmente considero que la barra de herramientas es más intuitiva para eso.\nKernel: es para trabajar con el kernel que se ejecuta en segundo plano. Aquí puede reiniciar el kernel, volver a conectarlo, apagarlo o incluso cambiar el kernel que está utilizando su computadora portátil.\nWidgets: es para guardar y borrar el estado del widget. Los widgets son básicamente widgets de JavaScript que puede agregar a sus celdas para crear contenido dinámico utilizando Python (u otro Kernel).\nHelp: es donde debe aprender sobre los atajos de teclado del Notebook, un recorrido por la interfaz de usuario y mucho material de referencia.\n\n\n\n\n\n\nJupyter Notebook permite que escribamos texto formateado, es decir, texto con cursiva, negritas, títulos de distintos tamaños, etc., de forma simple. Para ello Jupyter nos permite usar Markdown, que es un lenguaje de marcado (markup) muy popular.\nLos lenguajes de markup son lenguajes ideados para procesar texto, algunos de los más conocidos son HTML y \\(\\LaTeX\\). Markdown tiene como objetivo ser un lenguaje de sintaxis minimalista, simple de aprender y usar; de esa forma uno puede dar formato al texto pero sin perder demasiado tiempo en los detalles.\nLa cantidad de tutoriales en la red sobre Markdown es inmenso, por lo que nos centraremos en indicar las opciones que más se utilizan.\n\nTexto en negrita/cursiva: El texto en negrita se indica entre dos pares de asteriscos. De este modo **palabra** aparecerá como palabra. Por otro lado, el texto en cursiva se indica entre dos asteriscos simples; es decir *palabra* aparecerá como palabra.\nListas: Las listas en Markdown se realizan indicando un asterisco o un número seguido de un punto si se desean listas numeradas. Markdown organiza automáticamente los items asignándoles el número correcto.\nInclusión de imágenes: La sintaxis para incluir imágenes en Markdown es ![nombre alternativo](dirección de la imagen) en donde el nombre alternativo aparecerá en caso de que no se pueda cargar la imágen y la dirección puede referirse a una imagen local o un enlace en Internet.\nInclusión de código HTML: El lenguaje Markdown es un subconjunto del lenguaje HTML y en donde se necesite un mayor control del formato, se puede incluir directamente el código HTML.\nEnlaces: Las celdas de texto pueden contener enlaces, tanto a otras partes del documento, como a páginas en internet u otros archivos locales. Su sintaxis es [texto](dirección del enlace).\nFórmulas matemáticas: Gracias al uso de MathJax, se puede incluir código en \\(\\LaTeX\\) para mostrar todo tipo de fórmulas y expresiones matemáticas. Las fórmulas dentro de una línea de texto se escriben entre símbolos de dólar $...$, mientras que las expresiones separadas del texto utilizan símbolos de dólar dobles $$...$$. Los siguientes son ejemplos de fórmulas matemáticas escritas en \\(\\LaTeX\\):\n\n\\[p(x) = 3x^2 + 5y^2 + x^2y^2\\]\n\\[e^{\\pi i} - 1 = 0\\]\n\\[\\lim_{x \\rightarrow \\infty} 3x+1\\]\n\\[\\sum_{n=1}^\\infty\\frac{1}{n^2}\\]\n\\[\\int_0^\\infty\\frac{\\sin x}{x}\\,\\mathrm{d}x=\\frac{\\pi}{2}\\]\n\n\n\n\nJupyter Notebook permite que escribamos código dependiendo del kernel a trabajar. Por defecto, se trabaja con el kernel de Python.\nVeamos unos ejemplos sencillos de código:\n\nimport math\nn = 16\nprint(f\"La raiz cuadra de {n} es {math.sqrt(n)}\")\n\nLa raiz cuadra de 16 es 4.0\n\n\nTambién es posible visualizar tablas de datos con la librería pandas:\n\n#collapse-hide\nimport pandas as pd\nimport altair as alt\n\n# datasets\nmovies = 'https://vega.github.io/vega-datasets/data/movies.json'\nstocks = 'https://vega.github.io/vega-datasets/data/stocks.csv'\n\n\ndf = pd.read_json(movies) # load movies data\ndf.columns = [x.replace(' ', '_') for x in df.columns.values]\n\ndf.head()\n\n\n\n\n\n\n\n\n\nTitle\nUS_Gross\nWorldwide_Gross\nUS_DVD_Sales\nProduction_Budget\nRelease_Date\nMPAA_Rating\nRunning_Time_min\nDistributor\nSource\nMajor_Genre\nCreative_Type\nDirector\nRotten_Tomatoes_Rating\nIMDB_Rating\nIMDB_Votes\n\n\n\n\n0\nThe Land Girls\n146083.0\n146083.0\nNaN\n8000000.0\nJun 12 1998\nR\nNaN\nGramercy\nNone\nNone\nNone\nNone\nNaN\n6.1\n1071.0\n\n\n1\nFirst Love, Last Rites\n10876.0\n10876.0\nNaN\n300000.0\nAug 07 1998\nR\nNaN\nStrand\nNone\nDrama\nNone\nNone\nNaN\n6.9\n207.0\n\n\n2\nI Married a Strange Person\n203134.0\n203134.0\nNaN\n250000.0\nAug 28 1998\nNone\nNaN\nLionsgate\nNone\nComedy\nNone\nNone\nNaN\n6.8\n865.0\n\n\n3\nLet's Talk About Sex\n373615.0\n373615.0\nNaN\n300000.0\nSep 11 1998\nNone\nNaN\nFine Line\nNone\nComedy\nNone\nNone\n13.0\nNaN\nNaN\n\n\n4\nSlam\n1009819.0\n1087521.0\nNaN\n1000000.0\nOct 09 1998\nR\nNaN\nTrimark\nOriginal Screenplay\nDrama\nContemporary Fiction\nNone\n62.0\n3.4\n165.0\n\n\n\n\n\n\n\n\nUnas de las cosas más significativas de Jupyter notebook es poder trabajar con distintos tipos de gráficos (imagen estática o interactiva). Estos son de bastante utilidad para poder comprender nuestros procedimientos.\n\n#collapse-hide\n\n# select a point for which to provide details-on-demand\nlabel = alt.selection_single(\n    encodings=['x'], # limit selection to x-axis value\n    on='mouseover',  # select on mouseover events\n    nearest=True,    # select data point nearest the cursor\n    empty='none'     # empty selection includes no data points\n)\n\n# define our base line chart of stock prices\nbase = alt.Chart().mark_line().encode(\n    alt.X('date:T'),\n    alt.Y('price:Q', scale=alt.Scale(type='log')),\n    alt.Color('symbol:N')\n)\n\nalt.layer(\n    base, # base line chart\n    \n    # add a rule mark to serve as a guide line\n    alt.Chart().mark_rule(color='#aaa').encode(\n        x='date:T'\n    ).transform_filter(label),\n    \n    # add circle marks for selected time points, hide unselected points\n    base.mark_circle().encode(\n        opacity=alt.condition(label, alt.value(1), alt.value(0))\n    ).add_selection(label),\n\n    # add white stroked text to provide a legible background for labels\n    base.mark_text(align='left', dx=5, dy=-5, stroke='white', strokeWidth=2).encode(\n        text='price:Q'\n    ).transform_filter(label),\n\n    # add text labels for stock prices\n    base.mark_text(align='left', dx=5, dy=-5).encode(\n        text='price:Q'\n    ).transform_filter(label),\n    \n    data=stocks\n).properties(\n    width=500,\n    height=400\n)\n\n\n\n\n\n\n\n\nLa completación mediante tabs, especialmente para los atributos, es una forma conveniente de explorar la estructura de cualquier objeto con el que esté tratando.\nSimplemente escriba object_name.&lt;TAB&gt; para ver los atributos del objeto. Además de los objetos y palabras clave de Python, la finalización de pestañas también funciona en nombres de archivos y directorios.\nimport collections\ncollections. # aprete la tecla &lt;𝑇𝐴𝐵&gt;\n\n\n\nEn caso de necesitar ayuda sobre cualquier comando de Python, Jupyter nos ofrece una función llamada help.\nEn resumen, ¡suele ser más importante saber como buscar información que memorizarla! Por todo esto, Jupyter nos ofrece ayuda sobre cualquier comando agregando un signo de interrogación ? luego del nombre del comando (y luego ejecutar la celda con la combinación de teclas SHIFT + ENTER).\nimport numpy as np\nnp.sum?\n\n\n\nJupyter posee varias funciones mágicas predefinidas que sirven para simplificar tareas comunes.\nHay dos tipos de magias:\n\nMagias por linea (line magics): son comandos que empiezan con el caracter % y que toman como argumentos valores escritos en la misma línea.\nMagias por celda (cell magics): son comandos que empiezan con los caracteres %%, y que reciben argumentos en la misma línea y en toda la celda.\n\nEn general solo se puede usar una sola mágias por celda en cada celda y debe ser escrita en la primer linea de la celda.\nUn buen ejemplo de mágia es %lsmagic que lista todas las magias disponibles:\n\n%lsmagic\n\nAvailable line magics:\n%alias  %alias_magic  %autoawait  %autocall  %automagic  %autosave  %bookmark  %cat  %cd  %clear  %colors  %conda  %config  %connect_info  %cp  %debug  %dhist  %dirs  %doctest_mode  %ed  %edit  %env  %gui  %hist  %history  %killbgscripts  %ldir  %less  %lf  %lk  %ll  %load  %load_ext  %loadpy  %logoff  %logon  %logstart  %logstate  %logstop  %ls  %lsmagic  %lx  %macro  %magic  %man  %matplotlib  %mkdir  %more  %mv  %notebook  %page  %pastebin  %pdb  %pdef  %pdoc  %pfile  %pinfo  %pinfo2  %pip  %popd  %pprint  %precision  %prun  %psearch  %psource  %pushd  %pwd  %pycat  %pylab  %qtconsole  %quickref  %recall  %rehashx  %reload_ext  %rep  %rerun  %reset  %reset_selective  %rm  %rmdir  %run  %save  %sc  %set_env  %store  %sx  %system  %tb  %time  %timeit  %unalias  %unload_ext  %who  %who_ls  %whos  %xdel  %xmode\n\nAvailable cell magics:\n%%!  %%HTML  %%SVG  %%bash  %%capture  %%debug  %%file  %%html  %%javascript  %%js  %%latex  %%markdown  %%perl  %%prun  %%pypy  %%python  %%python2  %%python3  %%ruby  %%script  %%sh  %%svg  %%sx  %%system  %%time  %%timeit  %%writefile\n\nAutomagic is ON, % prefix IS NOT needed for line magics.\n\n\nEn varias situaciones resulta necesario medir el tiempo de ejecución de una porción de código. Para ello podemos usar la magia %timeit. Esta magia está disponible tanto para línea como para celda:\n\n%%timeit \n1+1 # timeit repite (adaptativamente) la medición a fin de reducir el error.\n\n8.68 ns ± 0.387 ns per loop (mean ± std. dev. of 7 runs, 100000000 loops each)\n\n\nJupyter notebook permite también mezclar varios lenguajes de programación en una misma notebook. Por ejemplo, podríamos escribir en bash lo siguiente:\n\n%%bash\nfor i in {3..1}; do\n    echo $i\ndone\necho \"Hola desde $BASH\"\n\n3\n2\n1\nHola desde /usr/bin/bash\n\n\nTambién, puede acceder a la línea de comandos, anteponiendo el símbolo de !. Esto es de bastante utilidad cuando se quiere mostrar las dependencias que se necesitan instalar. (ejemplo: !pip install pandas).\nVeamos un ejemplo:\n\n!pwd\n\n/home/fralfaro/PycharmProjects/ds_blog/_notebooks\n\n\n\n\n\n\n\nNotebook Basics\nRunning the Notebook"
  },
  {
    "objectID": "posts/2021/2021-07-31-jupyter.html#introducción",
    "href": "posts/2021/2021-07-31-jupyter.html#introducción",
    "title": "Jupyter Noteboook",
    "section": "",
    "text": "Jupyter Notebook, es un entorno de trabajo interactivo que permite desarrollar código en Python (por defecto, aunque permite otros lenguajes) de manera dinámica, a la vez que integrar en un mismo documento tanto bloques de código como texto, gráficas o imágenes. Es un SaaS utilizado ampliamente en análisis numérico, estadística y machine learning, entre otros campos de la informática y las matemáticas.\nPor otro lado, JupyterLab es similar a Jupyter Notebook en cuanto a sus funcionalidade, pero tiene un interfaz más interesante para los usuarios. Eventualmente Jupyter Lab reemplazará a Jupyter Notebok.\nNos centraremos en comprender aspectos básicos de cómo trabajar un archivo en jupyter notebook (extensión .ipynb)."
  },
  {
    "objectID": "posts/2021/2021-07-31-jupyter.html#primeros-pasos",
    "href": "posts/2021/2021-07-31-jupyter.html#primeros-pasos",
    "title": "Jupyter Noteboook",
    "section": "",
    "text": "Para instalar RISE, necesitará usar la línea de comando. Si ha instalado Anaconda, puede usar:\nconda install -c conda-forge notebook\nDe lo contrario, puede instalar con pip:\npip install notebook\n\nNota: SI desea instalar JupyterLab, simplemente reemplaza notebook por jupyterlab."
  },
  {
    "objectID": "posts/2021/2021-07-31-jupyter.html#primeros-pasos-1",
    "href": "posts/2021/2021-07-31-jupyter.html#primeros-pasos-1",
    "title": "Jupyter Noteboook",
    "section": "",
    "text": "Una vez que haya instalado Jupyter Notebook en su computadora, estará listo para ejecutar el servidor de la computadora portátil. Puede iniciar el servidor del portátil desde la línea de comandos (usando Terminal en Mac/Linux, Símbolo del sistema en Windows) ejecutando:\njupyter notebook\n\nEsto imprimirá cierta información sobre el servidor en su terminal, incluida la URL de la aplicación web (de forma predeterminada, http://localhost:8888):\n$ jupyter notebook\n[I 08:58:24.417 NotebookApp] Serving notebooks from local directory: /Users/catherine\n[I 08:58:24.417 NotebookApp] 0 active kernels\n[I 08:58:24.417 NotebookApp] The Jupyter Notebook is running at: http://localhost:8888/\n[I 08:58:24.417 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).\nA continuación, abrirá su navegador web predeterminado a esta URL. Cuando el notebook se abra en su navegador, verá el Panel, que mostrará una lista de notebooks, archivos y subdirectorios en el directorio donde se inició el servidor.\n\nLa parte superior de la lista de notebooks se muestran rutas de navegación en las que se puede hacer clic del directorio actual.\nPara crear un nuevo notebook, haga clic en el botón New en la parte superior de la lista y seleccione el kernel del menú desplegable (como se ve a continuación). Los kernels que se enumeran dependen de lo que esté instalado en el servidor.\n\nNota: Es posible que algunos de los kernels de la siguiente captura de pantalla no existan como una opción para usted.\n\n\nUna vez seleccionado el kernel, se abrira nuestro primer notebook!.\n\n\n\n\nJupyter notebook nos ofrece el siguiente toolbox:\n\n\nFile: En él, puede crear un nuevo cuaderno o abrir uno preexistente. Aquí es también a donde iría para cambiar el nombre de un Cuaderno. Creo que el elemento de menú más interesante es la opción Guardar y Checkpoint. Esto le permite crear puntos de control a los que puede retroceder si lo necesita.\nEdit: Aquí puede cortar, copiar y pegar celdas. Aquí también es donde irías si quisieras eliminar, dividir o fusionar una celda. Puede reordenar celdas aquí también.\nView: es útil para alternar la visibilidad del encabezado y la barra de herramientas. También puede activar o desactivar los números de línea dentro de las celdas. Aquí también es donde irías si quieres meterte con la barra de herramientas de la celda.\nInsert: es solo para insertar celdas encima o debajo de la celda seleccionada actualmente.\nCell: le permite ejecutar una celda, un grupo de celdas o todas las celdas. También puede ir aquí para cambiar el tipo de celda, aunque personalmente considero que la barra de herramientas es más intuitiva para eso.\nKernel: es para trabajar con el kernel que se ejecuta en segundo plano. Aquí puede reiniciar el kernel, volver a conectarlo, apagarlo o incluso cambiar el kernel que está utilizando su computadora portátil.\nWidgets: es para guardar y borrar el estado del widget. Los widgets son básicamente widgets de JavaScript que puede agregar a sus celdas para crear contenido dinámico utilizando Python (u otro Kernel).\nHelp: es donde debe aprender sobre los atajos de teclado del Notebook, un recorrido por la interfaz de usuario y mucho material de referencia."
  },
  {
    "objectID": "posts/2021/2021-07-31-jupyter.html#markdown",
    "href": "posts/2021/2021-07-31-jupyter.html#markdown",
    "title": "Jupyter Noteboook",
    "section": "",
    "text": "Jupyter Notebook permite que escribamos texto formateado, es decir, texto con cursiva, negritas, títulos de distintos tamaños, etc., de forma simple. Para ello Jupyter nos permite usar Markdown, que es un lenguaje de marcado (markup) muy popular.\nLos lenguajes de markup son lenguajes ideados para procesar texto, algunos de los más conocidos son HTML y \\(\\LaTeX\\). Markdown tiene como objetivo ser un lenguaje de sintaxis minimalista, simple de aprender y usar; de esa forma uno puede dar formato al texto pero sin perder demasiado tiempo en los detalles.\nLa cantidad de tutoriales en la red sobre Markdown es inmenso, por lo que nos centraremos en indicar las opciones que más se utilizan.\n\nTexto en negrita/cursiva: El texto en negrita se indica entre dos pares de asteriscos. De este modo **palabra** aparecerá como palabra. Por otro lado, el texto en cursiva se indica entre dos asteriscos simples; es decir *palabra* aparecerá como palabra.\nListas: Las listas en Markdown se realizan indicando un asterisco o un número seguido de un punto si se desean listas numeradas. Markdown organiza automáticamente los items asignándoles el número correcto.\nInclusión de imágenes: La sintaxis para incluir imágenes en Markdown es ![nombre alternativo](dirección de la imagen) en donde el nombre alternativo aparecerá en caso de que no se pueda cargar la imágen y la dirección puede referirse a una imagen local o un enlace en Internet.\nInclusión de código HTML: El lenguaje Markdown es un subconjunto del lenguaje HTML y en donde se necesite un mayor control del formato, se puede incluir directamente el código HTML.\nEnlaces: Las celdas de texto pueden contener enlaces, tanto a otras partes del documento, como a páginas en internet u otros archivos locales. Su sintaxis es [texto](dirección del enlace).\nFórmulas matemáticas: Gracias al uso de MathJax, se puede incluir código en \\(\\LaTeX\\) para mostrar todo tipo de fórmulas y expresiones matemáticas. Las fórmulas dentro de una línea de texto se escriben entre símbolos de dólar $...$, mientras que las expresiones separadas del texto utilizan símbolos de dólar dobles $$...$$. Los siguientes son ejemplos de fórmulas matemáticas escritas en \\(\\LaTeX\\):\n\n\\[p(x) = 3x^2 + 5y^2 + x^2y^2\\]\n\\[e^{\\pi i} - 1 = 0\\]\n\\[\\lim_{x \\rightarrow \\infty} 3x+1\\]\n\\[\\sum_{n=1}^\\infty\\frac{1}{n^2}\\]\n\\[\\int_0^\\infty\\frac{\\sin x}{x}\\,\\mathrm{d}x=\\frac{\\pi}{2}\\]"
  },
  {
    "objectID": "posts/2021/2021-07-31-jupyter.html#código",
    "href": "posts/2021/2021-07-31-jupyter.html#código",
    "title": "Jupyter Noteboook",
    "section": "",
    "text": "Jupyter Notebook permite que escribamos código dependiendo del kernel a trabajar. Por defecto, se trabaja con el kernel de Python.\nVeamos unos ejemplos sencillos de código:\n\nimport math\nn = 16\nprint(f\"La raiz cuadra de {n} es {math.sqrt(n)}\")\n\nLa raiz cuadra de 16 es 4.0\n\n\nTambién es posible visualizar tablas de datos con la librería pandas:\n\n#collapse-hide\nimport pandas as pd\nimport altair as alt\n\n# datasets\nmovies = 'https://vega.github.io/vega-datasets/data/movies.json'\nstocks = 'https://vega.github.io/vega-datasets/data/stocks.csv'\n\n\ndf = pd.read_json(movies) # load movies data\ndf.columns = [x.replace(' ', '_') for x in df.columns.values]\n\ndf.head()\n\n\n\n\n\n\n\n\n\nTitle\nUS_Gross\nWorldwide_Gross\nUS_DVD_Sales\nProduction_Budget\nRelease_Date\nMPAA_Rating\nRunning_Time_min\nDistributor\nSource\nMajor_Genre\nCreative_Type\nDirector\nRotten_Tomatoes_Rating\nIMDB_Rating\nIMDB_Votes\n\n\n\n\n0\nThe Land Girls\n146083.0\n146083.0\nNaN\n8000000.0\nJun 12 1998\nR\nNaN\nGramercy\nNone\nNone\nNone\nNone\nNaN\n6.1\n1071.0\n\n\n1\nFirst Love, Last Rites\n10876.0\n10876.0\nNaN\n300000.0\nAug 07 1998\nR\nNaN\nStrand\nNone\nDrama\nNone\nNone\nNaN\n6.9\n207.0\n\n\n2\nI Married a Strange Person\n203134.0\n203134.0\nNaN\n250000.0\nAug 28 1998\nNone\nNaN\nLionsgate\nNone\nComedy\nNone\nNone\nNaN\n6.8\n865.0\n\n\n3\nLet's Talk About Sex\n373615.0\n373615.0\nNaN\n300000.0\nSep 11 1998\nNone\nNaN\nFine Line\nNone\nComedy\nNone\nNone\n13.0\nNaN\nNaN\n\n\n4\nSlam\n1009819.0\n1087521.0\nNaN\n1000000.0\nOct 09 1998\nR\nNaN\nTrimark\nOriginal Screenplay\nDrama\nContemporary Fiction\nNone\n62.0\n3.4\n165.0\n\n\n\n\n\n\n\n\nUnas de las cosas más significativas de Jupyter notebook es poder trabajar con distintos tipos de gráficos (imagen estática o interactiva). Estos son de bastante utilidad para poder comprender nuestros procedimientos.\n\n#collapse-hide\n\n# select a point for which to provide details-on-demand\nlabel = alt.selection_single(\n    encodings=['x'], # limit selection to x-axis value\n    on='mouseover',  # select on mouseover events\n    nearest=True,    # select data point nearest the cursor\n    empty='none'     # empty selection includes no data points\n)\n\n# define our base line chart of stock prices\nbase = alt.Chart().mark_line().encode(\n    alt.X('date:T'),\n    alt.Y('price:Q', scale=alt.Scale(type='log')),\n    alt.Color('symbol:N')\n)\n\nalt.layer(\n    base, # base line chart\n    \n    # add a rule mark to serve as a guide line\n    alt.Chart().mark_rule(color='#aaa').encode(\n        x='date:T'\n    ).transform_filter(label),\n    \n    # add circle marks for selected time points, hide unselected points\n    base.mark_circle().encode(\n        opacity=alt.condition(label, alt.value(1), alt.value(0))\n    ).add_selection(label),\n\n    # add white stroked text to provide a legible background for labels\n    base.mark_text(align='left', dx=5, dy=-5, stroke='white', strokeWidth=2).encode(\n        text='price:Q'\n    ).transform_filter(label),\n\n    # add text labels for stock prices\n    base.mark_text(align='left', dx=5, dy=-5).encode(\n        text='price:Q'\n    ).transform_filter(label),\n    \n    data=stocks\n).properties(\n    width=500,\n    height=400\n)\n\n\n\n\n\n\n\n\nLa completación mediante tabs, especialmente para los atributos, es una forma conveniente de explorar la estructura de cualquier objeto con el que esté tratando.\nSimplemente escriba object_name.&lt;TAB&gt; para ver los atributos del objeto. Además de los objetos y palabras clave de Python, la finalización de pestañas también funciona en nombres de archivos y directorios.\nimport collections\ncollections. # aprete la tecla &lt;𝑇𝐴𝐵&gt;\n\n\n\nEn caso de necesitar ayuda sobre cualquier comando de Python, Jupyter nos ofrece una función llamada help.\nEn resumen, ¡suele ser más importante saber como buscar información que memorizarla! Por todo esto, Jupyter nos ofrece ayuda sobre cualquier comando agregando un signo de interrogación ? luego del nombre del comando (y luego ejecutar la celda con la combinación de teclas SHIFT + ENTER).\nimport numpy as np\nnp.sum?\n\n\n\nJupyter posee varias funciones mágicas predefinidas que sirven para simplificar tareas comunes.\nHay dos tipos de magias:\n\nMagias por linea (line magics): son comandos que empiezan con el caracter % y que toman como argumentos valores escritos en la misma línea.\nMagias por celda (cell magics): son comandos que empiezan con los caracteres %%, y que reciben argumentos en la misma línea y en toda la celda.\n\nEn general solo se puede usar una sola mágias por celda en cada celda y debe ser escrita en la primer linea de la celda.\nUn buen ejemplo de mágia es %lsmagic que lista todas las magias disponibles:\n\n%lsmagic\n\nAvailable line magics:\n%alias  %alias_magic  %autoawait  %autocall  %automagic  %autosave  %bookmark  %cat  %cd  %clear  %colors  %conda  %config  %connect_info  %cp  %debug  %dhist  %dirs  %doctest_mode  %ed  %edit  %env  %gui  %hist  %history  %killbgscripts  %ldir  %less  %lf  %lk  %ll  %load  %load_ext  %loadpy  %logoff  %logon  %logstart  %logstate  %logstop  %ls  %lsmagic  %lx  %macro  %magic  %man  %matplotlib  %mkdir  %more  %mv  %notebook  %page  %pastebin  %pdb  %pdef  %pdoc  %pfile  %pinfo  %pinfo2  %pip  %popd  %pprint  %precision  %prun  %psearch  %psource  %pushd  %pwd  %pycat  %pylab  %qtconsole  %quickref  %recall  %rehashx  %reload_ext  %rep  %rerun  %reset  %reset_selective  %rm  %rmdir  %run  %save  %sc  %set_env  %store  %sx  %system  %tb  %time  %timeit  %unalias  %unload_ext  %who  %who_ls  %whos  %xdel  %xmode\n\nAvailable cell magics:\n%%!  %%HTML  %%SVG  %%bash  %%capture  %%debug  %%file  %%html  %%javascript  %%js  %%latex  %%markdown  %%perl  %%prun  %%pypy  %%python  %%python2  %%python3  %%ruby  %%script  %%sh  %%svg  %%sx  %%system  %%time  %%timeit  %%writefile\n\nAutomagic is ON, % prefix IS NOT needed for line magics.\n\n\nEn varias situaciones resulta necesario medir el tiempo de ejecución de una porción de código. Para ello podemos usar la magia %timeit. Esta magia está disponible tanto para línea como para celda:\n\n%%timeit \n1+1 # timeit repite (adaptativamente) la medición a fin de reducir el error.\n\n8.68 ns ± 0.387 ns per loop (mean ± std. dev. of 7 runs, 100000000 loops each)\n\n\nJupyter notebook permite también mezclar varios lenguajes de programación en una misma notebook. Por ejemplo, podríamos escribir en bash lo siguiente:\n\n%%bash\nfor i in {3..1}; do\n    echo $i\ndone\necho \"Hola desde $BASH\"\n\n3\n2\n1\nHola desde /usr/bin/bash\n\n\nTambién, puede acceder a la línea de comandos, anteponiendo el símbolo de !. Esto es de bastante utilidad cuando se quiere mostrar las dependencias que se necesitan instalar. (ejemplo: !pip install pandas).\nVeamos un ejemplo:\n\n!pwd\n\n/home/fralfaro/PycharmProjects/ds_blog/_notebooks"
  },
  {
    "objectID": "posts/2021/2021-07-31-jupyter.html#referencias",
    "href": "posts/2021/2021-07-31-jupyter.html#referencias",
    "title": "Jupyter Noteboook",
    "section": "",
    "text": "Notebook Basics\nRunning the Notebook"
  },
  {
    "objectID": "posts/2023/art_docs.html",
    "href": "posts/2023/art_docs.html",
    "title": "Documentación",
    "section": "",
    "text": "Esperamos que, si estás leyendo este tutorial, ya comprendas la importancia de documentar tu código. Pero, por si acaso, permíteme citar algo que Guido mencionó en la reciente PyCon 2016:\n\n\n\n\n\n\nCita\n\n\n\n“El código se lee más a menudo de lo que se escribe.” — Guido van Rossum\n\n\nCuando escribes código, lo haces para dos audiencias principales: tus usuarios y tus desarrolladores (incluyéndote a ti mismo). Ambas audiencias son igualmente cruciales. Si eres como yo, es posible que hayas abierto antiguas bases de código y te hayas preguntado: “¿En qué estaba pensando?”. Si tienes dificultades para entender tu propio código, imagina lo que tus usuarios u otros desarrolladores sienten cuando intentan utilizarlo o contribuir a tu código.\nPor otro lado, es probable que hayas pasado por situaciones en las que deseabas realizar algo en Python y encontraste lo que parecía ser una excelente biblioteca que podría hacer el trabajo. Sin embargo, al comenzar a usar la biblioteca, buscaste ejemplos, descripciones o incluso documentación oficial sobre cómo realizar una tarea específica y no pudiste encontrar una solución de inmediato.\nDespués de buscar durante un tiempo, te das cuenta de que la documentación es insuficiente o, peor aún, está completamente ausente. Esta es una experiencia frustrante que te impide utilizar la biblioteca, sin importar cuán bueno o eficiente sea el código. Daniele Procida resumió esta situación de manera acertada:\n!!! quote “No importa cuán bueno sea tu software, porque si la documentación no es lo suficientemente buena, la gente no lo usará.” — Daniele Procida\nEn esta guía, aprenderás desde cero cómo documentar adecuadamente tu código en Python, desde los scripts más pequeños hasta los proyectos más grandes de Python, para evitar que tus usuarios se sientan frustrados al usar o contribuir a tu proyecto.\n\n\n\nAntes de sumergirnos en el arte de documentar tu código en Python, es crucial establecer una distinción fundamental: los comentarios y la documentación desempeñan roles distintos y están dirigidos a audiencias diferentes.\nComentarios:\nEn términos generales, los comentarios están diseñados para proporcionar información sobre tu código a los desarrolladores.\nLa audiencia principal a la que se dirigen son aquellos que mantienen y trabajan en el código Python. Cuando se combinan con un código bien escrito, los comentarios actúan como guías que ayudan a los lectores a comprender mejor el código, su propósito y su estructura. Esto se alinea perfectamente con la sabia observación de Jeff Atwood,\n!!! quote “El código te dice cómo; los comentarios te dicen por qué.” — Jeff Atwood\nDocumentación del Código:\nPor otro lado, la documentación del código se enfoca en describir el uso y la funcionalidad del código a los usuarios. Aunque puede ser útil durante el proceso de desarrollo, su audiencia principal son los usuarios finales del software. La siguiente sección de este artículo se adentrará en cuándo y cómo debes abordar la tarea de comentar tu código en Python.\n\n\n\n\nEn Python, los comentarios son esenciales para proporcionar información adicional sobre tu código.\nSe crean utilizando el símbolo de número (#) y deben ser declaraciones breves, no más largas que unas pocas frases. Aquí tienes un ejemplo simple:\ndef hello_world():    \n    # Un comentario simple antes de una simple declaración de impresión\n    print(\"Hola Mundo\")\nDe acuerdo con las pautas de estilo de código de Python (PEP 8), los comentarios deben tener una longitud máxima de 72 caracteres. Esto es válido incluso si tu proyecto cambia la longitud máxima de línea recomendada para que sea mayor que los 80 caracteres. Si un comentario va a superar el límite de caracteres recomendado, es apropiado usar múltiples líneas para el comentario:\ndef hello_long_world():     \n    # Una declaración muy larga que sigue y sigue y sigue y sigue y sigue \n    # sin terminar hasta que alcance el límite de 80 caracteres\n    print(\"¡Hola Mundoooooooooooooooooooooooooooooooooooooooooooooooooooooo!\")\nComentar tu código sirve para varios propósitos, incluyendo:\n\nPlanificación y Revisión: Durante el desarrollo de nuevas partes de tu código, los comentarios pueden servir como una forma de planificar o esquematizar esa sección. Es importante recordar eliminar estos comentarios una vez que se haya implementado y revisado/testeado el código real:\n# Primer paso\n# Segundo paso\n# Tercer paso\nDescripción del Código: Los comentarios se utilizan para explicar la intención de secciones específicas del código:\n# Intentar una conexión basada en configuraciones anteriores. Si no tiene éxito,\n# solicitar al usuario nuevas configuraciones.\nDescripción Algorítmica: Al usar algoritmos, especialmente los complicados, es útil explicar cómo funcionan o cómo se implementan en tu código. También es apropiado describir por qué seleccionaste un algoritmo específico en lugar de otro:\n# Usar el ordenamiento rápido para obtener ganancias de rendimiento.\nEtiquetado: Puedes utilizar etiquetas para señalar secciones específicas de código donde se encuentran problemas conocidos o áreas de mejora. Algunos ejemplos son BUG, FIXME y TODO:\n# TODO: Agregar condición para cuando 'val' sea None\n\nLos comentarios en tu código deben ser breves y centrados. Evita comentarios largos cuando sea posible. Además, sigue las siguientes cuatro reglas esenciales sugeridas por Jeff Atwood:\n\nMantén los Comentarios Cerca del Código: Los comentarios deben estar lo más cerca posible del código que describen. Los comentarios distantes del código descriptivo son frustrantes y pueden pasarse por alto fácilmente al realizar actualizaciones.\nEvita el Formato Complejo: No uses formatos complejos como tablas o figuras ASCII. Estos formatos pueden distraer y ser difíciles de mantener con el tiempo.\nEvita Información Redundante: Supón que el lector del código tiene un entendimiento básico de los principios de programación y la sintaxis del lenguaje. No incluyas información redundante.\nDiseña Tu Código para que se Comente por Sí Mismo: La forma más fácil de entender el código es leyéndolo. Cuando diseñes tu código utilizando conceptos claros y fáciles de entender, ayudarás al lector a comprender tu intención de manera rápida y sencilla.\n\nRecuerda que los comentarios están diseñados para los lectores, incluyéndote a ti mismo, para ayudarlos a comprender el propósito y diseño del software.\n\n\n\nEl Type Hinting es una característica que te permite indicar explícitamente los tipos de datos que esperas en las funciones y métodos. Aunque Python es un lenguaje de programación de tipado dinámico, el Type Hinting no cambia esa naturaleza, pero proporciona información adicional a los desarrolladores y a las herramientas de análisis estático sobre cómo debería funcionar el código.\nEl Type Hinting no afecta el comportamiento en tiempo de ejecución, por lo que no impide que el código funcione si los tipos no coinciden.\nEn cambio, es una herramienta para ayudar a los desarrolladores a comprender y depurar el código de manera más eficiente y prevenir posibles errores.\nConsidera la siguiente función hello_name:\ndef hello_name(name: str) -&gt; str:\n    return f\"Hello {name}\"\nEn este ejemplo, hemos utilizado Type Hinting para especificar que el parámetro name debe ser una cadena (str) y que la función hello_name debe devolver una cadena (str). Esta información es útil para otros desarrolladores que utilicen esta función porque ahora saben qué tipo de dato esperar como entrada y qué tipo de dato obtendrán como resultado.\n\n\n\n\n\nUna parte fundamental de la documentación en Python son las docstrings, que son cadenas de texto utilizadas para describir funciones, clases, módulos y más.\n\n\nLas docstrings son cadenas de documentación que se encuentran dentro del código fuente Python. Estas cadenas proporcionan información sobre el propósito y el funcionamiento de funciones, clases y otros elementos del código.\nLas docstrings son especialmente valiosas para ayudar a los usuarios y desarrolladores a comprender cómo utilizar y trabajar con tu código.\n¿Cómo Funcionan las Docstrings?\nCuando definimos una función, clase o módulo en Python, podemos incluir una docstring justo debajo de la definición. Por ejemplo:\ndef saludar(nombre):\n    \"\"\"Esta función imprime un saludo personalizado.\"\"\"\n    print(f\"Hola, {nombre}!\")\nLas docstrings se pueden acceder a través del atributo __doc__ del objeto. Por ejemplo:\nprint(saludar.__doc__)\nLa salida sería: “Esta función imprime un saludo personalizado.” Las docstrings también se utilizan en entornos de desarrollo interactivo y se muestran al utilizar la función help().\nManipulación de Docstrings\nEs importante destacar que puedes manipular directamente las docstrings. Sin embargo, existen restricciones para los objetos incorporados en Python. Por ejemplo, no puedes cambiar la docstring de un objeto str incorporado:\nstr.__doc__ = \"¡Esto no funcionará para objetos incorporados!\"\nPero para funciones y objetos personalizados, puedes establecer o modificar sus docstrings de la siguiente manera:\ndef decir_hola(nombre):\n    \"\"\"Una función simple que saluda a la manera de Richie.\"\"\"\n    print(f\"Hola {nombre}, ¿soy yo a quien estás buscando?\")\n\ndecir_hola.__doc__ = \"Una función que saluda estilo Richie Rich.\"\nUbicación Estratégica de las Docstrings\nUna forma más sencilla de definir docstrings es colocar una cadena literal justo debajo de la definición de la función o clase. Python automáticamente interpreta esta cadena como la docstring. Por ejemplo:\ndef decir_hola(nombre):\n    \"\"\"Una función simple que saluda a la manera de Richie.\"\"\"\n    print(f\"Hola {nombre}, ¿soy yo a quien estás buscando?\")\n\n\n\nLas docstrings son elementos esenciales para documentar tu código Python de manera clara y coherente. Siguen convenciones y pautas que se describen en PEP 257.\nEl propósito de las docstrings es proporcionar a los usuarios de tu código un resumen conciso y útil del objeto, como una función, clase, módulo o script. Deben ser lo suficientemente concisas como para ser fáciles de mantener, pero lo suficientemente detalladas como para que los nuevos usuarios comprendan su propósito y cómo utilizar el objeto documentado.\n\n\nTodas las docstrings deben utilizar el formato de triple comilla doble (\"\"\") y deben colocarse justo debajo de la definición del objeto, ya sea en una sola línea o en varias líneas:\nUna línea:\n\"\"\"Esta es una línea de resumen rápida utilizada como descripción del objeto.\"\"\"\nVarias líneas:\n\"\"\"\nEsta es la línea de resumen\nEsta es la elaboración adicional de la docstring. Dentro de esta sección, puedes proporcionar más detalles según sea apropiado para la situación. Observa que el resumen y la elaboración están separados por una nueva línea en blanco.\n\"\"\"\nEs importante destacar que todas las docstrings de varias líneas deben seguir un patrón específico:\n\nUna línea de resumen de una sola línea.\nUna línea en blanco después del resumen.\nCualquier elaboración adicional de la docstring.\nOtra línea en blanco.\n\nAdemás, todas las docstrings deben tener una longitud máxima de caracteres que sigue las mismas pautas que los comentarios, que es de 72 caracteres.\n\n\n\nLas docstrings de clase se crean para la clase en sí, así como para cualquier método de clase. Las docstrings se colocan inmediatamente después de la clase o el método de clase, con un nivel de sangría:\nclass ClaseSimple:\n    \"\"\"Aquí van las docstrings de clase.\"\"\"\n    def decir_hola(self, nombre: str):\n        \"\"\"Aquí van las docstrings de método de clase.\"\"\"\n        print(f'Hola {nombre}')\nLas docstrings de clase deben contener la siguiente información:\n\nUn breve resumen de su propósito y comportamiento.\nCualquier método público, junto con una breve descripción.\nCualquier propiedad de clase (atributos).\nCualquier cosa relacionada con la interfaz para los subclases, si la clase está destinada a ser subclaseada.\n\nLos parámetros del constructor de clase deben documentarse dentro de la docstring del método __init__ de la clase. Los métodos individuales deben documentarse utilizando sus propias docstrings individuales. Las docstrings de método de clase deben contener lo siguiente:\n\nUna breve descripción de lo que hace el método y para qué se utiliza.\nCualquier argumento (tanto requerido como opcional) que se pase, incluidos los argumentos de palabras clave.\nEtiqueta para cualquier argumento que se considere opcional o tenga un valor predeterminado.\nCualquier efecto secundario que ocurra al ejecutar el método.\nCualquier excepción que se genere.\nCualquier restricción sobre cuándo se puede llamar al método.\n\nEchemos un vistazo a un ejemplo simple de una clase de datos que representa un Animal. Esta clase contendrá algunas propiedades de clase, propiedades de instancia, un __init__ y un único método de instancia:\nclass Animal:\n    \"\"\"Una clase utilizada para representar un Animal\n    \n    Attributes:\n        dice_str (str): una cadena formateada para imprimir lo que dice el animal\n        nombre (str): el nombre del animal\n        sonido (str): el sonido que hace el animal\n        num_patas (int): el número de patas del animal (predeterminado 4)\n    \"\"\"\n    \n    dice_str = \"Un {nombre} dice {sonido}\"\n    \n    def __init__(self, nombre, sonido, num_patas=4):\n        \"\"\"Inicializa una nueva instancia de Animal\n        \n        Parameters:\n            nombre (str): El nombre del animal\n            sonido (str): El sonido que hace el animal\n            num_patas (int, opcional): El número de patas del animal (predeterminado es 4)\n        \"\"\"\n        self.nombre = nombre\n        self.sonido = sonido\n        self.num_patas = num_patas\n        \n    def dice(self, sonido=None):\n        \"\"\"Imprime el nombre del animal y el sonido que hace.\n        \n        Si no se pasa el argumento `sonido`, se utiliza el sonido predeterminado del Animal.\n        \n        Parameters:\n            sonido (str, opcional): El sonido que hace el animal (predeterminado es None)\n        \n        Raises:\n            NotImplementedError: Si no se establece ningún sonido para el animal o se pasa como parámetro.\n        \"\"\"\n        if self.sonido is None and sonido is None:\n            raise NotImplementedError(\"¡No se admiten animales silenciosos!\")\n        sonido_salida = self.sonido if sonido is None else sonido\n        print(self.dice_str.format(nombre=self.nombre, sonido=sonido_salida))\n\n\n\n\n\nExisten formatos específicos de docstrings que pueden ser utilizados para ayudar a los analizadores de docstrings y a los usuarios a tener un formato familiar y reconocido.\nAlgunos de los formatos más comunes son los siguientes:\n\n\n\nTipo de Formato\nDescripción\nCompatible con Sphinx\nEspecificación Formal\n\n\n\n\nGoogle docstrings\nForma de documentación recomendada por Google\nSí\nNo\n\n\nreStructuredText\nEstándar oficial de documentación de Python; no es amigable para principiantes pero rico en características\nSí\nSí\n\n\nNumPy/SciPy docstrings\nCombinación de reStructuredText y Docstrings de Google utilizada por NumPy\nSí\nSí\n\n\nEpytext\nUna adaptación de Epydoc para Python; ideal para desarrolladores de Java\nNo oficialmente\nSí\n\n\n\nLa elección del formato de docstring depende de ti, pero debes mantener el mismo formato en todo tu documento o proyecto. A continuación, se presentan ejemplos de cada tipo para darte una idea de cómo se ve cada formato de documentación.\n\n\n\"\"\"Obtiene e imprime las columnas de encabezado de la hoja de cálculo\n\nArgs:\n    file_loc (str): La ubicación del archivo de la hoja de cálculo.\n    print_cols (bool): Una bandera utilizada para imprimir las columnas en la consola\n        (el valor predeterminado es Falso)\n\nReturns:\n    list: una lista de cadenas que representan las columnas de encabezado\n\"\"\"\n\n\n\n\"\"\"Obtiene e imprime las columnas de encabezado de la hoja de cálculo\n\n:param file_loc: La ubicación del archivo de la hoja de cálculo\n:type file_loc: str\n:param print_cols: Una bandera utilizada para imprimir las columnas en la consola\n    (el valor predeterminado es Falso)\n:type print_cols: bool\n\n:returns: una lista de cadenas que representan las columnas de encabezado\n:rtype: list\n\"\"\"\n\n\n\n\"\"\"Obtiene e imprime las columnas de encabezado de la hoja de cálculo\n\nParameters\n----------\nfile_loc : str\n    La ubicación del archivo de la hoja de cálculo\nprint_cols : bool, opcional\n    Una bandera utilizada para imprimir las columnas en la consola (el valor predeterminado es Falso)\n\nReturns\n-------\nlist\n    una lista de cadenas que representan las columnas de encabezado\n\"\"\"\n\n\n\n\"\"\"Obtiene e imprime las columnas de encabezado de la hoja de cálculo\n\n@type file_loc: str\n@param file_loc: La ubicación del archivo de la hoja de cálculo\n@type print_cols: bool\n@param print_cols: Una bandera utilizada para imprimir las columnas en la consola\n    (el valor predeterminado es Falso)\n@rtype: list\n@returns: una lista de cadenas que representan las columnas de encabezado\n\"\"\"\nEstos ejemplos te proporcionan una idea de cómo se estructuran y formatean las docstrings en diferentes estilos de documentación.\nPuedes elegir el que mejor se adapte a tus preferencias y necesidades de documentación, pero asegúrate de mantener la coherencia en todo tu proyecto.\n\n\n\n\n\nLos proyectos de Python vienen en todo tipo de formas, tamaños y propósitos. La forma en que documentas tu proyecto debe adaptarse a tu situación específica. Ten en cuenta quiénes serán los usuarios de tu proyecto y adáptate a sus necesidades. Dependiendo del tipo de proyecto, se recomiendan ciertos aspectos de la documentación. La estructura general del proyecto y su documentación debe ser la siguiente:\nproject_root/\n│\n├── project/  # Project source code\n├── docs/\n├── README\n├── HOW_TO_CONTRIBUTE\n├── CODE_OF_CONDUCT\n├── examples.py\nEsta estructura de directorios es un diseño común para organizar un proyecto de software en Python. A continuación, se explica en detalle cada elemento de esta estructura:\n\nproject_root (Directorio Raíz del Proyecto): Este es el directorio principal que contiene todos los archivos y carpetas relacionados con tu proyecto. Es el punto de partida para tu proyecto.\nproject/ (Carpeta “project”): Esta carpeta suele contener el código fuente principal de tu proyecto. Aquí se almacenan todos los archivos de Python que forman parte de tu proyecto. Puedes organizar estos archivos en subdirectorios según la estructura de tu proyecto. Por ejemplo, puedes tener subdirectorios para módulos específicos o componentes del proyecto.\ndocs/ (Carpeta “docs”): La carpeta “docs” se utiliza para almacenar la documentación de tu proyecto. Aquí puedes incluir documentos explicativos, manuales de usuario, instrucciones de instalación y cualquier otra documentación relevante. Mantener una documentación clara y organizada es esencial para que los usuarios comprendan y utilicen tu proyecto de manera efectiva.\nREADME: El archivo “README” es un documento importante que proporciona una breve descripción de tu proyecto y su propósito. Suele incluir información sobre cómo instalar y utilizar el proyecto, así como otros detalles importantes. Los usuarios suelen consultar este archivo primero cuando exploran un proyecto.\nHOW_TO_CONTRIBUTE: Este archivo contiene instrucciones para las personas que deseen contribuir al desarrollo de tu proyecto. Incluye detalles sobre cómo pueden colaborar, enviar correcciones, agregar nuevas funciones y seguir las pautas de contribución.\nCODE_OF_CONDUCT: El archivo “CODE_OF_CONDUCT” establece las reglas y pautas de comportamiento que deben seguir los colaboradores y usuarios del proyecto. Define cómo deben interactuar entre sí de manera respetuosa y profesional. También puede indicar las consecuencias en caso de violación del código de conducta.\nexamples.py: Este archivo es un script de Python que contiene ejemplos simples de cómo utilizar las funcionalidades de tu proyecto. Estos ejemplos pueden ayudar a los usuarios a comprender cómo utilizar tu código en situaciones reales y proporcionar ejemplos de uso práctico.\n\n\n\n\nLa documentación es una parte fundamental de cualquier proyecto de desarrollo de software. Proporciona información crucial sobre cómo utilizar, mantener y contribuir al código. En el ecosistema de Python, existen varias bibliotecas y herramientas que facilitan la tarea de documentar el código de manera efectiva. En este artículo, exploraremos algunas de las principales bibliotecas de Python utilizadas para documentar código.\n\n\n\nSphinx es una de las herramientas de documentación más populares en el mundo de Python. Fue originalmente desarrollada para documentar la propia documentación de Python y se ha convertido en una elección común para proyectos de código abierto y proyectos internos. Algunas de sus características clave incluyen:\n\nGeneración de documentación en varios formatos, incluyendo HTML, PDF, ePub y más.\nUtiliza reStructuredText como su formato de marcado predeterminado, que es altamente estructurado y permite documentar de manera eficiente los aspectos técnicos.\nAmplia gama de extensiones y complementos que permiten personalizar y mejorar la documentación.\nAdmite la generación automática de documentación a partir de docstrings en el código Python.\nEs especialmente adecuado para documentar bibliotecas, API y proyectos técnicos.\n\nSphinx es altamente configurable y puede generar documentación de alta calidad y profesional. Sin embargo, puede requerir un tiempo de configuración inicial y tiene una curva de aprendizaje empinada para los principiantes.\n\n\n\n\nMkDocs es una herramienta de generación de documentación que se centra en la simplicidad y la facilidad de uso. Está diseñada para crear documentación de proyectos de una manera simple y rápida, principalmente enfocada en la generación de sitios web de documentación. Algunas de sus características clave incluyen:\n\nUtiliza Markdown como formato de marcado predeterminado, que es fácil de aprender y escribir.\nOfrece una interfaz de línea de comandos simple para iniciar y generar sitios de documentación.\nProporciona temas y extensiones para personalizar el aspecto y la funcionalidad de la documentación generada.\nIdeal para proyectos de código abierto y documentación de proyectos pequeños a medianos.\n\nMkDocs es especialmente adecuado para proyectos con necesidades de documentación simples. Es fácil de aprender y usar, lo que lo convierte en una excelente opción para principiantes. Sin embargo, puede ser limitado en funcionalidad en comparación con Sphinx para proyectos técnicos y complejos.\n\n\n\nMkDocs-Material es un tema personalizado para MkDocs, una popular herramienta de generación de sitios web estáticos diseñada para crear documentación de proyectos de manera sencilla y efectiva. Este tema, conocido como “Material for MkDocs”, se inspira en el elegante diseño de Material Design de Google y está diseñado para ofrecer una experiencia de documentación moderna y atractiva.\nUna de las principales características de MkDocs-Material es su enfoque en la legibilidad y la facilidad de navegación. Esto se logra mediante un diseño limpio y organizado que hace que la documentación sea más accesible para los usuarios. Además, el tema proporciona herramientas útiles para mejorar la experiencia de los lectores, como una función de búsqueda integrada que permite a los usuarios encontrar rápidamente la información que necesitan.\nOtra ventaja de MkDocs-Material es su navegación intuitiva, que facilita a los usuarios la exploración de la documentación y la navegación entre secciones y páginas. Esto es fundamental para garantizar que los usuarios puedan acceder fácilmente a la información que están buscando sin esfuerzo.\n\n\n\n\n\nLa documentación de proyectos sigue una progresión simple:\n\nSin Documentación\nAlguna Documentación\nDocumentación Completa\nBuena Documentación\nExcelente Documentación\n\nSi te sientes perdido acerca de por dónde continuar con tu documentación, observa en qué punto se encuentra tu proyecto en relación con la progresión mencionada anteriormente. ¿Tienes alguna documentación? Si no la tienes, comienza por ahí. Si ya tienes algo de documentación pero te faltan algunos de los archivos clave del proyecto, comienza agregándolos.\nAl final, no te desanimes ni te sientas abrumado por la cantidad de trabajo necesario para documentar el código.\nUna vez que comiences a documentar tu código, te resultará más fácil seguir adelante."
  },
  {
    "objectID": "posts/2023/art_docs.html#comentarios-vs-documentación",
    "href": "posts/2023/art_docs.html#comentarios-vs-documentación",
    "title": "Documentación",
    "section": "",
    "text": "Antes de sumergirnos en el arte de documentar tu código en Python, es crucial establecer una distinción fundamental: los comentarios y la documentación desempeñan roles distintos y están dirigidos a audiencias diferentes.\nComentarios:\nEn términos generales, los comentarios están diseñados para proporcionar información sobre tu código a los desarrolladores.\nLa audiencia principal a la que se dirigen son aquellos que mantienen y trabajan en el código Python. Cuando se combinan con un código bien escrito, los comentarios actúan como guías que ayudan a los lectores a comprender mejor el código, su propósito y su estructura. Esto se alinea perfectamente con la sabia observación de Jeff Atwood,\n!!! quote “El código te dice cómo; los comentarios te dicen por qué.” — Jeff Atwood\nDocumentación del Código:\nPor otro lado, la documentación del código se enfoca en describir el uso y la funcionalidad del código a los usuarios. Aunque puede ser útil durante el proceso de desarrollo, su audiencia principal son los usuarios finales del software. La siguiente sección de este artículo se adentrará en cuándo y cómo debes abordar la tarea de comentar tu código en Python."
  },
  {
    "objectID": "posts/2023/art_docs.html#comentarios",
    "href": "posts/2023/art_docs.html#comentarios",
    "title": "Documentación",
    "section": "",
    "text": "En Python, los comentarios son esenciales para proporcionar información adicional sobre tu código.\nSe crean utilizando el símbolo de número (#) y deben ser declaraciones breves, no más largas que unas pocas frases. Aquí tienes un ejemplo simple:\ndef hello_world():    \n    # Un comentario simple antes de una simple declaración de impresión\n    print(\"Hola Mundo\")\nDe acuerdo con las pautas de estilo de código de Python (PEP 8), los comentarios deben tener una longitud máxima de 72 caracteres. Esto es válido incluso si tu proyecto cambia la longitud máxima de línea recomendada para que sea mayor que los 80 caracteres. Si un comentario va a superar el límite de caracteres recomendado, es apropiado usar múltiples líneas para el comentario:\ndef hello_long_world():     \n    # Una declaración muy larga que sigue y sigue y sigue y sigue y sigue \n    # sin terminar hasta que alcance el límite de 80 caracteres\n    print(\"¡Hola Mundoooooooooooooooooooooooooooooooooooooooooooooooooooooo!\")\nComentar tu código sirve para varios propósitos, incluyendo:\n\nPlanificación y Revisión: Durante el desarrollo de nuevas partes de tu código, los comentarios pueden servir como una forma de planificar o esquematizar esa sección. Es importante recordar eliminar estos comentarios una vez que se haya implementado y revisado/testeado el código real:\n# Primer paso\n# Segundo paso\n# Tercer paso\nDescripción del Código: Los comentarios se utilizan para explicar la intención de secciones específicas del código:\n# Intentar una conexión basada en configuraciones anteriores. Si no tiene éxito,\n# solicitar al usuario nuevas configuraciones.\nDescripción Algorítmica: Al usar algoritmos, especialmente los complicados, es útil explicar cómo funcionan o cómo se implementan en tu código. También es apropiado describir por qué seleccionaste un algoritmo específico en lugar de otro:\n# Usar el ordenamiento rápido para obtener ganancias de rendimiento.\nEtiquetado: Puedes utilizar etiquetas para señalar secciones específicas de código donde se encuentran problemas conocidos o áreas de mejora. Algunos ejemplos son BUG, FIXME y TODO:\n# TODO: Agregar condición para cuando 'val' sea None\n\nLos comentarios en tu código deben ser breves y centrados. Evita comentarios largos cuando sea posible. Además, sigue las siguientes cuatro reglas esenciales sugeridas por Jeff Atwood:\n\nMantén los Comentarios Cerca del Código: Los comentarios deben estar lo más cerca posible del código que describen. Los comentarios distantes del código descriptivo son frustrantes y pueden pasarse por alto fácilmente al realizar actualizaciones.\nEvita el Formato Complejo: No uses formatos complejos como tablas o figuras ASCII. Estos formatos pueden distraer y ser difíciles de mantener con el tiempo.\nEvita Información Redundante: Supón que el lector del código tiene un entendimiento básico de los principios de programación y la sintaxis del lenguaje. No incluyas información redundante.\nDiseña Tu Código para que se Comente por Sí Mismo: La forma más fácil de entender el código es leyéndolo. Cuando diseñes tu código utilizando conceptos claros y fáciles de entender, ayudarás al lector a comprender tu intención de manera rápida y sencilla.\n\nRecuerda que los comentarios están diseñados para los lectores, incluyéndote a ti mismo, para ayudarlos a comprender el propósito y diseño del software.\n\n\n\nEl Type Hinting es una característica que te permite indicar explícitamente los tipos de datos que esperas en las funciones y métodos. Aunque Python es un lenguaje de programación de tipado dinámico, el Type Hinting no cambia esa naturaleza, pero proporciona información adicional a los desarrolladores y a las herramientas de análisis estático sobre cómo debería funcionar el código.\nEl Type Hinting no afecta el comportamiento en tiempo de ejecución, por lo que no impide que el código funcione si los tipos no coinciden.\nEn cambio, es una herramienta para ayudar a los desarrolladores a comprender y depurar el código de manera más eficiente y prevenir posibles errores.\nConsidera la siguiente función hello_name:\ndef hello_name(name: str) -&gt; str:\n    return f\"Hello {name}\"\nEn este ejemplo, hemos utilizado Type Hinting para especificar que el parámetro name debe ser una cadena (str) y que la función hello_name debe devolver una cadena (str). Esta información es útil para otros desarrolladores que utilicen esta función porque ahora saben qué tipo de dato esperar como entrada y qué tipo de dato obtendrán como resultado."
  },
  {
    "objectID": "posts/2023/art_docs.html#docstrings",
    "href": "posts/2023/art_docs.html#docstrings",
    "title": "Documentación",
    "section": "",
    "text": "Una parte fundamental de la documentación en Python son las docstrings, que son cadenas de texto utilizadas para describir funciones, clases, módulos y más.\n\n\nLas docstrings son cadenas de documentación que se encuentran dentro del código fuente Python. Estas cadenas proporcionan información sobre el propósito y el funcionamiento de funciones, clases y otros elementos del código.\nLas docstrings son especialmente valiosas para ayudar a los usuarios y desarrolladores a comprender cómo utilizar y trabajar con tu código.\n¿Cómo Funcionan las Docstrings?\nCuando definimos una función, clase o módulo en Python, podemos incluir una docstring justo debajo de la definición. Por ejemplo:\ndef saludar(nombre):\n    \"\"\"Esta función imprime un saludo personalizado.\"\"\"\n    print(f\"Hola, {nombre}!\")\nLas docstrings se pueden acceder a través del atributo __doc__ del objeto. Por ejemplo:\nprint(saludar.__doc__)\nLa salida sería: “Esta función imprime un saludo personalizado.” Las docstrings también se utilizan en entornos de desarrollo interactivo y se muestran al utilizar la función help().\nManipulación de Docstrings\nEs importante destacar que puedes manipular directamente las docstrings. Sin embargo, existen restricciones para los objetos incorporados en Python. Por ejemplo, no puedes cambiar la docstring de un objeto str incorporado:\nstr.__doc__ = \"¡Esto no funcionará para objetos incorporados!\"\nPero para funciones y objetos personalizados, puedes establecer o modificar sus docstrings de la siguiente manera:\ndef decir_hola(nombre):\n    \"\"\"Una función simple que saluda a la manera de Richie.\"\"\"\n    print(f\"Hola {nombre}, ¿soy yo a quien estás buscando?\")\n\ndecir_hola.__doc__ = \"Una función que saluda estilo Richie Rich.\"\nUbicación Estratégica de las Docstrings\nUna forma más sencilla de definir docstrings es colocar una cadena literal justo debajo de la definición de la función o clase. Python automáticamente interpreta esta cadena como la docstring. Por ejemplo:\ndef decir_hola(nombre):\n    \"\"\"Una función simple que saluda a la manera de Richie.\"\"\"\n    print(f\"Hola {nombre}, ¿soy yo a quien estás buscando?\")\n\n\n\nLas docstrings son elementos esenciales para documentar tu código Python de manera clara y coherente. Siguen convenciones y pautas que se describen en PEP 257.\nEl propósito de las docstrings es proporcionar a los usuarios de tu código un resumen conciso y útil del objeto, como una función, clase, módulo o script. Deben ser lo suficientemente concisas como para ser fáciles de mantener, pero lo suficientemente detalladas como para que los nuevos usuarios comprendan su propósito y cómo utilizar el objeto documentado.\n\n\nTodas las docstrings deben utilizar el formato de triple comilla doble (\"\"\") y deben colocarse justo debajo de la definición del objeto, ya sea en una sola línea o en varias líneas:\nUna línea:\n\"\"\"Esta es una línea de resumen rápida utilizada como descripción del objeto.\"\"\"\nVarias líneas:\n\"\"\"\nEsta es la línea de resumen\nEsta es la elaboración adicional de la docstring. Dentro de esta sección, puedes proporcionar más detalles según sea apropiado para la situación. Observa que el resumen y la elaboración están separados por una nueva línea en blanco.\n\"\"\"\nEs importante destacar que todas las docstrings de varias líneas deben seguir un patrón específico:\n\nUna línea de resumen de una sola línea.\nUna línea en blanco después del resumen.\nCualquier elaboración adicional de la docstring.\nOtra línea en blanco.\n\nAdemás, todas las docstrings deben tener una longitud máxima de caracteres que sigue las mismas pautas que los comentarios, que es de 72 caracteres.\n\n\n\nLas docstrings de clase se crean para la clase en sí, así como para cualquier método de clase. Las docstrings se colocan inmediatamente después de la clase o el método de clase, con un nivel de sangría:\nclass ClaseSimple:\n    \"\"\"Aquí van las docstrings de clase.\"\"\"\n    def decir_hola(self, nombre: str):\n        \"\"\"Aquí van las docstrings de método de clase.\"\"\"\n        print(f'Hola {nombre}')\nLas docstrings de clase deben contener la siguiente información:\n\nUn breve resumen de su propósito y comportamiento.\nCualquier método público, junto con una breve descripción.\nCualquier propiedad de clase (atributos).\nCualquier cosa relacionada con la interfaz para los subclases, si la clase está destinada a ser subclaseada.\n\nLos parámetros del constructor de clase deben documentarse dentro de la docstring del método __init__ de la clase. Los métodos individuales deben documentarse utilizando sus propias docstrings individuales. Las docstrings de método de clase deben contener lo siguiente:\n\nUna breve descripción de lo que hace el método y para qué se utiliza.\nCualquier argumento (tanto requerido como opcional) que se pase, incluidos los argumentos de palabras clave.\nEtiqueta para cualquier argumento que se considere opcional o tenga un valor predeterminado.\nCualquier efecto secundario que ocurra al ejecutar el método.\nCualquier excepción que se genere.\nCualquier restricción sobre cuándo se puede llamar al método.\n\nEchemos un vistazo a un ejemplo simple de una clase de datos que representa un Animal. Esta clase contendrá algunas propiedades de clase, propiedades de instancia, un __init__ y un único método de instancia:\nclass Animal:\n    \"\"\"Una clase utilizada para representar un Animal\n    \n    Attributes:\n        dice_str (str): una cadena formateada para imprimir lo que dice el animal\n        nombre (str): el nombre del animal\n        sonido (str): el sonido que hace el animal\n        num_patas (int): el número de patas del animal (predeterminado 4)\n    \"\"\"\n    \n    dice_str = \"Un {nombre} dice {sonido}\"\n    \n    def __init__(self, nombre, sonido, num_patas=4):\n        \"\"\"Inicializa una nueva instancia de Animal\n        \n        Parameters:\n            nombre (str): El nombre del animal\n            sonido (str): El sonido que hace el animal\n            num_patas (int, opcional): El número de patas del animal (predeterminado es 4)\n        \"\"\"\n        self.nombre = nombre\n        self.sonido = sonido\n        self.num_patas = num_patas\n        \n    def dice(self, sonido=None):\n        \"\"\"Imprime el nombre del animal y el sonido que hace.\n        \n        Si no se pasa el argumento `sonido`, se utiliza el sonido predeterminado del Animal.\n        \n        Parameters:\n            sonido (str, opcional): El sonido que hace el animal (predeterminado es None)\n        \n        Raises:\n            NotImplementedError: Si no se establece ningún sonido para el animal o se pasa como parámetro.\n        \"\"\"\n        if self.sonido is None and sonido is None:\n            raise NotImplementedError(\"¡No se admiten animales silenciosos!\")\n        sonido_salida = self.sonido if sonido is None else sonido\n        print(self.dice_str.format(nombre=self.nombre, sonido=sonido_salida))\n\n\n\n\n\nExisten formatos específicos de docstrings que pueden ser utilizados para ayudar a los analizadores de docstrings y a los usuarios a tener un formato familiar y reconocido.\nAlgunos de los formatos más comunes son los siguientes:\n\n\n\nTipo de Formato\nDescripción\nCompatible con Sphinx\nEspecificación Formal\n\n\n\n\nGoogle docstrings\nForma de documentación recomendada por Google\nSí\nNo\n\n\nreStructuredText\nEstándar oficial de documentación de Python; no es amigable para principiantes pero rico en características\nSí\nSí\n\n\nNumPy/SciPy docstrings\nCombinación de reStructuredText y Docstrings de Google utilizada por NumPy\nSí\nSí\n\n\nEpytext\nUna adaptación de Epydoc para Python; ideal para desarrolladores de Java\nNo oficialmente\nSí\n\n\n\nLa elección del formato de docstring depende de ti, pero debes mantener el mismo formato en todo tu documento o proyecto. A continuación, se presentan ejemplos de cada tipo para darte una idea de cómo se ve cada formato de documentación.\n\n\n\"\"\"Obtiene e imprime las columnas de encabezado de la hoja de cálculo\n\nArgs:\n    file_loc (str): La ubicación del archivo de la hoja de cálculo.\n    print_cols (bool): Una bandera utilizada para imprimir las columnas en la consola\n        (el valor predeterminado es Falso)\n\nReturns:\n    list: una lista de cadenas que representan las columnas de encabezado\n\"\"\"\n\n\n\n\"\"\"Obtiene e imprime las columnas de encabezado de la hoja de cálculo\n\n:param file_loc: La ubicación del archivo de la hoja de cálculo\n:type file_loc: str\n:param print_cols: Una bandera utilizada para imprimir las columnas en la consola\n    (el valor predeterminado es Falso)\n:type print_cols: bool\n\n:returns: una lista de cadenas que representan las columnas de encabezado\n:rtype: list\n\"\"\"\n\n\n\n\"\"\"Obtiene e imprime las columnas de encabezado de la hoja de cálculo\n\nParameters\n----------\nfile_loc : str\n    La ubicación del archivo de la hoja de cálculo\nprint_cols : bool, opcional\n    Una bandera utilizada para imprimir las columnas en la consola (el valor predeterminado es Falso)\n\nReturns\n-------\nlist\n    una lista de cadenas que representan las columnas de encabezado\n\"\"\"\n\n\n\n\"\"\"Obtiene e imprime las columnas de encabezado de la hoja de cálculo\n\n@type file_loc: str\n@param file_loc: La ubicación del archivo de la hoja de cálculo\n@type print_cols: bool\n@param print_cols: Una bandera utilizada para imprimir las columnas en la consola\n    (el valor predeterminado es Falso)\n@rtype: list\n@returns: una lista de cadenas que representan las columnas de encabezado\n\"\"\"\nEstos ejemplos te proporcionan una idea de cómo se estructuran y formatean las docstrings en diferentes estilos de documentación.\nPuedes elegir el que mejor se adapte a tus preferencias y necesidades de documentación, pero asegúrate de mantener la coherencia en todo tu proyecto."
  },
  {
    "objectID": "posts/2023/art_docs.html#documentar-tus-proyectos-de-python",
    "href": "posts/2023/art_docs.html#documentar-tus-proyectos-de-python",
    "title": "Documentación",
    "section": "",
    "text": "Los proyectos de Python vienen en todo tipo de formas, tamaños y propósitos. La forma en que documentas tu proyecto debe adaptarse a tu situación específica. Ten en cuenta quiénes serán los usuarios de tu proyecto y adáptate a sus necesidades. Dependiendo del tipo de proyecto, se recomiendan ciertos aspectos de la documentación. La estructura general del proyecto y su documentación debe ser la siguiente:\nproject_root/\n│\n├── project/  # Project source code\n├── docs/\n├── README\n├── HOW_TO_CONTRIBUTE\n├── CODE_OF_CONDUCT\n├── examples.py\nEsta estructura de directorios es un diseño común para organizar un proyecto de software en Python. A continuación, se explica en detalle cada elemento de esta estructura:\n\nproject_root (Directorio Raíz del Proyecto): Este es el directorio principal que contiene todos los archivos y carpetas relacionados con tu proyecto. Es el punto de partida para tu proyecto.\nproject/ (Carpeta “project”): Esta carpeta suele contener el código fuente principal de tu proyecto. Aquí se almacenan todos los archivos de Python que forman parte de tu proyecto. Puedes organizar estos archivos en subdirectorios según la estructura de tu proyecto. Por ejemplo, puedes tener subdirectorios para módulos específicos o componentes del proyecto.\ndocs/ (Carpeta “docs”): La carpeta “docs” se utiliza para almacenar la documentación de tu proyecto. Aquí puedes incluir documentos explicativos, manuales de usuario, instrucciones de instalación y cualquier otra documentación relevante. Mantener una documentación clara y organizada es esencial para que los usuarios comprendan y utilicen tu proyecto de manera efectiva.\nREADME: El archivo “README” es un documento importante que proporciona una breve descripción de tu proyecto y su propósito. Suele incluir información sobre cómo instalar y utilizar el proyecto, así como otros detalles importantes. Los usuarios suelen consultar este archivo primero cuando exploran un proyecto.\nHOW_TO_CONTRIBUTE: Este archivo contiene instrucciones para las personas que deseen contribuir al desarrollo de tu proyecto. Incluye detalles sobre cómo pueden colaborar, enviar correcciones, agregar nuevas funciones y seguir las pautas de contribución.\nCODE_OF_CONDUCT: El archivo “CODE_OF_CONDUCT” establece las reglas y pautas de comportamiento que deben seguir los colaboradores y usuarios del proyecto. Define cómo deben interactuar entre sí de manera respetuosa y profesional. También puede indicar las consecuencias en caso de violación del código de conducta.\nexamples.py: Este archivo es un script de Python que contiene ejemplos simples de cómo utilizar las funcionalidades de tu proyecto. Estos ejemplos pueden ayudar a los usuarios a comprender cómo utilizar tu código en situaciones reales y proporcionar ejemplos de uso práctico."
  },
  {
    "objectID": "posts/2023/art_docs.html#principales-librerías",
    "href": "posts/2023/art_docs.html#principales-librerías",
    "title": "Documentación",
    "section": "",
    "text": "La documentación es una parte fundamental de cualquier proyecto de desarrollo de software. Proporciona información crucial sobre cómo utilizar, mantener y contribuir al código. En el ecosistema de Python, existen varias bibliotecas y herramientas que facilitan la tarea de documentar el código de manera efectiva. En este artículo, exploraremos algunas de las principales bibliotecas de Python utilizadas para documentar código.\n\n\n\nSphinx es una de las herramientas de documentación más populares en el mundo de Python. Fue originalmente desarrollada para documentar la propia documentación de Python y se ha convertido en una elección común para proyectos de código abierto y proyectos internos. Algunas de sus características clave incluyen:\n\nGeneración de documentación en varios formatos, incluyendo HTML, PDF, ePub y más.\nUtiliza reStructuredText como su formato de marcado predeterminado, que es altamente estructurado y permite documentar de manera eficiente los aspectos técnicos.\nAmplia gama de extensiones y complementos que permiten personalizar y mejorar la documentación.\nAdmite la generación automática de documentación a partir de docstrings en el código Python.\nEs especialmente adecuado para documentar bibliotecas, API y proyectos técnicos.\n\nSphinx es altamente configurable y puede generar documentación de alta calidad y profesional. Sin embargo, puede requerir un tiempo de configuración inicial y tiene una curva de aprendizaje empinada para los principiantes.\n\n\n\n\nMkDocs es una herramienta de generación de documentación que se centra en la simplicidad y la facilidad de uso. Está diseñada para crear documentación de proyectos de una manera simple y rápida, principalmente enfocada en la generación de sitios web de documentación. Algunas de sus características clave incluyen:\n\nUtiliza Markdown como formato de marcado predeterminado, que es fácil de aprender y escribir.\nOfrece una interfaz de línea de comandos simple para iniciar y generar sitios de documentación.\nProporciona temas y extensiones para personalizar el aspecto y la funcionalidad de la documentación generada.\nIdeal para proyectos de código abierto y documentación de proyectos pequeños a medianos.\n\nMkDocs es especialmente adecuado para proyectos con necesidades de documentación simples. Es fácil de aprender y usar, lo que lo convierte en una excelente opción para principiantes. Sin embargo, puede ser limitado en funcionalidad en comparación con Sphinx para proyectos técnicos y complejos.\n\n\n\nMkDocs-Material es un tema personalizado para MkDocs, una popular herramienta de generación de sitios web estáticos diseñada para crear documentación de proyectos de manera sencilla y efectiva. Este tema, conocido como “Material for MkDocs”, se inspira en el elegante diseño de Material Design de Google y está diseñado para ofrecer una experiencia de documentación moderna y atractiva.\nUna de las principales características de MkDocs-Material es su enfoque en la legibilidad y la facilidad de navegación. Esto se logra mediante un diseño limpio y organizado que hace que la documentación sea más accesible para los usuarios. Además, el tema proporciona herramientas útiles para mejorar la experiencia de los lectores, como una función de búsqueda integrada que permite a los usuarios encontrar rápidamente la información que necesitan.\nOtra ventaja de MkDocs-Material es su navegación intuitiva, que facilita a los usuarios la exploración de la documentación y la navegación entre secciones y páginas. Esto es fundamental para garantizar que los usuarios puedan acceder fácilmente a la información que están buscando sin esfuerzo."
  },
  {
    "objectID": "posts/2023/art_docs.html#por-dónde-empiezo",
    "href": "posts/2023/art_docs.html#por-dónde-empiezo",
    "title": "Documentación",
    "section": "",
    "text": "La documentación de proyectos sigue una progresión simple:\n\nSin Documentación\nAlguna Documentación\nDocumentación Completa\nBuena Documentación\nExcelente Documentación\n\nSi te sientes perdido acerca de por dónde continuar con tu documentación, observa en qué punto se encuentra tu proyecto en relación con la progresión mencionada anteriormente. ¿Tienes alguna documentación? Si no la tienes, comienza por ahí. Si ya tienes algo de documentación pero te faltan algunos de los archivos clave del proyecto, comienza agregándolos.\nAl final, no te desanimes ni te sientas abrumado por la cantidad de trabajo necesario para documentar el código.\nUna vez que comiences a documentar tu código, te resultará más fácil seguir adelante."
  },
  {
    "objectID": "posts/2023/data_sharing.html",
    "href": "posts/2023/data_sharing.html",
    "title": "Compartir datos",
    "section": "",
    "text": "Esta guía está diseñada para todos aquellos que necesitan colaborar con un estadístico o científico de datos en el proceso de análisis de datos. Nuestro público objetivo abarca:\n\nColaboradores que requieren análisis estadísticos o de datos para sus proyectos.\nEstudiantes o posdoctorados en diversas disciplinas en busca de asesoramiento y consultoría.\nEstudiantes de estadística junior que desempeñan un papel crucial en la recopilación, limpieza y preparación de conjuntos de datos.\n\nEl propósito de esta guía es ofrecer pautas y recomendaciones para compartir datos de manera eficiente, evitando errores comunes y retrasos en la transición desde la recopilación de datos hasta su análisis.\nSostenemos firmemente la idea de que los estadísticos deben ser capaces de trabajar con los datos en cualquier estado en el que se encuentren.\nEs esencial examinar los datos en su estado crudo, comprender los pasos involucrados en su procesamiento y poder identificar fuentes ocultas de variabilidad en el análisis de datos. Sin embargo, para muchos tipos de datos, los pasos de procesamiento están bien documentados y estandarizados. Por lo tanto, la labor de transformar los datos desde su forma original a una forma directamente analizable puede realizarse antes de involucrar a un estadístico.\nEsta práctica puede acelerar significativamente el tiempo de respuesta, ya que el estadístico no tendrá que ocuparse de todos los pasos de preprocesamiento inicialmente.\n\n\nSi deseas asegurar un análisis eficiente y oportuno, es fundamental proporcionar al estadístico la siguiente información:\n\nDatos en su Estado Original (raw data): Esto incluye los datos sin procesar, tal como se recopilaron, sin ningún tipo de transformación o manipulación. Al brindar los datos en su forma bruta, permites al estadístico comprender la fuente y la calidad de la información.\nConjunto de Datos Ordenado (Tidy Data Set): Un conjunto de datos ordenado sigue los principios del “Tidy Data”, lo que significa que se organiza de manera que cada variable se encuentre en una columna y cada observación en una fila. Esta estructura facilita el análisis y la interpretación de los datos de manera efectiva.\nDetalles del Conjunto de Datos: Se debe describir minuciosamente cada variable presente en el conjunto de datos ordenado, incluyendo información sobre los valores que pueden tomar. Proporcionar un libro de códigos es esencial para que el estadístico comprenda la naturaleza de las variables y cómo interpretarlas correctamente.\nLista de Instrucciones (script): Es crucial suministrar una descripción detallada de los pasos exactos que seguiste para transformar los datos desde su estado original hasta el conjunto de datos ordenado. Esto garantiza la reproducibilidad y una comprensión completa de tu proceso por parte del estadístico.\n\n\n\nUno de los elementos fundamentales en cualquier análisis de datos es la inclusión de los datos en su forma más “cruda” y original. Esto garantiza que la procedencia de los datos se mantenga intacta a lo largo de todo el proceso de trabajo. Aquí te proporcionamos ejemplos de lo que se entiende por datos en su estado más “crudo”:\n\nUn enigmático archivo binario generado por tu máquina de medición.\nUn archivo de Excel sin procesar con 10 hojas de trabajo que has recibido de la empresa con la que colaboras.\nComplejos datos en formato JSON que has extraído al hacer web scraping de la API de Twitter.\nNúmeros ingresados manualmente mientras observabas a través de un microscopio.\n\nPuedes considerar que los datos están en su formato crudo si:\n\nNo se ha aplicado ningún software a los datos.\nLos valores de los datos no han sido alterados.\nNo se ha eliminado ninguna parte de los datos del conjunto.\nLos datos no han sido resumidos o transformados de ninguna manera.\n\nSi realizaste alguna modificación en los datos en bruto, estos ya no se consideran en su forma cruda. Reportar datos modificados como datos en bruto es una práctica común que puede ralentizar significativamente el proceso de análisis, ya que el analista a menudo debe realizar un examen minucioso de tus datos para comprender por qué los datos en su forma cruda parecen inusuales. (Imagina también cómo impactaría en futuros análisis si llegan nuevos datos). Mantener la integridad de los datos en su forma cruda es esencial para un análisis sólido y confiable.\nClaro, te proporcionaré un ejemplo práctico y aplicado en Python que ilustra los principios de datos ordenados (tidy data) de Hadley Wickham. En este ejemplo, trabajaremos con un conjunto de datos de resultados de exámenes médicos para varios pacientes.\nEjemplo Práctico\nSupongamos que tenemos un conjunto de datos de resultados de exámenes médicos para tres pacientes en una clínica. Los exámenes incluyen mediciones de presión arterial (sistólica y diastólica), nivel de glucosa y nivel de colesterol. El conjunto de datos se ve de la siguiente manera:\n\n\n\n\n\n\n\n\n\n\nPaciente\nPresion_Sistolica\nPresion_Diastolica\nGlucosa\nColesterol\n\n\n\n\n1\n120\n80\n95\n180\n\n\n2\n130\n85\n105\n190\n\n\n3\n115\n75\n90\n170\n\n\n\nEstos datos están sin procesar, tal como se recopilarían de los pacientes.\n\n\n\nLos principios generales de los datos ordenados (tidy data) son presentados por Hadley Wickham en este artículo y este video. Aunque tanto el artículo como el video describen los datos ordenados utilizando R, los principios son aplicables de manera más general:\n\nCada variable que mides debe estar en una columna.\nCada observación diferente de esa variable debe estar en una fila diferente.\nDebe haber una tabla para cada “tipo” de variable.\nSi tienes varias tablas, deben incluir una columna en la tabla que permita unirlas o combinarlas.\n\nLa idea detrás de los datos ordenados es que los datos se organicen de manera que sea fácil identificar y seleccionar variables, así como realizar análisis de manera eficiente. Siguiendo estos principios, se logra una estructura de datos que es intuitiva y facilita el trabajo de los analistas y científicos de datos, ya que no necesitan descifrar la estructura de los datos en cada proyecto. También se promueve el uso de nombres descriptivos para las variables, lo que mejora la comprensión de los datos por parte de otros colaboradores.\nEjemplo Práctico\nContinuando con el ejemplo, tenemos cuatro variables: Paciente, Presion_Sistolica, Presion_Diastolica, Glucosa y Colesterol. Así que, creamos un conjunto de datos ordenado donde cada una de estas variables tiene su propia columna:\n\n\n\nPaciente\nVariable\nValor\n\n\n\n\n1\nPresion_Sistolica\n120\n\n\n1\nPresion_Diastolica\n80\n\n\n1\nGlucosa\n95\n\n\n1\nColesterol\n180\n\n\n2\nPresion_Sistolica\n130\n\n\n2\nPresion_Diastolica\n85\n\n\n2\nGlucosa\n105\n\n\n2\nColesterol\n190\n\n\n3\nPresion_Sistolica\n115\n\n\n3\nPresion_Diastolica\n75\n\n\n3\nGlucosa\n90\n\n\n3\nColesterol\n170\n\n\n\nEsto es un conjunto de datos ordenado porque cumple con las reglas mencionadas anteriormente.\n\n\n\nPara casi cualquier conjunto de datos, las mediciones que calculas necesitarán ser descritas con más detalle de lo que puedes o debes incluir en la hoja de cálculo. El libro de códigos contiene esta información. Como mínimo, debería contener:\n\nInformación sobre las variables (¡incluyendo unidades!) en el conjunto de datos que no están contenidas en los datos ordenados (tidy data).\nInformación sobre las elecciones de resumen que hiciste.\nInformación sobre el diseño experimental que utilizaste.\n\nEjemplo Práctico\nInformación sobre las Variables:\nEn nuestro ejemplo de resultados de exámenes médicos, tenemos los detalles adicionales sobre las variables:\n\nPresion_Sistolica: Presión arterial sistólica medida en mmHg (milímetros de mercurio).\nPresion_Diastolica: Presión arterial diastólica medida en mmHg.\nGlucosa: Nivel de glucosa en sangre medido en mg/dL (miligramos por decilitro).\nColesterol: Nivel de colesterol en sangre medido en mg/dL.\n\nEstos detalles son importantes para que otros comprendan las unidades de medida y la naturaleza de las variables.\nInformación sobre las Elecciones de Resumen:\nSi realizaste alguna operación de resumen en los datos, como el cálculo de promedios o medianas, debes explicar estas elecciones en el libro de códigos. Por ejemplo, si calculaste el promedio de la presión arterial sistólica para resumir los datos, debes mencionarlo y proporcionar la fórmula utilizada.\n\nPromedio de la presión sistólica: Se calculó como la media de todas las mediciones de presión sistólica en el conjunto de datos.\n\nPromedio de la presión sistólica = (Suma de todas las mediciones de presión sistólica) / (Número de pacientes)\n\nMediana de la presión diastólica: Se calculó como la mediana de todas las mediciones de presión diastólica en el conjunto de datos.\n\nMediana de la presión diastólica = Valor central cuando las mediciones se ordenan de menor a mayor\n\n\nInformación sobre el Diseño Experimental:\nSe debe incluir detalles sobre cómo se recopilaron los datos y el diseño experimental subyacente. Por ejemplo, si los datos se recopilaron de pacientes en un estudio clínico, debes describir cómo se seleccionaron los pacientes, si hubo aleatorización en los tratamientos, y cualquier otro detalle relevante sobre el diseño del estudio.\n\nSelección de Pacientes: Los pacientes fueron seleccionados de un centro de atención médica en el que se atienden personas con afecciones cardiovasculares. La selección de pacientes se realizó de manera no aleatoria, y se incluyeron aquellos que cumplieron con los siguientes criterios de inclusión: diagnóstico de enfermedad cardiovascular confirmado, disponibilidad de datos de presión arterial, niveles de glucosa y colesterol, y consentimiento para participar en el estudio.\nAleatorización en Tratamientos: En este estudio, no se aplicó aleatorización en los tratamientos. Los pacientes recibieron el tratamiento médico habitual según las pautas clínicas establecidas por sus médicos tratantes. Por lo tanto, no se realizaron intervenciones experimentales ni asignaciones aleatorias de tratamientos.\nRecopilación de Datos: Los datos se recopilaron mediante la revisión de los registros médicos electrónicos de los pacientes. Se registraron las mediciones de presión arterial sistólica y diastólica, los niveles de glucosa en sangre y los niveles de colesterol en momentos específicos de las consultas médicas de los pacientes.\n\n\n\n\nEs posible que hayas escuchado esto antes, pero la reproducibilidad es un gran tema en la ciencia computacional. Esto significa que cuando envíes tu artículo, los revisores y el resto del mundo deberían poder replicar exactamente los análisis desde los datos crudos hasta los resultados finales. Si estás tratando de ser eficiente, es probable que realices algunas etapas de resumen/análisis de datos antes de que los datos se consideren ordenados.\nLo ideal que debes hacer al realizar el resumen es crear un script de computadora (en R, Python, u otro lenguaje) que tome los datos crudos como entrada y produzca los datos ordenados que estás compartiendo como salida. Puedes intentar ejecutar tu script algunas veces y ver si produce la misma salida.\nEn muchos casos, la persona que recopiló los datos tiene incentivos para que sean ordenados para un estadístico y acelerar el proceso de colaboración. Es posible que no sepa cómo programar en un lenguaje de script. En ese caso, lo que debes proporcionar al estadístico es algo llamado pseudocódigo. Debería verse algo como:\n\nPaso 1 - tomar el archivo crudo, ejecutar la versión 3.1.2 del software de resumen con parámetros a=1, b=2, c=3\nPaso 2 - ejecutar el software por separado para cada muestra\nPaso 3 - tomar la tercera columna del archivo de salida para cada muestra y esa es la fila correspondiente en el conjunto de datos de salida\n\nTambién debes incluir información sobre qué sistema (Mac/Windows/Linux) utilizaste para el software y si lo intentaste más de una vez para confirmar que daba los mismos resultados. Idealmente, deberías hacer que otro estudiante o compañero de laboratorio confirme que puede obtener el mismo archivo de salida que tú.\nEjemplo Práctico\nEste código se ejecutó en Python 3.8.5 y utiliza las bibliotecas pandas y IPython.display. Las dependencias y la versión de Python están indicadas en los comentarios para mayor claridad.\n# Importar las bibliotecas necesarias\nimport pandas as pd\nfrom IPython.display import display\n\n# Versión de Python utilizada: 3.8.5\n\n# Crear un DataFrame con los datos originales\ndata = {\n    'Paciente': [1, 2, 3],\n    'Presion_Sistolica': [120, 130, 115],\n    'Presion_Diastolica': [80, 85, 75],\n    'Glucosa': [95, 105, 90],\n    'Colesterol': [180, 190, 170]\n}\n\ndf_original = pd.DataFrame(data)\n\n# Crear un nuevo DataFrame en el formato deseado (unstack)\ndf_unstacked = df_original.set_index('Paciente').stack().reset_index()\ndf_unstacked.columns = ['Paciente', 'Variable', 'Valor']\n\n# Calcular diferentes estadísticas\nestadisticas = df_unstacked.groupby('Variable')['Valor'].describe()\n\n# Mostrar el DataFrame en el formato antiguo   \nprint(\"Mostrar el DataFrame en el formato antiguo:\")\ndisplay(df_original)\n\n# Mostrar el DataFrame en el formato nuevo   \nprint(\"\\nMostrar el DataFrame en el formato nuevo:\")\ndisplay(df_unstacked)\n\n# Mostrar las estadísticas básicas\nprint(\"\\nMostrar las estadísticas básicas:\")\ndisplay(estadisticas)\n\n\n\nCuando entregas un conjunto de datos debidamente ordenados, disminuye drásticamente la carga de trabajo del estadístico. Entonces, con suerte, te responderán mucho más rápido. Pero la mayoría de los estadísticos cuidadosos verificarán tu procedimiento, harán preguntas sobre las etapas que realizaste y tratarán de confirmar que pueden obtener los mismos datos ordenados que tú, al menos con verificaciones puntuales.\nDeberías esperar del estadístico:\n\nUn script de análisis que realice cada uno de los análisis (no solo instrucciones).\nEl código de computadora exacto que utilizaron para ejecutar el análisis.\nTodos los archivos de salida/figuras que generaron.\n\nEsta es la información que utilizarás para establecer la reproducibilidad y la precisión de tus resultados. Cada uno de los pasos en el análisis debe estar claramente explicado y debes hacer preguntas cuando no entiendas lo que hizo el analista. Es responsabilidad tanto del estadístico como del científico comprender el análisis estadístico.\nEs posible que no puedas realizar los análisis exactos sin el código del estadístico, pero deberías poder explicar por qué el estadístico realizó cada paso a un compañero de laboratorio o a tu investigador principal."
  },
  {
    "objectID": "posts/2023/data_sharing.html#elementos-esenciales-para-una-colaboración-exitosa",
    "href": "posts/2023/data_sharing.html#elementos-esenciales-para-una-colaboración-exitosa",
    "title": "Compartir datos",
    "section": "",
    "text": "Si deseas asegurar un análisis eficiente y oportuno, es fundamental proporcionar al estadístico la siguiente información:\n\nDatos en su Estado Original (raw data): Esto incluye los datos sin procesar, tal como se recopilaron, sin ningún tipo de transformación o manipulación. Al brindar los datos en su forma bruta, permites al estadístico comprender la fuente y la calidad de la información.\nConjunto de Datos Ordenado (Tidy Data Set): Un conjunto de datos ordenado sigue los principios del “Tidy Data”, lo que significa que se organiza de manera que cada variable se encuentre en una columna y cada observación en una fila. Esta estructura facilita el análisis y la interpretación de los datos de manera efectiva.\nDetalles del Conjunto de Datos: Se debe describir minuciosamente cada variable presente en el conjunto de datos ordenado, incluyendo información sobre los valores que pueden tomar. Proporcionar un libro de códigos es esencial para que el estadístico comprenda la naturaleza de las variables y cómo interpretarlas correctamente.\nLista de Instrucciones (script): Es crucial suministrar una descripción detallada de los pasos exactos que seguiste para transformar los datos desde su estado original hasta el conjunto de datos ordenado. Esto garantiza la reproducibilidad y una comprensión completa de tu proceso por parte del estadístico.\n\n\n\nUno de los elementos fundamentales en cualquier análisis de datos es la inclusión de los datos en su forma más “cruda” y original. Esto garantiza que la procedencia de los datos se mantenga intacta a lo largo de todo el proceso de trabajo. Aquí te proporcionamos ejemplos de lo que se entiende por datos en su estado más “crudo”:\n\nUn enigmático archivo binario generado por tu máquina de medición.\nUn archivo de Excel sin procesar con 10 hojas de trabajo que has recibido de la empresa con la que colaboras.\nComplejos datos en formato JSON que has extraído al hacer web scraping de la API de Twitter.\nNúmeros ingresados manualmente mientras observabas a través de un microscopio.\n\nPuedes considerar que los datos están en su formato crudo si:\n\nNo se ha aplicado ningún software a los datos.\nLos valores de los datos no han sido alterados.\nNo se ha eliminado ninguna parte de los datos del conjunto.\nLos datos no han sido resumidos o transformados de ninguna manera.\n\nSi realizaste alguna modificación en los datos en bruto, estos ya no se consideran en su forma cruda. Reportar datos modificados como datos en bruto es una práctica común que puede ralentizar significativamente el proceso de análisis, ya que el analista a menudo debe realizar un examen minucioso de tus datos para comprender por qué los datos en su forma cruda parecen inusuales. (Imagina también cómo impactaría en futuros análisis si llegan nuevos datos). Mantener la integridad de los datos en su forma cruda es esencial para un análisis sólido y confiable.\nClaro, te proporcionaré un ejemplo práctico y aplicado en Python que ilustra los principios de datos ordenados (tidy data) de Hadley Wickham. En este ejemplo, trabajaremos con un conjunto de datos de resultados de exámenes médicos para varios pacientes.\nEjemplo Práctico\nSupongamos que tenemos un conjunto de datos de resultados de exámenes médicos para tres pacientes en una clínica. Los exámenes incluyen mediciones de presión arterial (sistólica y diastólica), nivel de glucosa y nivel de colesterol. El conjunto de datos se ve de la siguiente manera:\n\n\n\n\n\n\n\n\n\n\nPaciente\nPresion_Sistolica\nPresion_Diastolica\nGlucosa\nColesterol\n\n\n\n\n1\n120\n80\n95\n180\n\n\n2\n130\n85\n105\n190\n\n\n3\n115\n75\n90\n170\n\n\n\nEstos datos están sin procesar, tal como se recopilarían de los pacientes.\n\n\n\nLos principios generales de los datos ordenados (tidy data) son presentados por Hadley Wickham en este artículo y este video. Aunque tanto el artículo como el video describen los datos ordenados utilizando R, los principios son aplicables de manera más general:\n\nCada variable que mides debe estar en una columna.\nCada observación diferente de esa variable debe estar en una fila diferente.\nDebe haber una tabla para cada “tipo” de variable.\nSi tienes varias tablas, deben incluir una columna en la tabla que permita unirlas o combinarlas.\n\nLa idea detrás de los datos ordenados es que los datos se organicen de manera que sea fácil identificar y seleccionar variables, así como realizar análisis de manera eficiente. Siguiendo estos principios, se logra una estructura de datos que es intuitiva y facilita el trabajo de los analistas y científicos de datos, ya que no necesitan descifrar la estructura de los datos en cada proyecto. También se promueve el uso de nombres descriptivos para las variables, lo que mejora la comprensión de los datos por parte de otros colaboradores.\nEjemplo Práctico\nContinuando con el ejemplo, tenemos cuatro variables: Paciente, Presion_Sistolica, Presion_Diastolica, Glucosa y Colesterol. Así que, creamos un conjunto de datos ordenado donde cada una de estas variables tiene su propia columna:\n\n\n\nPaciente\nVariable\nValor\n\n\n\n\n1\nPresion_Sistolica\n120\n\n\n1\nPresion_Diastolica\n80\n\n\n1\nGlucosa\n95\n\n\n1\nColesterol\n180\n\n\n2\nPresion_Sistolica\n130\n\n\n2\nPresion_Diastolica\n85\n\n\n2\nGlucosa\n105\n\n\n2\nColesterol\n190\n\n\n3\nPresion_Sistolica\n115\n\n\n3\nPresion_Diastolica\n75\n\n\n3\nGlucosa\n90\n\n\n3\nColesterol\n170\n\n\n\nEsto es un conjunto de datos ordenado porque cumple con las reglas mencionadas anteriormente.\n\n\n\nPara casi cualquier conjunto de datos, las mediciones que calculas necesitarán ser descritas con más detalle de lo que puedes o debes incluir en la hoja de cálculo. El libro de códigos contiene esta información. Como mínimo, debería contener:\n\nInformación sobre las variables (¡incluyendo unidades!) en el conjunto de datos que no están contenidas en los datos ordenados (tidy data).\nInformación sobre las elecciones de resumen que hiciste.\nInformación sobre el diseño experimental que utilizaste.\n\nEjemplo Práctico\nInformación sobre las Variables:\nEn nuestro ejemplo de resultados de exámenes médicos, tenemos los detalles adicionales sobre las variables:\n\nPresion_Sistolica: Presión arterial sistólica medida en mmHg (milímetros de mercurio).\nPresion_Diastolica: Presión arterial diastólica medida en mmHg.\nGlucosa: Nivel de glucosa en sangre medido en mg/dL (miligramos por decilitro).\nColesterol: Nivel de colesterol en sangre medido en mg/dL.\n\nEstos detalles son importantes para que otros comprendan las unidades de medida y la naturaleza de las variables.\nInformación sobre las Elecciones de Resumen:\nSi realizaste alguna operación de resumen en los datos, como el cálculo de promedios o medianas, debes explicar estas elecciones en el libro de códigos. Por ejemplo, si calculaste el promedio de la presión arterial sistólica para resumir los datos, debes mencionarlo y proporcionar la fórmula utilizada.\n\nPromedio de la presión sistólica: Se calculó como la media de todas las mediciones de presión sistólica en el conjunto de datos.\n\nPromedio de la presión sistólica = (Suma de todas las mediciones de presión sistólica) / (Número de pacientes)\n\nMediana de la presión diastólica: Se calculó como la mediana de todas las mediciones de presión diastólica en el conjunto de datos.\n\nMediana de la presión diastólica = Valor central cuando las mediciones se ordenan de menor a mayor\n\n\nInformación sobre el Diseño Experimental:\nSe debe incluir detalles sobre cómo se recopilaron los datos y el diseño experimental subyacente. Por ejemplo, si los datos se recopilaron de pacientes en un estudio clínico, debes describir cómo se seleccionaron los pacientes, si hubo aleatorización en los tratamientos, y cualquier otro detalle relevante sobre el diseño del estudio.\n\nSelección de Pacientes: Los pacientes fueron seleccionados de un centro de atención médica en el que se atienden personas con afecciones cardiovasculares. La selección de pacientes se realizó de manera no aleatoria, y se incluyeron aquellos que cumplieron con los siguientes criterios de inclusión: diagnóstico de enfermedad cardiovascular confirmado, disponibilidad de datos de presión arterial, niveles de glucosa y colesterol, y consentimiento para participar en el estudio.\nAleatorización en Tratamientos: En este estudio, no se aplicó aleatorización en los tratamientos. Los pacientes recibieron el tratamiento médico habitual según las pautas clínicas establecidas por sus médicos tratantes. Por lo tanto, no se realizaron intervenciones experimentales ni asignaciones aleatorias de tratamientos.\nRecopilación de Datos: Los datos se recopilaron mediante la revisión de los registros médicos electrónicos de los pacientes. Se registraron las mediciones de presión arterial sistólica y diastólica, los niveles de glucosa en sangre y los niveles de colesterol en momentos específicos de las consultas médicas de los pacientes.\n\n\n\n\nEs posible que hayas escuchado esto antes, pero la reproducibilidad es un gran tema en la ciencia computacional. Esto significa que cuando envíes tu artículo, los revisores y el resto del mundo deberían poder replicar exactamente los análisis desde los datos crudos hasta los resultados finales. Si estás tratando de ser eficiente, es probable que realices algunas etapas de resumen/análisis de datos antes de que los datos se consideren ordenados.\nLo ideal que debes hacer al realizar el resumen es crear un script de computadora (en R, Python, u otro lenguaje) que tome los datos crudos como entrada y produzca los datos ordenados que estás compartiendo como salida. Puedes intentar ejecutar tu script algunas veces y ver si produce la misma salida.\nEn muchos casos, la persona que recopiló los datos tiene incentivos para que sean ordenados para un estadístico y acelerar el proceso de colaboración. Es posible que no sepa cómo programar en un lenguaje de script. En ese caso, lo que debes proporcionar al estadístico es algo llamado pseudocódigo. Debería verse algo como:\n\nPaso 1 - tomar el archivo crudo, ejecutar la versión 3.1.2 del software de resumen con parámetros a=1, b=2, c=3\nPaso 2 - ejecutar el software por separado para cada muestra\nPaso 3 - tomar la tercera columna del archivo de salida para cada muestra y esa es la fila correspondiente en el conjunto de datos de salida\n\nTambién debes incluir información sobre qué sistema (Mac/Windows/Linux) utilizaste para el software y si lo intentaste más de una vez para confirmar que daba los mismos resultados. Idealmente, deberías hacer que otro estudiante o compañero de laboratorio confirme que puede obtener el mismo archivo de salida que tú.\nEjemplo Práctico\nEste código se ejecutó en Python 3.8.5 y utiliza las bibliotecas pandas y IPython.display. Las dependencias y la versión de Python están indicadas en los comentarios para mayor claridad.\n# Importar las bibliotecas necesarias\nimport pandas as pd\nfrom IPython.display import display\n\n# Versión de Python utilizada: 3.8.5\n\n# Crear un DataFrame con los datos originales\ndata = {\n    'Paciente': [1, 2, 3],\n    'Presion_Sistolica': [120, 130, 115],\n    'Presion_Diastolica': [80, 85, 75],\n    'Glucosa': [95, 105, 90],\n    'Colesterol': [180, 190, 170]\n}\n\ndf_original = pd.DataFrame(data)\n\n# Crear un nuevo DataFrame en el formato deseado (unstack)\ndf_unstacked = df_original.set_index('Paciente').stack().reset_index()\ndf_unstacked.columns = ['Paciente', 'Variable', 'Valor']\n\n# Calcular diferentes estadísticas\nestadisticas = df_unstacked.groupby('Variable')['Valor'].describe()\n\n# Mostrar el DataFrame en el formato antiguo   \nprint(\"Mostrar el DataFrame en el formato antiguo:\")\ndisplay(df_original)\n\n# Mostrar el DataFrame en el formato nuevo   \nprint(\"\\nMostrar el DataFrame en el formato nuevo:\")\ndisplay(df_unstacked)\n\n# Mostrar las estadísticas básicas\nprint(\"\\nMostrar las estadísticas básicas:\")\ndisplay(estadisticas)\n\n\n\nCuando entregas un conjunto de datos debidamente ordenados, disminuye drásticamente la carga de trabajo del estadístico. Entonces, con suerte, te responderán mucho más rápido. Pero la mayoría de los estadísticos cuidadosos verificarán tu procedimiento, harán preguntas sobre las etapas que realizaste y tratarán de confirmar que pueden obtener los mismos datos ordenados que tú, al menos con verificaciones puntuales.\nDeberías esperar del estadístico:\n\nUn script de análisis que realice cada uno de los análisis (no solo instrucciones).\nEl código de computadora exacto que utilizaron para ejecutar el análisis.\nTodos los archivos de salida/figuras que generaron.\n\nEsta es la información que utilizarás para establecer la reproducibilidad y la precisión de tus resultados. Cada uno de los pasos en el análisis debe estar claramente explicado y debes hacer preguntas cuando no entiendas lo que hizo el analista. Es responsabilidad tanto del estadístico como del científico comprender el análisis estadístico.\nEs posible que no puedas realizar los análisis exactos sin el código del estadístico, pero deberías poder explicar por qué el estadístico realizó cada paso a un compañero de laboratorio o a tu investigador principal."
  },
  {
    "objectID": "about-es.html",
    "href": "about-es.html",
    "title": "Francisco Alfaro M.",
    "section": "",
    "text": "English Español\n\n\n\nHola👋 Mi nombre es Francisco!\n\n\n\n\nProfesión: 📊 Ingeniero Matemático\nTrabajo Actual:\n\n💻 Jefe de Analítica Avanzada (Grupo Security)\n📖 Profesor Asociado (UTFSM)\n\n\n\nIntereses\n🎮 Videojuegos | 🏀 Baloncesto | 💡 Aprendizaje\n✅ Desarrollo de Software  ✅ Modelado Estadístico, Series Temporales  ✅ Aprendizaje Automático/Profundo  ✅ Computación en la Nube, Big Data"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Francisco Alfaro M.",
    "section": "",
    "text": "English Español\n\n\n\nHi 👋 My name is Francisco!\n\n\n\n\nProfession: 📊 Mathematical Engineer\nCurrent Work:\n\n💻 Head of Advanced Analytics (Grupo Security)\n📖 Associate Lecturer (UTFSM)\n\n\n\nInterests\n🎮 Gaming | 🏀 Basketball | 💡 Learning\n✅ Software Development  ✅ Statistical Modelling, Time Series  ✅ Machine/Deep Learning  ✅ Cloud computing, Big Data"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Posts",
    "section": "",
    "text": "Compartir datos\n\n\nCómo Compartir Datos de Manera Efectiva (desde el punto de vista estadístico) y no morir en el intento.\n\n\n\npython\n\n\ndata-analysis\n\n\n\n\n\n\n\n\n\n26 oct 2023\n\n\nFrancisco Alfaro\n\n\n\n\n\n\n\n\n\n\n\n\nDocumentación\n\n\nEnteneder los pasos para crear una buena documentación en Python (más algunas recomendaciones).\n\n\n\npython\n\n\ndocs\n\n\n\n\n\n\n\n\n\n7 oct 2023\n\n\nFrancisco Alfaro\n\n\n\n\n\n\n\n\n\n\n\n\nGitlab PDF\n\n\nCómo aprovechar GitLab CI/CD para generar archivos PDF utilizando los artefactos de un Pipeline.\n\n\n\nci-cd\n\n\nsoftware-development\n\n\n\n\n\n\n\n\n\n1 oct 2023\n\n\nFrancisco Alfaro\n\n\n\n\n\n\n\n\n\n\n\n\nCausal Impact\n\n\nCausalImpact creado por Google estima el impacto de una intervención en una serie temporal.\n\n\n\npython\n\n\nmachine-learning\n\n\n\n\n\n\n\n\n\n15 oct 2022\n\n\nFrancisco Alfaro\n\n\n\n\n\n\n\n\n\n\n\n\nCollaborative Filtering\n\n\nUser-based collaborative filtering para realizar un mejor sistema de recomendación de películas.\n\n\n\npython\n\n\nmachine-learning\n\n\n\n\n\n\n\n\n\n12 oct 2022\n\n\nFrancisco Alfaro\n\n\n\n\n\n\n\n\n\n\n\n\nTest Driven Development\n\n\nCómo abordar el desarrollo de software para Data Science usando Test Driven Development.\n\n\n\npython\n\n\nsoftware-development\n\n\n\n\n\n\n\n\n\n25 may 2022\n\n\nFrancisco Alfaro\n\n\n\n\n\n\n\n\n\n\n\n\nPolars\n\n\nPolars es una librería de DataFrames increíblemente rápida y eficiente implementada en Rust.\n\n\n\npython\n\n\ndata-analysis\n\n\n\n\n\n\n\n\n\n25 may 2022\n\n\nFrancisco Alfaro\n\n\n\n\n\n\n\n\n\n\n\n\nImpact on Digital Learning\n\n\nCompetition Solution: LearnPlatform COVID-19 Impact on Digital Learning proposed by Kaggle.\n\n\n\nkaggle\n\n\ndata-analysis\n\n\n\n\n\n\n\n\n\n31 ago 2021\n\n\nFrancisco Alfaro\n\n\n\n\n\n\n\n\n\n\n\n\nFastpages\n\n\nFastpages es una plataforma que te permite crear y alojar un blog con Jupyter Notebooks.\n\n\n\npython\n\n\njupyter-notebooks\n\n\n\n\n\n\n\n\n\n20 ago 2021\n\n\nFrancisco Alfaro\n\n\n\n\n\n\n\n\n\n\n\n\nBuenas Prácticas - Python\n\n\nConsejos que te ayudarán a mejorar tus skills en el desarrollo de software (con Python).\n\n\n\npython\n\n\nsoftware-development\n\n\n\n\n\n\n\n\n\n15 ago 2021\n\n\nFrancisco Alfaro\n\n\n\n\n\n\n\n\n\n\n\n\nJupyter Book\n\n\nJupyter Book es una herramienta para crear documentos mediante Jupyter Notebooks y/o Markdown.\n\n\n\npython\n\n\njupyter-notebooks\n\n\n\n\n\n\n\n\n\n11 ago 2021\n\n\nFrancisco Alfaro\n\n\n\n\n\n\n\n\n\n\n\n\nRISE\n\n\nRISE es una extensión a los Jupyter Notebooks que permite transformar tus notebooks en presentaciones interactivas.\n\n\n\npython\n\n\njupyter-notebooks\n\n\n\n\n\n\n\n\n\n5 ago 2021\n\n\nFrancisco Alfaro\n\n\n\n\n\n\n\n\n\n\n\n\nJupyter Noteboook\n\n\nJupyter Notebook, es un entorno de trabajo interactivo que permite desarrollar código en Python.\n\n\n\npython\n\n\njupyter-notebooks\n\n\n\n\n\n\n\n\n\n31 jul 2021\n\n\nFrancisco Alfaro\n\n\n\n\n\n\nNo hay resultados"
  },
  {
    "objectID": "posts/2023/gitlab_pdf.html",
    "href": "posts/2023/gitlab_pdf.html",
    "title": "Gitlab PDF",
    "section": "",
    "text": "GitLab CI/CD es una potente herramienta que permite automatizar y gestionar el ciclo de vida de las aplicaciones de software.\nCI (Integración Continua) y CD (Entrega Continua) son prácticas esenciales en el desarrollo de software moderno que buscan mejorar la calidad del código, aumentar la eficiencia y reducir los errores. GitLab CI/CD se integra de manera nativa en el flujo de trabajo de GitLab, lo que lo convierte en una opción atractiva para equipos de desarrollo.\n\nCI (Integración Continua): Es el proceso de integrar cambios de código frecuentes en un repositorio compartido. Esto implica la ejecución automática de pruebas y análisis de calidad cada vez que se envía código. El objetivo es identificar y corregir problemas de manera temprana en el ciclo de desarrollo.\nCD (Entrega Continua): Una vez que las pruebas de CI se han superado con éxito, el código se considera apto para su implementación en entornos de producción o de pruebas. El objetivo es entregar de manera eficiente y confiable el software a los usuarios finales.\n\n\n\n\nAntes de profundizar en la generación de archivos PDF, es importante comprender el concepto de “artefactos” en GitLab CI/CD. Los artefactos son archivos o conjuntos de archivos generados como resultado de una ejecución exitosa de un pipeline.\nEstos artefactos se almacenan en GitLab y se pueden utilizar posteriormente en otros trabajos o pipelines.\nEn el contexto de la generación de archivos PDF, los artefactos son esenciales porque permiten que los archivos PDF generados en un trabajo se conserven y utilicen en otros trabajos o etapas del pipeline.\n\n\n\n\nLa generación de archivos PDF como parte de su proceso de CI/CD puede ser útil en varios escenarios, como la creación de informes automatizados, la generación de documentación técnica o la producción de facturas en línea.\nA continuación, detallaremos cómo lograrlo utilizando GitLab CI/CD:\n\nNota: Tomaremos como referencia el siguiente repositorio.\n\n\n\nAntes de comenzar, asegúrese de que su proyecto de GitLab esté configurado correctamente y tenga acceso a GitLab CI/CD. También debe tener un archivo de código fuente que desee convertir en un archivo PDF.\nAsegúrese de que cualquier dependencia necesaria esté especificada en su archivo de configuración de CI/CD.\n\n\n\n\nCree un script que sea capaz de generar el archivo PDF a partir de sus datos de entrada.\nEste script debería tomar los datos relevantes y formatearlos en un archivo PDF.\n\n\n\n\nEn su repositorio de GitLab, cree un archivo llamado .gitlab-ci.yml si aún no lo ha hecho.\nEste archivo contiene la configuración de su pipeline.\nAquí hay un ejemplo de cómo podría verse:\nstages:\n  - pdf\n\ngenerate_pdf:\n  stage: pdf\n  image: aergus/latex\n  script:\n    - latexmk -pdf **/*.tex\n\n  artifacts:\n    paths:\n      - ./*.pdf\nEn este ejemplo:\nstages:\n  - pdf\n\nstages: Esta sección define las etapas (stages). En este caso, solo se define una etapa llamada “pdf”. Las etapas son divisiones lógicas en el pipeline que agrupan trabajos relacionados. En este caso, el pipeline tiene una sola etapa llamada “pdf”.\n\ngenerate_pdf:\n  stage: pdf\n  image: aergus/latex\n  script:\n    - latexmk -pdf **/*.tex\n\ngenerate_pdf: Esta sección define un trabajo (job) llamado “generate_pdf”. Un trabajo es una unidad de ejecución en el pipeline. Aquí está el desglose de esta sección:\n\nstage: pdf: Esta línea especifica que este trabajo pertenece a la etapa “pdf” definida previamente. En otras palabras, este trabajo se ejecutará en la etapa “pdf” del pipeline.\nimage: aergus/latex: Esta línea especifica la imagen Docker que se utilizará para ejecutar este trabajo. En este caso, se utiliza la imagen “aergus/latex”, que contiene un entorno LaTeX para compilar documentos PDF. Esta imagen es esencial para compilar archivos LaTeX en archivos PDF.\nscript: Aquí se definen los comandos que se ejecutarán en el trabajo. En este caso, se utiliza el comando “latexmk -pdf **/*.tex”. Este comando utiliza “latexmk” para compilar todos los archivos “.tex” en el proyecto en archivos PDF. El uso de **/*.tex significa que buscará archivos “.tex” en todos los subdirectorios del proyecto.\n\n\nartifacts:\n  paths:\n    - ./*.pdf\n\nartifacts: Esta sección especifica qué archivos deben considerarse artefactos y, por lo tanto, se conservarán después de una ejecución exitosa del trabajo. Aquí está el desglose de esta sección:\n\npaths: ./*.pdf: Esta línea especifica que todos los archivos con extensión “.pdf” en el directorio actual deben considerarse artefactos. Esto significa que los archivos PDF generados como resultado de la ejecución de este trabajo se conservarán y estarán disponibles para su descarga después de el pipeline se haya ejecutado con éxito.\n\n\n\n\n\nCada vez que realice un envío de código (push) o active manualmente el pipeline, GitLab ejecutará el trabajo de generación de PDF. El script generará el archivo PDF y lo almacenará como un artefacto.\n\n\n\n\nUna vez que el pipeline se haya ejecutado con éxito, puede acceder a los archivos PDF generados en GitLab.\nVaya a la página de su proyecto en GitLab, seleccione “CI/CD” y luego “Artefactos”.\nAquí encontrará el archivo PDF generado que puede descargar.\n\n\n\n\nGitLab CI/CD es una herramienta poderosa que puede ayudar en la automatización de una amplia variedad de tareas, incluida la generación de archivos PDF.\nAl comprender cómo utilizar artefactos en GitLab CI/CD y seguir los pasos mencionados anteriormente, puede integrar fácilmente la generación de PDF en su flujo de trabajo de desarrollo, lo que ahorra tiempo y esfuerzo, y garantiza la consistencia y la calidad en la creación de documentos PDF automatizados."
  },
  {
    "objectID": "posts/2023/gitlab_pdf.html#artefactos-en-gitlab-cicd",
    "href": "posts/2023/gitlab_pdf.html#artefactos-en-gitlab-cicd",
    "title": "Gitlab PDF",
    "section": "",
    "text": "Antes de profundizar en la generación de archivos PDF, es importante comprender el concepto de “artefactos” en GitLab CI/CD. Los artefactos son archivos o conjuntos de archivos generados como resultado de una ejecución exitosa de un pipeline.\nEstos artefactos se almacenan en GitLab y se pueden utilizar posteriormente en otros trabajos o pipelines.\nEn el contexto de la generación de archivos PDF, los artefactos son esenciales porque permiten que los archivos PDF generados en un trabajo se conserven y utilicen en otros trabajos o etapas del pipeline."
  },
  {
    "objectID": "posts/2023/gitlab_pdf.html#generación-de-archivos-pdf-con-gitlab-cicd",
    "href": "posts/2023/gitlab_pdf.html#generación-de-archivos-pdf-con-gitlab-cicd",
    "title": "Gitlab PDF",
    "section": "",
    "text": "La generación de archivos PDF como parte de su proceso de CI/CD puede ser útil en varios escenarios, como la creación de informes automatizados, la generación de documentación técnica o la producción de facturas en línea.\nA continuación, detallaremos cómo lograrlo utilizando GitLab CI/CD:\n\nNota: Tomaremos como referencia el siguiente repositorio.\n\n\n\nAntes de comenzar, asegúrese de que su proyecto de GitLab esté configurado correctamente y tenga acceso a GitLab CI/CD. También debe tener un archivo de código fuente que desee convertir en un archivo PDF.\nAsegúrese de que cualquier dependencia necesaria esté especificada en su archivo de configuración de CI/CD.\n\n\n\n\nCree un script que sea capaz de generar el archivo PDF a partir de sus datos de entrada.\nEste script debería tomar los datos relevantes y formatearlos en un archivo PDF.\n\n\n\n\nEn su repositorio de GitLab, cree un archivo llamado .gitlab-ci.yml si aún no lo ha hecho.\nEste archivo contiene la configuración de su pipeline.\nAquí hay un ejemplo de cómo podría verse:\nstages:\n  - pdf\n\ngenerate_pdf:\n  stage: pdf\n  image: aergus/latex\n  script:\n    - latexmk -pdf **/*.tex\n\n  artifacts:\n    paths:\n      - ./*.pdf\nEn este ejemplo:\nstages:\n  - pdf\n\nstages: Esta sección define las etapas (stages). En este caso, solo se define una etapa llamada “pdf”. Las etapas son divisiones lógicas en el pipeline que agrupan trabajos relacionados. En este caso, el pipeline tiene una sola etapa llamada “pdf”.\n\ngenerate_pdf:\n  stage: pdf\n  image: aergus/latex\n  script:\n    - latexmk -pdf **/*.tex\n\ngenerate_pdf: Esta sección define un trabajo (job) llamado “generate_pdf”. Un trabajo es una unidad de ejecución en el pipeline. Aquí está el desglose de esta sección:\n\nstage: pdf: Esta línea especifica que este trabajo pertenece a la etapa “pdf” definida previamente. En otras palabras, este trabajo se ejecutará en la etapa “pdf” del pipeline.\nimage: aergus/latex: Esta línea especifica la imagen Docker que se utilizará para ejecutar este trabajo. En este caso, se utiliza la imagen “aergus/latex”, que contiene un entorno LaTeX para compilar documentos PDF. Esta imagen es esencial para compilar archivos LaTeX en archivos PDF.\nscript: Aquí se definen los comandos que se ejecutarán en el trabajo. En este caso, se utiliza el comando “latexmk -pdf **/*.tex”. Este comando utiliza “latexmk” para compilar todos los archivos “.tex” en el proyecto en archivos PDF. El uso de **/*.tex significa que buscará archivos “.tex” en todos los subdirectorios del proyecto.\n\n\nartifacts:\n  paths:\n    - ./*.pdf\n\nartifacts: Esta sección especifica qué archivos deben considerarse artefactos y, por lo tanto, se conservarán después de una ejecución exitosa del trabajo. Aquí está el desglose de esta sección:\n\npaths: ./*.pdf: Esta línea especifica que todos los archivos con extensión “.pdf” en el directorio actual deben considerarse artefactos. Esto significa que los archivos PDF generados como resultado de la ejecución de este trabajo se conservarán y estarán disponibles para su descarga después de el pipeline se haya ejecutado con éxito.\n\n\n\n\n\nCada vez que realice un envío de código (push) o active manualmente el pipeline, GitLab ejecutará el trabajo de generación de PDF. El script generará el archivo PDF y lo almacenará como un artefacto.\n\n\n\n\nUna vez que el pipeline se haya ejecutado con éxito, puede acceder a los archivos PDF generados en GitLab.\nVaya a la página de su proyecto en GitLab, seleccione “CI/CD” y luego “Artefactos”.\nAquí encontrará el archivo PDF generado que puede descargar."
  },
  {
    "objectID": "posts/2023/gitlab_pdf.html#conclusiones",
    "href": "posts/2023/gitlab_pdf.html#conclusiones",
    "title": "Gitlab PDF",
    "section": "",
    "text": "GitLab CI/CD es una herramienta poderosa que puede ayudar en la automatización de una amplia variedad de tareas, incluida la generación de archivos PDF.\nAl comprender cómo utilizar artefactos en GitLab CI/CD y seguir los pasos mencionados anteriormente, puede integrar fácilmente la generación de PDF en su flujo de trabajo de desarrollo, lo que ahorra tiempo y esfuerzo, y garantiza la consistencia y la calidad en la creación de documentos PDF automatizados."
  },
  {
    "objectID": "posts/2021/2021-08-11-jb.html",
    "href": "posts/2021/2021-08-11-jb.html",
    "title": "Jupyter Book",
    "section": "",
    "text": "Jupyter Book es un proyecto de código abierto para crear libros y documentos mediante Jupyter Notebooks y/o Markdown.\nAlgunas características importantes del uso de Jupyter Book:\n\ncontenido con calidad de publicación que incluya figuras, símbolos matemáticos, citas y referencias cruzadas!\nescribir contenido como Jupyter Notebooks, markdown o reStructuredText\nAgregue interactividad a su libro, por ejemplo, alternar la visibilidad de las celdas, conectarse con un servicio en línea como Binder e incluir resultados interactivos (por ejemplo, figuras y widgets).\ngenerar una variedad de resultados, incluidos sitios web (HTML, CSS, JS), markdown y PDF.\nuna interfaz de línea de comandos para crear libros rápidamente, por ejemplo, jupyter-book build mybook\n\nEn esta sesión, se muestra un ejemplo de cómo crear un Jupyter Book desde cero y algunas de las características clave que ofrece Jupyter Book.\n\nNota: Puede encontrar los códigos de este ejemplo en el siguiente repositorio. Por otro lado, puede revisar el siguente link para ver la compilación con GitLab CI/CD.\n\n\n\n\n\n\nPara instalar Jupyter Book, necesitará usar la línea de comando. Si ha instalado Anaconda, puede usar:\nconda install -c conda-forge jupyter-book\nDe lo contrario, puede instalar con pip:\npip install jupyter-book\n\n\n\nJupyter Book viene con una herramienta que le permite crear y construir libros rápidamente. Para crear el esqueleto del libro, escriba lo siguiente en la línea de comando:\njupyter-book create jupiter\n\nNota: Aquí llamamos al libro jupiter, pero puedes elegir llamar a tu libro como quieras.\n\nAhora tendrás un nuevo directorio llamado jupiter (o como quieras llamar a tu libro), con el siguiente contenido:\njupiter\n  ├── _config.yml\n  ├── _toc.yml\n  ├── content.md\n  ├── intro.md\n  ├── markdown.md\n  ├── notebooks.ipynb\n  ├── references.bib\n  └── requirements.txt\nen donde: * _config.yml: archivo que contiene las configuraciones del proyecto. * _toc.yml: archivo que ordena los capítulos del libro. * content.md: archivo genérico .md. * intro.md: archivo genérico .md. * markdown.md: archivo genérico .md. * notebooks.ipynb: archivo genérico .ipynb. * references.bib: archivo para añadir las referencias. * requirements.txt: archivo que contiene las dependencias python) del proyecto.\n\n\n\nJupyter Book admite varios tipos de archivos:\n\nMarkdown (.md)\nnotebooks (.ipynb)\netc.\n\nComo Markdown y Jupyter Notebooks probablemente serán los tipos de archivo más comunes que usará, se mostrará un ejemplo de ello.\nLo primero será eliminar los archivos de inicio en el directorio:\n\ncontent.md\nintro.md\nmarkdown.md\nnotebooks.ipynb\n\nAsí que ejecutamos por línea de comando:\nrm content.md intro.md markdown.md notebooks.ipynb\nPor otro lado, nuestro proyecto estaré conformado por tres archivos:\n\nindex.md\nIntroduction.md\ngreat_red_spot.ipynb\n\nLuego, debemos indicar cómo serán mostrados estos documentos en el archivo _toc.yml. La estructura será la siguiente:\nformat: jb-book\nroot: index\nchapters:\n- file: Introduction\n- file: great_red_spot\nEn este caso, root: index corresponde al primer archivo que se visualiza en el jupyter-book. Dentro del archivo index.md escribiremos:\n# Home\n\njupyter book example\n\n## Contenidos\n\n\n```roknnpsbfrmktjfhd\n```\n\n\n\n\n\n\nSe comienza por agregar un archivo de markdown. Con algún editor a elección (por ejemplo, jupyter notebook o jupyterlab) se crea un nuevo archivo markdown llamado Introduction.md.\nSe usa este archivo como demostración de algunos de los principales tipos de contenido que puede agregar en Jupyter-Book.\n\n\nSe agrega un texto de Markdown simple a nuestro archivo. Si no está familiarizado con la sintaxis de markdown, consulte Markdown Cheat Sheet. Puede copiar y pegar el siguiente contenido directamente en su archivo Introduction.md.\n# Jupiter Book\n\nThis book contains information about the planet **Jupiter** - the fifth planet from the sun and the largest planet in the solar system!\n\n\n\nPuedes incluir figuras en tu Jupyter Book usando la siguiente sintaxis:\n```gkwoiqvj my_image.png\n---\nheight: 150px\nname: my-image\n---\nHere is my image's caption!\n```\n\nSi bien la imagen puede estar contenida y referenciada desde el directorio raíz, también se puede incluir imágenes a través de URL. Incluyamos una imagen del planeta Júpiter en nuestro archivo Introduction.md usando lo siguiente:\n```gkwoiqvj https://solarsystem.nasa.gov/system/resources/detail_files/2486_stsci-h-p1936a_1800.jpg\n---\nheight: 300px\nname: jupiter-figure\n---\nThe beautiful planet Jupiter!\n```\n\nLa razón por la que le damos un “nombre” a nuestra imagen es para que podamos hacer referencia a ella fácilmente con la sintaxis:\n{numref}`jupiter-figure`\nSe agregará una oración que incluya esta referencia. El archivo completo ahora debería verse así:\n# Jupiter Book\n\nThis book contains information about the planet Jupiter - the fifth planet from the sun and the largest planet in the solar system! {numref}`jupiter-figure` below shows an image of Jupiter captured by the Hubble Space Telescope on June 27, 2019.\n\n```gkwoiqvj https://solarsystem.nasa.gov/system/resources/detail_files/2486_stsci-h-p1936a_1800.jpg\n---\nheight: 300px\nname: jupiter-figure\n---\nThe beautiful planet Jupiter! Source: [NASA](https://solarsystem.nasa.gov/resources/2486/hubbles-new-portrait-of-jupiter/?category=planets_jupiter).\n```\n\nEn este punto, probablemente se debería crear nuetro libro para asegurarnos de que tenga el aspecto esperado. Para hacer eso, primero necesitamos modificar nuestro archivo _toc.yml. Este archivo contiene la tabla de contenido de nuestro libro. Abra ese archivo ahora y elimine todo lo que hay allí. Luego, simplemente agregue lo siguiente:\n- file: introduction\nAhora podemos construir nuestro libro desde la línea de comandos asegurándonos de que estamos en el directorio raíz de nuestro libro y luego usando:\njupyter-book build .\nUna vez finalizada la compilación, tendrá un nuevo subdirectorio llamado_build/html/ en la raíz de su libro, navegue hasta esa ubicación y abra _build/html/index.html. Debería verse algo como esto:\n\n\n\n\nJupyter Book usa MathJax para componer matemáticas, lo que le permite agregar matemáticas de estilo LaTeX a su libro. Puede agregar matemáticas en línea, bloques matemáticos y ecuaciones numeradas a su libro Jupyter. Sigamos adelante y creemos un nuevo encabezado en nuestro archivo Introduction.md que incluye algunas matemáticas.\nLas matemáticas en línea se pueden definir usando $ de la siguiente manera:\nJupiter has a mass of:  $m_{j} \\approx 1.9 \\times 10^{27} kg$\n\nJupiter has a mass of: \\(m_{j} \\approx 1.9 \\times 10^{27} kg\\)\nLos bloques matemáticos se pueden definir usando la notación $$:\n$$m_{j} \\approx 1.9 \\times 10^{27} kg$$\n\n\\[m_{j} \\approx 1.9 \\times 10^{27} kg\\]\n\nNota: Si lo prefiere, los bloques matemáticos también se pueden definir con \\begin{equation} en lugar de $$.\n\nLas ecuaciones numeradas se pueden definir así (este es el estilo que te recomiendo que uses con más frecuencia):\n```lbgrcp\n:label: my_label\nm_{j} \\approx 1.9 \\times 10^{27} kg\n```\n\n\nAgreguemos más contenido a nuestro libro. Copie y agregue el siguiente texto a su archivo Introduction.md:\n## The Mass of Jupiter\n\nWe can estimate the mass of Jupiter from the period and size of an object orbiting it. For example, we can use Jupiter's moon Callisto to estimate it's mass.\n\nCallisto's period: $p_{c}=16.7 days$\n\nCallisto's orbit radius: $r_{c}=1,900,000 km$\n\nNow, using [Kepler's Law](https://solarsystem.nasa.gov/resources/310/orbits-and-keplers-laws/) we can work out the mass of Jupiter.\n\n\n```lbgrcp\n:label: eq1\nm_{j} \\approx \\frac{r_{c}}{p_{c}} \\times 7.9 \\times 10^{10}\n```\n\n```lbgrcp\n:label: eq2\nm_{j} \\approx 1.9 \\times 10^{27} kg\n```\n\n\nA continuación, puede reconstruir su libro (jupyter-book build .) y abrir _build/html/index.html para asegurarse de que todo se esté procesando como se esperaba.\n\n\n\n\nHay varias formas diferentes de controlar el diseño de las páginas de su Jupyter Book. El cambio de diseño que utilizo con más frecuencia es agregar contenido a un margen en la página. Puede agregar un margen usando la siguiente directiva:\n```txbrpgfy An optional title\nSome margin content.\n```\n\nAgreguemos algo de contenido marginal al libro:\n```txbrpgfy Did you know?\nJupiter is 11.0x larger than Earth!\n```\n\n\n\n\nHay todo tipo de advertencias diferentes que puede usar en Jupyter Book que se enumeran aquí en la documentación de Jupyter Book. Las advertencias se crean con la sintaxis:\n```uzejlz\nI am a useful note!\n```\n\n\nNo dude en agregar la siguiente advertencia a Introduction.md:\n```lgcccw\nNASA provides a lot more information about the physical characteristics of Jupiter [here](https://solarsystem.nasa.gov/planets/jupiter/by-the-numbers/).\n```\n\n\n\n\n\nEl último contenido corresponde a referencias y una bibliografía. Puede agregar citas de cualquier trabajo almacenado en el archivo Bibtex Reference.bib que se encuentra en el directorio raíz de su libro.\nPara incluir una cita en su libro, agregue una entrada bibtex a references.bib, por ejemplo:\n@article{mayor1995jupiter,\n    title={A Jupiter-mass companion to a solar-type star},\n    author={Mayor, Michel and Queloz, Didier},\n    journal={Nature},\n    volume={378},\n    number={6555},\n    pages={355--359},\n    year={1995},\n    publisher={Nature Publishing Group}\n}\n\n@article{guillot1999interiors,\n    title={Interiors of giant planets inside and outside the solar system},\n    author={Guillot, Tristan},\n    journal={Science},\n    volume={286},\n    number={5437},\n    pages={72--77},\n    year={1999},\n    publisher={American Association for the Advancement of Science}\n}\n\nNota: Consulte la documentación de BibTex para obtener información sobre el estilo de referencia de BibTex. Google Scholar facilita la exportación de un formato de cita bibtex.\n\nA continuación, puede hacer referencia al trabajo en su libro utilizando la siguiente directiva:\n{cite}`mayor1995jupiter`\nO para múltiples citas:\n{cite}`mayor1995jupiter,guillot1999interiors`\nLuego puede crear una bibliografía a partir de reference.bib usando:\n```vrvfdaunxkbvhs references.bib\n```\n\nPor ejemplo, intente agregar esto a su archivo Introduction.md:\nThere might even be more planets out there with a similar mass to Jupiter {cite}`mayor1995jupiter,guillot1999interiors`!\n\n## Bibliography\n\n```vrvfdaunxkbvhs references.bib\n```\n\nSu archivo final Introduction.md debería verse así:\n# Jupiter Book\n\nThis book contains information about the planet Jupiter - the fifth planet from the sun and the largest planet in the solar system! {numref}`jupiter-figure` below shows an image of Jupiter captured by the Hubble Space Telescope on June 27, 2019.\n\n```gkwoiqvj https://solarsystem.nasa.gov/system/resources/detail_files/2486_stsci-h-p1936a_1800.jpg\n---\nheight: 300px\nname: jupiter-figure\n---\nThe beautiful planet Jupiter! Source: [NASA](https://solarsystem.nasa.gov/resources/2486/hubbles-new-portrait-of-jupiter/?category=planets_jupiter).\n```\n\n## The Mass of Jupiter\n\nWe can estimate the mass of Jupiter from the period and size of an object orbiting it. For example, we can use Jupiter's moon Callisto to estimate it's mass.\n\nCallisto's period: $p_{c}=16.7 days$\n\nCallisto's orbit radius: $r_{c}=1,900,000 km$\n\nNow, using [Kepler's Law](https://solarsystem.nasa.gov/resources/310/orbits-and-keplers-laws/) we can work out the mass of Jupiter.\n\n\n```lbgrcp\n:label: eq1\nm_{j} \\approx \\frac{r_{c}}{p_{c}} \\times 7.9 \\times 10^{10}\n```\n\n```lbgrcp\n:label: eq2\nm_{j} \\approx 1.9 \\times 10^{27} kg\n```\n\n\n```txbrpgfy Did you know?\nJupiter is 11.0x larger than Earth!\n```\n\n\n```lgcccw\nNASA provides a lot more information about the physical characteristics of Jupiter [here](https://solarsystem.nasa.gov/planets/jupiter/by-the-numbers/).\n```\n\n\nThere might even be more planets out there with a similar mass to Jupiter {cite}`mayor1995jupiter,guillot1999interiors`!\n\n## Bibliography\n\n```vrvfdaunxkbvhs references.bib\n```\n\nY debería renderizarse así: \n\n\n\nTodos los flujos de trabajo de formato y estilo que vimos en markdown también se aplican a un Jupyter Notebook; simplemente agréguelos a una celda de markdown y listo.\nComencemos con lo siguiente:\n\nCree un nuevo notebook llamado great_red_spot.ipynb;\nAgregue este archivo a su _toc.yml;\nAgregue una celda de markdown con el siguiente contenido:\n\n# The Great Red Spot\n\nJupiter’s iconic Great Red Spot (GRS) is actually an enormous storm that is bigger than Earth that has raged for hundreds of years! {numref}`great-red-spot` below shows an image of Jupiter captured by the Hubble Space Telescope on June 27, 2019.\n\n```gkwoiqvj https://solarsystem.nasa.gov/system/resources/detail_files/626_PIA21775.jpg\n---\nheight: 300px\nname: great-red-spot\n---\nJupiter's Great Red Spot! Source: [NASA](https://solarsystem.nasa.gov/resources/626/jupiters-great-red-spot-in-true-color/?category=planets_jupiter).\n```\n\nJupiter's GRS has been observed to be shrinking for about the last century and a half! [Here](https://github.com/UBC-DSCI/jupyterdays/tree/master/jupyterdays/sessions/beuzen/data) is some data of the length of the GRS spanning the last ~150 years which we can use to investigate this phenomenon.\n\n¡Ahora intente construir su libro (jupyter-book build .) para asegurarse de que todo se vea bien! Usando la barra de contenido del lado izquierdo, navegue a la nueva página “The Great Red Spot”, que debería verse así:\n\n¡Ok genial! Ahora importemos los datos a los que hicimos referencia para que podamos crear algunos gráficos.\nCree una nueva celda de código debajo de la celda de rebaja actual y agregue el siguiente código para leer en nuestro conjunto de datos de GRS como un marco de datos de Pandas.\nimport pandas as pd\npd.options.plotting.backend = \"plotly\"\n\nurl = \"https://raw.githubusercontent.com/UBC-DSCI/jupyterdays/master/jupyterdays/sessions/beuzen/data/GRS_data.csv\"\ndf = pd.read_csv(url)\ndf['Year'] = df['Year'].astype(int) \ndf.head()\n\nNota: Estamos imprimiendo la salida en la pantalla con el uso de df.head() y esto se mostrará en nuestro Jupyter Book renderizado.\n\nSi reconstruye su libro (jupyter-book build .) en este punto, verá algo como lo siguiente:\n\nAhora, podemos usar estos datos para crear algunos gráficos.\nLas tramas en su Jupyter Book pueden ser estáticas (por ejemplo, matplotlib, seaborn) o interactivas (por ejemplo, altair, plotly, bokeh). Para este tutorial, crearemos algunos gráficos de ejemplo usando Plotly (a través del backend de Pandas).\nPrimero creemos un diagrama de dispersión simple de nuestros datos. Cree una nueva celda de código en su cuaderno y agregue el siguiente código:\nimport plotly.io as pio\npio.renderers.default = \"notebook\"\nfig = df.plot.scatter(x=\"Year\", y=\"GRS Length\", color=\"Recorder\",\n                      range_x=[1870, 2030], range_y=[10, 40],\n                      width=650, height=400)\nfig.update_layout(title={'text': \"Great Red Spot Size\", 'x':0.5, 'y':0.92})\nfig.update_traces(marker=dict(size=7))\nYa que estamos en eso, creemos también una trama animada. Cree otra celda de código nueva y agregue el siguiente código:\nfig = df.plot.scatter(x=\"Year\", y=\"GRS Length\",\n                      animation_frame=\"Year\",\n                      range_x=[1870, 2030], range_y=[10, 40],\n                      width=600, height=520)\nfig.update_layout(title={'text': \"Great Red Spot Size Animation\", 'x':0.5, 'y':0.94})\nfig.layout.updatemenus[0].buttons[0].args[1][\"frame\"][\"duration\"] = 200\nfig.update_traces(marker=dict(size=10))\n\nNota: Plotly tiene diferentes renderizadores disponibles para generar gráficos. Es posible que deba experimentar con renderizadores para obtener el resultado que desea en su Jupyter Book. He descubierto que pio.renderers.default = \"notebook\" funciona con la versión actual de Jupyter Book.\n\n¡Ahora, reconstruyamos nuestro libro y echemos un vistazo!\n\nEs posible que desee ocultar parte del código en su libro, ¡no hay problema! Eso también se hace fácilmente con Jupyter Book.\nEl que nos interesa aquí es ocultar la entrada de código. Podemos hacerlo fácilmente agregando la etiqueta hide-input a la celda que deseamos ocultar. Hay varias formas de agregar etiquetas a la celda en Jupyter Notebooks. En Jupyter Lab, haga clic en el icono de engranaje en la barra lateral izquierda y luego agregue la etiqueta deseada como se muestra a continuación:\n\nContinúe y agregue las etiquetas hide-input a ambas celdas de trazado en su archivo great_red_spot.ipynb. Cuando reconstruyas el libro, verás que la entrada del código está oculta (pero se puede alternar con el ícono +):\n\n\nNota: También puede almacenar el contenido de la libreta como valores, gráficos o marcos de datos en variables que se pueden utilizar en toda su libreta mediante la herramienta glue.\n\n\n\n\n\n\nJupyter-Book - Documentation\nTutorial - Jupyter Book"
  },
  {
    "objectID": "posts/2021/2021-08-11-jb.html#introducción",
    "href": "posts/2021/2021-08-11-jb.html#introducción",
    "title": "Jupyter Book",
    "section": "",
    "text": "Jupyter Book es un proyecto de código abierto para crear libros y documentos mediante Jupyter Notebooks y/o Markdown.\nAlgunas características importantes del uso de Jupyter Book:\n\ncontenido con calidad de publicación que incluya figuras, símbolos matemáticos, citas y referencias cruzadas!\nescribir contenido como Jupyter Notebooks, markdown o reStructuredText\nAgregue interactividad a su libro, por ejemplo, alternar la visibilidad de las celdas, conectarse con un servicio en línea como Binder e incluir resultados interactivos (por ejemplo, figuras y widgets).\ngenerar una variedad de resultados, incluidos sitios web (HTML, CSS, JS), markdown y PDF.\nuna interfaz de línea de comandos para crear libros rápidamente, por ejemplo, jupyter-book build mybook\n\nEn esta sesión, se muestra un ejemplo de cómo crear un Jupyter Book desde cero y algunas de las características clave que ofrece Jupyter Book.\n\nNota: Puede encontrar los códigos de este ejemplo en el siguiente repositorio. Por otro lado, puede revisar el siguente link para ver la compilación con GitLab CI/CD."
  },
  {
    "objectID": "posts/2021/2021-08-11-jb.html#primeros-pasos",
    "href": "posts/2021/2021-08-11-jb.html#primeros-pasos",
    "title": "Jupyter Book",
    "section": "",
    "text": "Para instalar Jupyter Book, necesitará usar la línea de comando. Si ha instalado Anaconda, puede usar:\nconda install -c conda-forge jupyter-book\nDe lo contrario, puede instalar con pip:\npip install jupyter-book\n\n\n\nJupyter Book viene con una herramienta que le permite crear y construir libros rápidamente. Para crear el esqueleto del libro, escriba lo siguiente en la línea de comando:\njupyter-book create jupiter\n\nNota: Aquí llamamos al libro jupiter, pero puedes elegir llamar a tu libro como quieras.\n\nAhora tendrás un nuevo directorio llamado jupiter (o como quieras llamar a tu libro), con el siguiente contenido:\njupiter\n  ├── _config.yml\n  ├── _toc.yml\n  ├── content.md\n  ├── intro.md\n  ├── markdown.md\n  ├── notebooks.ipynb\n  ├── references.bib\n  └── requirements.txt\nen donde: * _config.yml: archivo que contiene las configuraciones del proyecto. * _toc.yml: archivo que ordena los capítulos del libro. * content.md: archivo genérico .md. * intro.md: archivo genérico .md. * markdown.md: archivo genérico .md. * notebooks.ipynb: archivo genérico .ipynb. * references.bib: archivo para añadir las referencias. * requirements.txt: archivo que contiene las dependencias python) del proyecto.\n\n\n\nJupyter Book admite varios tipos de archivos:\n\nMarkdown (.md)\nnotebooks (.ipynb)\netc.\n\nComo Markdown y Jupyter Notebooks probablemente serán los tipos de archivo más comunes que usará, se mostrará un ejemplo de ello.\nLo primero será eliminar los archivos de inicio en el directorio:\n\ncontent.md\nintro.md\nmarkdown.md\nnotebooks.ipynb\n\nAsí que ejecutamos por línea de comando:\nrm content.md intro.md markdown.md notebooks.ipynb\nPor otro lado, nuestro proyecto estaré conformado por tres archivos:\n\nindex.md\nIntroduction.md\ngreat_red_spot.ipynb\n\nLuego, debemos indicar cómo serán mostrados estos documentos en el archivo _toc.yml. La estructura será la siguiente:\nformat: jb-book\nroot: index\nchapters:\n- file: Introduction\n- file: great_red_spot\nEn este caso, root: index corresponde al primer archivo que se visualiza en el jupyter-book. Dentro del archivo index.md escribiremos:\n# Home\n\njupyter book example\n\n## Contenidos\n\n\n```roknnpsbfrmktjfhd\n```\n\n\n\n\n\n\nSe comienza por agregar un archivo de markdown. Con algún editor a elección (por ejemplo, jupyter notebook o jupyterlab) se crea un nuevo archivo markdown llamado Introduction.md.\nSe usa este archivo como demostración de algunos de los principales tipos de contenido que puede agregar en Jupyter-Book.\n\n\nSe agrega un texto de Markdown simple a nuestro archivo. Si no está familiarizado con la sintaxis de markdown, consulte Markdown Cheat Sheet. Puede copiar y pegar el siguiente contenido directamente en su archivo Introduction.md.\n# Jupiter Book\n\nThis book contains information about the planet **Jupiter** - the fifth planet from the sun and the largest planet in the solar system!\n\n\n\nPuedes incluir figuras en tu Jupyter Book usando la siguiente sintaxis:\n```gkwoiqvj my_image.png\n---\nheight: 150px\nname: my-image\n---\nHere is my image's caption!\n```\n\nSi bien la imagen puede estar contenida y referenciada desde el directorio raíz, también se puede incluir imágenes a través de URL. Incluyamos una imagen del planeta Júpiter en nuestro archivo Introduction.md usando lo siguiente:\n```gkwoiqvj https://solarsystem.nasa.gov/system/resources/detail_files/2486_stsci-h-p1936a_1800.jpg\n---\nheight: 300px\nname: jupiter-figure\n---\nThe beautiful planet Jupiter!\n```\n\nLa razón por la que le damos un “nombre” a nuestra imagen es para que podamos hacer referencia a ella fácilmente con la sintaxis:\n{numref}`jupiter-figure`\nSe agregará una oración que incluya esta referencia. El archivo completo ahora debería verse así:\n# Jupiter Book\n\nThis book contains information about the planet Jupiter - the fifth planet from the sun and the largest planet in the solar system! {numref}`jupiter-figure` below shows an image of Jupiter captured by the Hubble Space Telescope on June 27, 2019.\n\n```gkwoiqvj https://solarsystem.nasa.gov/system/resources/detail_files/2486_stsci-h-p1936a_1800.jpg\n---\nheight: 300px\nname: jupiter-figure\n---\nThe beautiful planet Jupiter! Source: [NASA](https://solarsystem.nasa.gov/resources/2486/hubbles-new-portrait-of-jupiter/?category=planets_jupiter).\n```\n\nEn este punto, probablemente se debería crear nuetro libro para asegurarnos de que tenga el aspecto esperado. Para hacer eso, primero necesitamos modificar nuestro archivo _toc.yml. Este archivo contiene la tabla de contenido de nuestro libro. Abra ese archivo ahora y elimine todo lo que hay allí. Luego, simplemente agregue lo siguiente:\n- file: introduction\nAhora podemos construir nuestro libro desde la línea de comandos asegurándonos de que estamos en el directorio raíz de nuestro libro y luego usando:\njupyter-book build .\nUna vez finalizada la compilación, tendrá un nuevo subdirectorio llamado_build/html/ en la raíz de su libro, navegue hasta esa ubicación y abra _build/html/index.html. Debería verse algo como esto:\n\n\n\n\nJupyter Book usa MathJax para componer matemáticas, lo que le permite agregar matemáticas de estilo LaTeX a su libro. Puede agregar matemáticas en línea, bloques matemáticos y ecuaciones numeradas a su libro Jupyter. Sigamos adelante y creemos un nuevo encabezado en nuestro archivo Introduction.md que incluye algunas matemáticas.\nLas matemáticas en línea se pueden definir usando $ de la siguiente manera:\nJupiter has a mass of:  $m_{j} \\approx 1.9 \\times 10^{27} kg$\n\nJupiter has a mass of: \\(m_{j} \\approx 1.9 \\times 10^{27} kg\\)\nLos bloques matemáticos se pueden definir usando la notación $$:\n$$m_{j} \\approx 1.9 \\times 10^{27} kg$$\n\n\\[m_{j} \\approx 1.9 \\times 10^{27} kg\\]\n\nNota: Si lo prefiere, los bloques matemáticos también se pueden definir con \\begin{equation} en lugar de $$.\n\nLas ecuaciones numeradas se pueden definir así (este es el estilo que te recomiendo que uses con más frecuencia):\n```lbgrcp\n:label: my_label\nm_{j} \\approx 1.9 \\times 10^{27} kg\n```\n\n\nAgreguemos más contenido a nuestro libro. Copie y agregue el siguiente texto a su archivo Introduction.md:\n## The Mass of Jupiter\n\nWe can estimate the mass of Jupiter from the period and size of an object orbiting it. For example, we can use Jupiter's moon Callisto to estimate it's mass.\n\nCallisto's period: $p_{c}=16.7 days$\n\nCallisto's orbit radius: $r_{c}=1,900,000 km$\n\nNow, using [Kepler's Law](https://solarsystem.nasa.gov/resources/310/orbits-and-keplers-laws/) we can work out the mass of Jupiter.\n\n\n```lbgrcp\n:label: eq1\nm_{j} \\approx \\frac{r_{c}}{p_{c}} \\times 7.9 \\times 10^{10}\n```\n\n```lbgrcp\n:label: eq2\nm_{j} \\approx 1.9 \\times 10^{27} kg\n```\n\n\nA continuación, puede reconstruir su libro (jupyter-book build .) y abrir _build/html/index.html para asegurarse de que todo se esté procesando como se esperaba.\n\n\n\n\nHay varias formas diferentes de controlar el diseño de las páginas de su Jupyter Book. El cambio de diseño que utilizo con más frecuencia es agregar contenido a un margen en la página. Puede agregar un margen usando la siguiente directiva:\n```txbrpgfy An optional title\nSome margin content.\n```\n\nAgreguemos algo de contenido marginal al libro:\n```txbrpgfy Did you know?\nJupiter is 11.0x larger than Earth!\n```\n\n\n\n\nHay todo tipo de advertencias diferentes que puede usar en Jupyter Book que se enumeran aquí en la documentación de Jupyter Book. Las advertencias se crean con la sintaxis:\n```uzejlz\nI am a useful note!\n```\n\n\nNo dude en agregar la siguiente advertencia a Introduction.md:\n```lgcccw\nNASA provides a lot more information about the physical characteristics of Jupiter [here](https://solarsystem.nasa.gov/planets/jupiter/by-the-numbers/).\n```\n\n\n\n\n\nEl último contenido corresponde a referencias y una bibliografía. Puede agregar citas de cualquier trabajo almacenado en el archivo Bibtex Reference.bib que se encuentra en el directorio raíz de su libro.\nPara incluir una cita en su libro, agregue una entrada bibtex a references.bib, por ejemplo:\n@article{mayor1995jupiter,\n    title={A Jupiter-mass companion to a solar-type star},\n    author={Mayor, Michel and Queloz, Didier},\n    journal={Nature},\n    volume={378},\n    number={6555},\n    pages={355--359},\n    year={1995},\n    publisher={Nature Publishing Group}\n}\n\n@article{guillot1999interiors,\n    title={Interiors of giant planets inside and outside the solar system},\n    author={Guillot, Tristan},\n    journal={Science},\n    volume={286},\n    number={5437},\n    pages={72--77},\n    year={1999},\n    publisher={American Association for the Advancement of Science}\n}\n\nNota: Consulte la documentación de BibTex para obtener información sobre el estilo de referencia de BibTex. Google Scholar facilita la exportación de un formato de cita bibtex.\n\nA continuación, puede hacer referencia al trabajo en su libro utilizando la siguiente directiva:\n{cite}`mayor1995jupiter`\nO para múltiples citas:\n{cite}`mayor1995jupiter,guillot1999interiors`\nLuego puede crear una bibliografía a partir de reference.bib usando:\n```vrvfdaunxkbvhs references.bib\n```\n\nPor ejemplo, intente agregar esto a su archivo Introduction.md:\nThere might even be more planets out there with a similar mass to Jupiter {cite}`mayor1995jupiter,guillot1999interiors`!\n\n## Bibliography\n\n```vrvfdaunxkbvhs references.bib\n```\n\nSu archivo final Introduction.md debería verse así:\n# Jupiter Book\n\nThis book contains information about the planet Jupiter - the fifth planet from the sun and the largest planet in the solar system! {numref}`jupiter-figure` below shows an image of Jupiter captured by the Hubble Space Telescope on June 27, 2019.\n\n```gkwoiqvj https://solarsystem.nasa.gov/system/resources/detail_files/2486_stsci-h-p1936a_1800.jpg\n---\nheight: 300px\nname: jupiter-figure\n---\nThe beautiful planet Jupiter! Source: [NASA](https://solarsystem.nasa.gov/resources/2486/hubbles-new-portrait-of-jupiter/?category=planets_jupiter).\n```\n\n## The Mass of Jupiter\n\nWe can estimate the mass of Jupiter from the period and size of an object orbiting it. For example, we can use Jupiter's moon Callisto to estimate it's mass.\n\nCallisto's period: $p_{c}=16.7 days$\n\nCallisto's orbit radius: $r_{c}=1,900,000 km$\n\nNow, using [Kepler's Law](https://solarsystem.nasa.gov/resources/310/orbits-and-keplers-laws/) we can work out the mass of Jupiter.\n\n\n```lbgrcp\n:label: eq1\nm_{j} \\approx \\frac{r_{c}}{p_{c}} \\times 7.9 \\times 10^{10}\n```\n\n```lbgrcp\n:label: eq2\nm_{j} \\approx 1.9 \\times 10^{27} kg\n```\n\n\n```txbrpgfy Did you know?\nJupiter is 11.0x larger than Earth!\n```\n\n\n```lgcccw\nNASA provides a lot more information about the physical characteristics of Jupiter [here](https://solarsystem.nasa.gov/planets/jupiter/by-the-numbers/).\n```\n\n\nThere might even be more planets out there with a similar mass to Jupiter {cite}`mayor1995jupiter,guillot1999interiors`!\n\n## Bibliography\n\n```vrvfdaunxkbvhs references.bib\n```\n\nY debería renderizarse así: \n\n\n\nTodos los flujos de trabajo de formato y estilo que vimos en markdown también se aplican a un Jupyter Notebook; simplemente agréguelos a una celda de markdown y listo.\nComencemos con lo siguiente:\n\nCree un nuevo notebook llamado great_red_spot.ipynb;\nAgregue este archivo a su _toc.yml;\nAgregue una celda de markdown con el siguiente contenido:\n\n# The Great Red Spot\n\nJupiter’s iconic Great Red Spot (GRS) is actually an enormous storm that is bigger than Earth that has raged for hundreds of years! {numref}`great-red-spot` below shows an image of Jupiter captured by the Hubble Space Telescope on June 27, 2019.\n\n```gkwoiqvj https://solarsystem.nasa.gov/system/resources/detail_files/626_PIA21775.jpg\n---\nheight: 300px\nname: great-red-spot\n---\nJupiter's Great Red Spot! Source: [NASA](https://solarsystem.nasa.gov/resources/626/jupiters-great-red-spot-in-true-color/?category=planets_jupiter).\n```\n\nJupiter's GRS has been observed to be shrinking for about the last century and a half! [Here](https://github.com/UBC-DSCI/jupyterdays/tree/master/jupyterdays/sessions/beuzen/data) is some data of the length of the GRS spanning the last ~150 years which we can use to investigate this phenomenon.\n\n¡Ahora intente construir su libro (jupyter-book build .) para asegurarse de que todo se vea bien! Usando la barra de contenido del lado izquierdo, navegue a la nueva página “The Great Red Spot”, que debería verse así:\n\n¡Ok genial! Ahora importemos los datos a los que hicimos referencia para que podamos crear algunos gráficos.\nCree una nueva celda de código debajo de la celda de rebaja actual y agregue el siguiente código para leer en nuestro conjunto de datos de GRS como un marco de datos de Pandas.\nimport pandas as pd\npd.options.plotting.backend = \"plotly\"\n\nurl = \"https://raw.githubusercontent.com/UBC-DSCI/jupyterdays/master/jupyterdays/sessions/beuzen/data/GRS_data.csv\"\ndf = pd.read_csv(url)\ndf['Year'] = df['Year'].astype(int) \ndf.head()\n\nNota: Estamos imprimiendo la salida en la pantalla con el uso de df.head() y esto se mostrará en nuestro Jupyter Book renderizado.\n\nSi reconstruye su libro (jupyter-book build .) en este punto, verá algo como lo siguiente:\n\nAhora, podemos usar estos datos para crear algunos gráficos.\nLas tramas en su Jupyter Book pueden ser estáticas (por ejemplo, matplotlib, seaborn) o interactivas (por ejemplo, altair, plotly, bokeh). Para este tutorial, crearemos algunos gráficos de ejemplo usando Plotly (a través del backend de Pandas).\nPrimero creemos un diagrama de dispersión simple de nuestros datos. Cree una nueva celda de código en su cuaderno y agregue el siguiente código:\nimport plotly.io as pio\npio.renderers.default = \"notebook\"\nfig = df.plot.scatter(x=\"Year\", y=\"GRS Length\", color=\"Recorder\",\n                      range_x=[1870, 2030], range_y=[10, 40],\n                      width=650, height=400)\nfig.update_layout(title={'text': \"Great Red Spot Size\", 'x':0.5, 'y':0.92})\nfig.update_traces(marker=dict(size=7))\nYa que estamos en eso, creemos también una trama animada. Cree otra celda de código nueva y agregue el siguiente código:\nfig = df.plot.scatter(x=\"Year\", y=\"GRS Length\",\n                      animation_frame=\"Year\",\n                      range_x=[1870, 2030], range_y=[10, 40],\n                      width=600, height=520)\nfig.update_layout(title={'text': \"Great Red Spot Size Animation\", 'x':0.5, 'y':0.94})\nfig.layout.updatemenus[0].buttons[0].args[1][\"frame\"][\"duration\"] = 200\nfig.update_traces(marker=dict(size=10))\n\nNota: Plotly tiene diferentes renderizadores disponibles para generar gráficos. Es posible que deba experimentar con renderizadores para obtener el resultado que desea en su Jupyter Book. He descubierto que pio.renderers.default = \"notebook\" funciona con la versión actual de Jupyter Book.\n\n¡Ahora, reconstruyamos nuestro libro y echemos un vistazo!\n\nEs posible que desee ocultar parte del código en su libro, ¡no hay problema! Eso también se hace fácilmente con Jupyter Book.\nEl que nos interesa aquí es ocultar la entrada de código. Podemos hacerlo fácilmente agregando la etiqueta hide-input a la celda que deseamos ocultar. Hay varias formas de agregar etiquetas a la celda en Jupyter Notebooks. En Jupyter Lab, haga clic en el icono de engranaje en la barra lateral izquierda y luego agregue la etiqueta deseada como se muestra a continuación:\n\nContinúe y agregue las etiquetas hide-input a ambas celdas de trazado en su archivo great_red_spot.ipynb. Cuando reconstruyas el libro, verás que la entrada del código está oculta (pero se puede alternar con el ícono +):\n\n\nNota: También puede almacenar el contenido de la libreta como valores, gráficos o marcos de datos en variables que se pueden utilizar en toda su libreta mediante la herramienta glue."
  },
  {
    "objectID": "posts/2021/2021-08-11-jb.html#referencias",
    "href": "posts/2021/2021-08-11-jb.html#referencias",
    "title": "Jupyter Book",
    "section": "",
    "text": "Jupyter-Book - Documentation\nTutorial - Jupyter Book"
  },
  {
    "objectID": "posts/2021/2021-08-05-rise.html",
    "href": "posts/2021/2021-08-05-rise.html",
    "title": "RISE",
    "section": "",
    "text": "RISE es una extensión a los jupyter notebooks que permite transformar tus notebooks en presentaciones interactivas.\nToda las celdas pueden editarse y ejecutarse directamente, durante la presentación. Esto es práctico si necesitas corregir un error en una celda de texto. Más importante aún, puedes ejecutar código directamente en el kernel. En una misma diapositiva puedes tener múltiples celdas y elegir cuál ejecutar, o corregir el texto y volver a ejecutar.\n\nAlgunas características importantes del uso de RISE:\n\nSimplifica la generación de material.\nSe mantiene un archivo y no varios archivos para hablar de lo mismo.\nEs fácil de corregir, no se necesita mucho esfuerzo (similar a una PPT).\n\nEn esta sesión, se muestra un ejemplo de cómo crear una presentación con RISE.\n\nNota: Puede encontrar los códigos de este ejemplo en el siguiente repositorio. Por otro lado, puede revisar el siguente link para ver la compilación con GitLab CI/CD.\n\n\n\n\n\n\nPara instalar RISE, necesitará usar la línea de comando. Si ha instalado Anaconda, puede usar:\nconda install -c conda-forge rise\nDe lo contrario, puede instalar con pip:\npip install RISE\n\nNota: No interactuarás directamente con RISE. En su lugar, podrá acceder a él a través de Jupyter Notebooks.\n\n\n\n\n\nPara crear una presentación, deberá iniciar Jupyter Notebooks y abrir un nuevo notebook (tenga en cuenta que debe hacer esto después de haber instalado RISE). Una vez que tenga un Jupyter Notebook nuevo, deberá habilitar la presentación de diapositivas. Puede hacer esto haciendo lo siguiente:\n\nHaga clic en “Ver” en la barra de herramientas de Jupyter\nColoca el cursor sobre “Barra de herramientas de celda” en el menú “Ver”\nHaga clic en “Presentación de diapositivas” en el menú “Barra de herramientas de celda”\n\n\n\n\nEn este punto, debería tener una barra de herramientas de celda con un menú desplegable en el lado derecho: \nDebería ver seis opciones aquí. Este menú desplegable y sus opciones determinan cómo encaja cada celda en la presentación. Las opciones y sus descripciones se encuentran a continuación:\n\nslide: indica que la celda seleccionada debe ser el comienzo de una nueva diapositiva.\nsub-slide -: indica que la celda seleccionada debe ser el comienzo de una nueva sub-diapositiva, que aparece en un nuevo marco debajo de la diapositiva anterior.\nfragment: indica que la celda seleccionada debe aparecer como una compilación de la diapositiva anterior.\nskip: indica que la celda seleccionada debe omitirse y no ser parte de la presentación de diapositivas.\nnotes: indica que la celda seleccionada debe ser solo notas del presentador.\n- -: indica que la celda seleccionada debe seguir el comportamiento de la celda anterior, lo cual es útil cuando una celda de rebaja y una celda de código deben aparecer simultáneamente.\n\nCada una de estas opciones puede incluir código Python o código Markdown/HTML/LaTeX como un Jupyter Notebook tradicional.\n\n\n\nUna vez que se han utilizado las celdas para crear material para la presentación, la presentación se puede ver directamente desde el notebook.\nHay dos opciones para ver la presentación de diapositivas:\n\nUsar el acceso directo OPTION + R shortcut (ALT + R on Windows) para ingresar y salir del modo de presentación desde dentro de la computadora portátil\nAl hacer clic en el botón “Modo de presentación” de la computadora portátil, esto solo aparecerá si ha instalado RISE.\n\n\nDespués de ingresar al modo de presentación, debería ver una pantalla similar a esta:\n\n\n\n\nSi bien puede ser tentador usar las teclas &lt;- y -&gt; para cambiar las diapositivas en la presentación, esto no funcionará por completo: omitirá las celdas marcadas como sub-slides. En su lugar, se debe usar ESPACIO para mover la presentación de diapositivas hacia adelante y MAYÚS + ESPACIO para mover la presentación de diapositivas hacia atrás.\nHay muchos otros atajos de teclado a los que se puede acceder dentro de la presentación haciendo clic en el signo de interrogación (?) en la esquina inferior izquierda.\n\n\n\nUna de las mejores cosas de RISE es que funciona en una sesión de Python en vivo, lo que significa que puede editar y ejecutar código mientras se ejecuta la presentación.\n\n\n\n\nPuedes exportar tu presentación desplegando la opción: File -&gt; Download as.\n\nNota: Para poder descargar en formato .pdf, necesita tener instalado pandoc.\n\n\n\n\n\n\nRISE - Documentation\nCreating Interactive Slideshows in Jupyter Notebooks"
  },
  {
    "objectID": "posts/2021/2021-08-05-rise.html#introducción",
    "href": "posts/2021/2021-08-05-rise.html#introducción",
    "title": "RISE",
    "section": "",
    "text": "RISE es una extensión a los jupyter notebooks que permite transformar tus notebooks en presentaciones interactivas.\nToda las celdas pueden editarse y ejecutarse directamente, durante la presentación. Esto es práctico si necesitas corregir un error en una celda de texto. Más importante aún, puedes ejecutar código directamente en el kernel. En una misma diapositiva puedes tener múltiples celdas y elegir cuál ejecutar, o corregir el texto y volver a ejecutar.\n\nAlgunas características importantes del uso de RISE:\n\nSimplifica la generación de material.\nSe mantiene un archivo y no varios archivos para hablar de lo mismo.\nEs fácil de corregir, no se necesita mucho esfuerzo (similar a una PPT).\n\nEn esta sesión, se muestra un ejemplo de cómo crear una presentación con RISE.\n\nNota: Puede encontrar los códigos de este ejemplo en el siguiente repositorio. Por otro lado, puede revisar el siguente link para ver la compilación con GitLab CI/CD."
  },
  {
    "objectID": "posts/2021/2021-08-05-rise.html#primeros-pasos",
    "href": "posts/2021/2021-08-05-rise.html#primeros-pasos",
    "title": "RISE",
    "section": "",
    "text": "Para instalar RISE, necesitará usar la línea de comando. Si ha instalado Anaconda, puede usar:\nconda install -c conda-forge rise\nDe lo contrario, puede instalar con pip:\npip install RISE\n\nNota: No interactuarás directamente con RISE. En su lugar, podrá acceder a él a través de Jupyter Notebooks."
  },
  {
    "objectID": "posts/2021/2021-08-05-rise.html#habilitación-del-modo-de-presentación",
    "href": "posts/2021/2021-08-05-rise.html#habilitación-del-modo-de-presentación",
    "title": "RISE",
    "section": "",
    "text": "Para crear una presentación, deberá iniciar Jupyter Notebooks y abrir un nuevo notebook (tenga en cuenta que debe hacer esto después de haber instalado RISE). Una vez que tenga un Jupyter Notebook nuevo, deberá habilitar la presentación de diapositivas. Puede hacer esto haciendo lo siguiente:\n\nHaga clic en “Ver” en la barra de herramientas de Jupyter\nColoca el cursor sobre “Barra de herramientas de celda” en el menú “Ver”\nHaga clic en “Presentación de diapositivas” en el menú “Barra de herramientas de celda”\n\n\n\n\nEn este punto, debería tener una barra de herramientas de celda con un menú desplegable en el lado derecho: \nDebería ver seis opciones aquí. Este menú desplegable y sus opciones determinan cómo encaja cada celda en la presentación. Las opciones y sus descripciones se encuentran a continuación:\n\nslide: indica que la celda seleccionada debe ser el comienzo de una nueva diapositiva.\nsub-slide -: indica que la celda seleccionada debe ser el comienzo de una nueva sub-diapositiva, que aparece en un nuevo marco debajo de la diapositiva anterior.\nfragment: indica que la celda seleccionada debe aparecer como una compilación de la diapositiva anterior.\nskip: indica que la celda seleccionada debe omitirse y no ser parte de la presentación de diapositivas.\nnotes: indica que la celda seleccionada debe ser solo notas del presentador.\n- -: indica que la celda seleccionada debe seguir el comportamiento de la celda anterior, lo cual es útil cuando una celda de rebaja y una celda de código deben aparecer simultáneamente.\n\nCada una de estas opciones puede incluir código Python o código Markdown/HTML/LaTeX como un Jupyter Notebook tradicional.\n\n\n\nUna vez que se han utilizado las celdas para crear material para la presentación, la presentación se puede ver directamente desde el notebook.\nHay dos opciones para ver la presentación de diapositivas:\n\nUsar el acceso directo OPTION + R shortcut (ALT + R on Windows) para ingresar y salir del modo de presentación desde dentro de la computadora portátil\nAl hacer clic en el botón “Modo de presentación” de la computadora portátil, esto solo aparecerá si ha instalado RISE.\n\n\nDespués de ingresar al modo de presentación, debería ver una pantalla similar a esta:\n\n\n\n\nSi bien puede ser tentador usar las teclas &lt;- y -&gt; para cambiar las diapositivas en la presentación, esto no funcionará por completo: omitirá las celdas marcadas como sub-slides. En su lugar, se debe usar ESPACIO para mover la presentación de diapositivas hacia adelante y MAYÚS + ESPACIO para mover la presentación de diapositivas hacia atrás.\nHay muchos otros atajos de teclado a los que se puede acceder dentro de la presentación haciendo clic en el signo de interrogación (?) en la esquina inferior izquierda.\n\n\n\nUna de las mejores cosas de RISE es que funciona en una sesión de Python en vivo, lo que significa que puede editar y ejecutar código mientras se ejecuta la presentación.\n\n\n\n\nPuedes exportar tu presentación desplegando la opción: File -&gt; Download as.\n\nNota: Para poder descargar en formato .pdf, necesita tener instalado pandoc."
  },
  {
    "objectID": "posts/2021/2021-08-05-rise.html#referencias",
    "href": "posts/2021/2021-08-05-rise.html#referencias",
    "title": "RISE",
    "section": "",
    "text": "RISE - Documentation\nCreating Interactive Slideshows in Jupyter Notebooks"
  },
  {
    "objectID": "posts/2021/2021-08-20-fastpages.html",
    "href": "posts/2021/2021-08-20-fastpages.html",
    "title": "Fastpages",
    "section": "",
    "text": "Fastpages es una plataforma que te permite crear y alojar un blog de forma gratuita, sin anuncios y con muchas funciones útiles, como:\n\nCree publicaciones que contengan código, salidas de código (que pueden ser interactivas), texto formateado, etc. directamente desde Jupyter Notebooks. Las publicaciones en notebooks admiten funciones como:\n\nLas visualizaciones interactivas realizadas con Altair siguen siendo interactivas.\nOcultar o mostrar la entrada y salida de la celda.\nCeldas de código contraíbles que están abiertas o cerradas de forma predeterminada.\nDefina el título, el resumen y otros metadatos a través de celdas de rebajas especiales\nPosibilidad de agregar enlaces a Colab y GitHub automáticamente.\n\nCree publicaciones, incluido el formato y las imágenes, directamente desde documentos de Microsoft Word.\nCree y edite publicaciones de Markdown completamente en línea usando el editor de Markdown incorporado de GitHub.\nInserta tarjetas de Twitter y videos de YouTube.\nCategorización de publicaciones de blog por etiquetas proporcionadas por el usuario para mayor visibilidad.\n\nEn esta sección se enseñará los pasos básicos para poder crear su propio blog con fastpages.\n\n\n\n\n\n¡El proceso de configuración de páginas rápidas también está automatizado con GitHub Actions! Al crear un repositorio a partir de la plantilla de páginas rápidas, se abrirá automáticamente una solicitud de extracción (después de ~ 30 segundos) configurando su blog para que pueda comenzar a funcionar. La solicitud de extracción automatizada lo recibirá con instrucciones como esta: \n¡Todo lo que tienes que hacer es seguir estas instrucciones (en el PR que recibes) y tu nuevo sitio de blogs estará en funcionamiento!\n\nNote: Si tienes dudas con la instalación, te recomiendo ver el siguiente video.\n\n\n\n\n\nEl repositorio de fastpages esta compuesto de la siguiente forma:\n├── .github\n├── _action_files\n├── _fastpages_docs\n├── images\n├── _includes\n├── _layouts\n├── _notebooks\n├── _pages\n├── _plugins\n├── _posts\n├── _sass\n└── _word\n└── .devcontainer.json\n└── Makefile\n└── index.html\n└── Gemfile\n└── _config.yml\n└── LICENSE\n└── docker-compose.yml\n└── Gemfile.lock\n└── README.md\n└── .gitattributes\n└── .gitignore\nDe momento nos vamos a centrar en algunos de estos archivos:\n\n_config.yml: es el archivo que funciona como el motor del proyecto. En ente archivo, podemos poner el nombre a nuestro blog, el logo, información personal (github, linkedin, etc), entre otras cosas.\nindex.html: Corresponde a la primera página cuando se despliega nuestro blog. por lo que es importante escribir algún mensaje para especificar la motivación de hacer un blog.\n/_notebooks: Lugar donde se deben guardar los notebooks (.ipynb) con la convención de nomenclatura **YYYY-MM-DD-*.ipynb**.\n/_posts: Lugar donde se deben guardar los archivos markdown (.md) con la convención de nomenclatura **YYYY-MM-DD-*.md**.\n/_word: Lugar donde se deben guardar los archivos word (.docx) con la convención de nomenclatura **YYYY-MM-DD-*.docx**.\n\n\nNote: fastpages usa nbdev para impulsar el proceso de conversión de Jupyter Notebooks en publicaciones de blog. Cuando guardas un notebook en la carpeta /_notebooks de tu repositorio, GitHub Actions aplica nbdev a esos notebooks automáticamente. El mismo proceso ocurre cuando guarda documentos de Word o markdown en el directorio _word o_posts, respectivamente.\n\n\n\n\nEn esta parte, se muestran características especiales que fastpages proporciona para los notebooks. También puede escribir las publicaciones de su blog con documentos de Word o makdown.\n\n\nLa primera celda de su Jupyter Notebook o markdown contiene información preliminar. El tema principal son los metadatos que pueden activar/desactivar opciones en su Notebook. Tiene el formato siguiente:\n# Title\n&gt; Awesome summary\n\n- toc: true\n- branch: master\n- badges: true\n- comments: true\n- author: Hamel Husain & Jeremy Howard\n- categories: [fastpages, jupyter]\nTodas las configuraciones anteriores están habilitadas en esta publicación, ¡para que pueda ver cómo se ven!\n\nEl campo de resumen (precedido por &gt;) se mostrará debajo de su título y también lo utilizarán las redes sociales para mostrar la descripción de su página.\ntoc: establecer esto en true generará automáticamente una tabla de contenido\nbadges: establecer esto en true mostrará los enlaces de Google Colab y GitHub en la publicación de su blog.\ncomments: establecer esto en true habilitará los comentarios. Consulte estas instrucciones para obtener más detalles.\nautor: esto mostrará los nombres de los autores.\ncategories: permitirá que su publicación sea categorizada en una página de “Etiquetas”, donde los lectores pueden navegar por su publicación por categorías.\n\nMarkdown Front Matters tiene un formato similar al de los notebooks. Las diferencias entre los dos se puede ver en el siguiente link.\n\n\n\ncoloque una marca # collapse-hide en la parte superior de cualquier celda si desea ocultar esa celda de forma predeterminada, pero déle al lector la opción de mostrarla:\n\n#hide\n!pip install pandas altair\n\nRequirement already satisfied: pandas in /home/fralfaro/.cache/pypoetry/virtualenvs/rise-example-Z8sxMVFk-py3.8/lib/python3.8/site-packages (1.3.1)\nRequirement already satisfied: altair in /home/fralfaro/.cache/pypoetry/virtualenvs/rise-example-Z8sxMVFk-py3.8/lib/python3.8/site-packages (4.1.0)\nRequirement already satisfied: pytz&gt;=2017.3 in /home/fralfaro/.cache/pypoetry/virtualenvs/rise-example-Z8sxMVFk-py3.8/lib/python3.8/site-packages (from pandas) (2021.1)\nRequirement already satisfied: numpy&gt;=1.17.3 in /home/fralfaro/.cache/pypoetry/virtualenvs/rise-example-Z8sxMVFk-py3.8/lib/python3.8/site-packages (from pandas) (1.21.1)\nRequirement already satisfied: python-dateutil&gt;=2.7.3 in /home/fralfaro/.cache/pypoetry/virtualenvs/rise-example-Z8sxMVFk-py3.8/lib/python3.8/site-packages (from pandas) (2.8.2)\nRequirement already satisfied: six&gt;=1.5 in /home/fralfaro/.cache/pypoetry/virtualenvs/rise-example-Z8sxMVFk-py3.8/lib/python3.8/site-packages (from python-dateutil&gt;=2.7.3-&gt;pandas) (1.16.0)\nRequirement already satisfied: jinja2 in /home/fralfaro/.cache/pypoetry/virtualenvs/rise-example-Z8sxMVFk-py3.8/lib/python3.8/site-packages (from altair) (3.0.1)\nRequirement already satisfied: toolz in /home/fralfaro/.cache/pypoetry/virtualenvs/rise-example-Z8sxMVFk-py3.8/lib/python3.8/site-packages (from altair) (0.11.1)\nRequirement already satisfied: entrypoints in /home/fralfaro/.cache/pypoetry/virtualenvs/rise-example-Z8sxMVFk-py3.8/lib/python3.8/site-packages (from altair) (0.3)\nRequirement already satisfied: jsonschema in /home/fralfaro/.cache/pypoetry/virtualenvs/rise-example-Z8sxMVFk-py3.8/lib/python3.8/site-packages (from altair) (3.2.0)\nRequirement already satisfied: MarkupSafe&gt;=2.0 in /home/fralfaro/.cache/pypoetry/virtualenvs/rise-example-Z8sxMVFk-py3.8/lib/python3.8/site-packages (from jinja2-&gt;altair) (2.0.1)\nRequirement already satisfied: attrs&gt;=17.4.0 in /home/fralfaro/.cache/pypoetry/virtualenvs/rise-example-Z8sxMVFk-py3.8/lib/python3.8/site-packages (from jsonschema-&gt;altair) (21.2.0)\nRequirement already satisfied: setuptools in /home/fralfaro/.cache/pypoetry/virtualenvs/rise-example-Z8sxMVFk-py3.8/lib/python3.8/site-packages (from jsonschema-&gt;altair) (57.1.0)\nRequirement already satisfied: pyrsistent&gt;=0.14.0 in /home/fralfaro/.cache/pypoetry/virtualenvs/rise-example-Z8sxMVFk-py3.8/lib/python3.8/site-packages (from jsonschema-&gt;altair) (0.18.0)\nWARNING: You are using pip version 21.1.3; however, version 21.2.2 is available.\nYou should consider upgrading via the '/home/fralfaro/.cache/pypoetry/virtualenvs/rise-example-Z8sxMVFk-py3.8/bin/python -m pip install --upgrade pip' command.\n\n\n\n#collapse-hide\nimport pandas as pd\nimport altair as alt\n\ncoloque una marca # collapse-show en la parte superior de cualquier celda si desea mostrar esa celda de forma predeterminada, pero déle al lector la opción de ocultarla:\n\n#collapse-show\ncars = 'https://vega.github.io/vega-datasets/data/cars.json'\nmovies = 'https://vega.github.io/vega-datasets/data/movies.json'\nsp500 = 'https://vega.github.io/vega-datasets/data/sp500.csv'\nstocks = 'https://vega.github.io/vega-datasets/data/stocks.csv'\nflights = 'https://vega.github.io/vega-datasets/data/flights-5k.json'\n\nSi desea ocultar las celdas por completo (no solo contraerlas), lea estas instrucciones.\n\n# hide\ndf = pd.read_json(movies) # load movies data\ndf.columns = [x.replace(' ', '_') for x in df.columns.values]\ngenres = df['Major_Genre'].unique() # get unique field values\ngenres = list(filter(lambda d: d is not None, genres)) # filter out None values\ngenres.sort() # sort alphabetically\n\n\n\n\nPuede mostrar tablas de la forma habitual en su blog:\n\n# display table with pandas\ndf[['Title', 'Worldwide_Gross', \n    'Production_Budget', 'IMDB_Rating']].head()\n\n\n\n\n\n\n\n\n\nTitle\nWorldwide_Gross\nProduction_Budget\nIMDB_Rating\n\n\n\n\n0\nThe Land Girls\n146083.0\n8000000.0\n6.1\n\n\n1\nFirst Love, Last Rites\n10876.0\n300000.0\n6.9\n\n\n2\nI Married a Strange Person\n203134.0\n250000.0\n6.8\n\n\n3\nLet's Talk About Sex\n373615.0\n300000.0\nNaN\n\n\n4\nSlam\n1087521.0\n1000000.0\n3.4\n\n\n\n\n\n\n\n\n\n\n\n¡Las visualizaciones interactivas realizadas con Altair siguen siendo interactivas!\nDejamos esta celda de abajo sin ocultar para que pueda disfrutar de una vista previa del resaltado de sintaxis en páginas rápidas, que utiliza el tema de drácula.\n\n#collapse-hide\n# select a point for which to provide details-on-demand\nlabel = alt.selection_single(\n    encodings=['x'], # limit selection to x-axis value\n    on='mouseover',  # select on mouseover events\n    nearest=True,    # select data point nearest the cursor\n    empty='none'     # empty selection includes no data points\n)\n\n# define our base line chart of stock prices\nbase = alt.Chart().mark_line().encode(\n    alt.X('date:T'),\n    alt.Y('price:Q', scale=alt.Scale(type='log')),\n    alt.Color('symbol:N')\n)\n\nalt.layer(\n    base, # base line chart\n    \n    # add a rule mark to serve as a guide line\n    alt.Chart().mark_rule(color='#aaa').encode(\n        x='date:T'\n    ).transform_filter(label),\n    \n    # add circle marks for selected time points, hide unselected points\n    base.mark_circle().encode(\n        opacity=alt.condition(label, alt.value(1), alt.value(0))\n    ).add_selection(label),\n\n    # add white stroked text to provide a legible background for labels\n    base.mark_text(align='left', dx=5, dy=-5, stroke='white', strokeWidth=2).encode(\n        text='price:Q'\n    ).transform_filter(label),\n\n    # add text labels for stock prices\n    base.mark_text(align='left', dx=5, dy=-5).encode(\n        text='price:Q'\n    ).transform_filter(label),\n    \n    data=stocks\n).properties(\n    width=500,\n    height=400\n)\n\n\n\n\n\n\n\n\n\n\n\nAl escribir Le doy a esta publicación dos :+1:! Se mostrará esto:\nLe doy a esta publicación dos :+1:!\n\n\nPuede incluir imágenes de rebajas con leyenda (caption) como este:\n![](https://www.fast.ai/images/fastai_paper/show_batch.png \"Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/\")\n\nPor supuesto, la leyenda es opcional.\n\n\n\nSi escribe &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 mostrará esto:\n\ntwitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20\n\n\n\n\nSi escribe &gt; youtube: https://youtu.be/XfoYk_Z5AkI mostrará esto:\n\nyoutube: https://youtu.be/XfoYk_Z5AkI\n\n\n\n\nPuedes ejecutar algunas cajas con mensajes de nota, información o warning!. A continuación se muestran algunos ejemplos:\n&gt; Warning: There will be no second warning! &gt; Warning: There will be no second warning!\n&gt; Important: Pay attention! It's important.\n\nImportant: Pay attention! It’s important.\n\n&gt; Tip: This is my tip.\n\nTip: This is my tip.\n\n&gt; Note: Take note of this.\n\nNote: Take note of this.\n\n\n\n\n\n\nRepositorio: fastpages\nnbdev: notebooks a posts\nFastai: foro"
  },
  {
    "objectID": "posts/2021/2021-08-20-fastpages.html#introducción",
    "href": "posts/2021/2021-08-20-fastpages.html#introducción",
    "title": "Fastpages",
    "section": "",
    "text": "Fastpages es una plataforma que te permite crear y alojar un blog de forma gratuita, sin anuncios y con muchas funciones útiles, como:\n\nCree publicaciones que contengan código, salidas de código (que pueden ser interactivas), texto formateado, etc. directamente desde Jupyter Notebooks. Las publicaciones en notebooks admiten funciones como:\n\nLas visualizaciones interactivas realizadas con Altair siguen siendo interactivas.\nOcultar o mostrar la entrada y salida de la celda.\nCeldas de código contraíbles que están abiertas o cerradas de forma predeterminada.\nDefina el título, el resumen y otros metadatos a través de celdas de rebajas especiales\nPosibilidad de agregar enlaces a Colab y GitHub automáticamente.\n\nCree publicaciones, incluido el formato y las imágenes, directamente desde documentos de Microsoft Word.\nCree y edite publicaciones de Markdown completamente en línea usando el editor de Markdown incorporado de GitHub.\nInserta tarjetas de Twitter y videos de YouTube.\nCategorización de publicaciones de blog por etiquetas proporcionadas por el usuario para mayor visibilidad.\n\nEn esta sección se enseñará los pasos básicos para poder crear su propio blog con fastpages."
  },
  {
    "objectID": "posts/2021/2021-08-20-fastpages.html#primeros-pasos",
    "href": "posts/2021/2021-08-20-fastpages.html#primeros-pasos",
    "title": "Fastpages",
    "section": "",
    "text": "¡El proceso de configuración de páginas rápidas también está automatizado con GitHub Actions! Al crear un repositorio a partir de la plantilla de páginas rápidas, se abrirá automáticamente una solicitud de extracción (después de ~ 30 segundos) configurando su blog para que pueda comenzar a funcionar. La solicitud de extracción automatizada lo recibirá con instrucciones como esta: \n¡Todo lo que tienes que hacer es seguir estas instrucciones (en el PR que recibes) y tu nuevo sitio de blogs estará en funcionamiento!\n\nNote: Si tienes dudas con la instalación, te recomiendo ver el siguiente video."
  },
  {
    "objectID": "posts/2021/2021-08-20-fastpages.html#estructura-de-fastpages",
    "href": "posts/2021/2021-08-20-fastpages.html#estructura-de-fastpages",
    "title": "Fastpages",
    "section": "",
    "text": "El repositorio de fastpages esta compuesto de la siguiente forma:\n├── .github\n├── _action_files\n├── _fastpages_docs\n├── images\n├── _includes\n├── _layouts\n├── _notebooks\n├── _pages\n├── _plugins\n├── _posts\n├── _sass\n└── _word\n└── .devcontainer.json\n└── Makefile\n└── index.html\n└── Gemfile\n└── _config.yml\n└── LICENSE\n└── docker-compose.yml\n└── Gemfile.lock\n└── README.md\n└── .gitattributes\n└── .gitignore\nDe momento nos vamos a centrar en algunos de estos archivos:\n\n_config.yml: es el archivo que funciona como el motor del proyecto. En ente archivo, podemos poner el nombre a nuestro blog, el logo, información personal (github, linkedin, etc), entre otras cosas.\nindex.html: Corresponde a la primera página cuando se despliega nuestro blog. por lo que es importante escribir algún mensaje para especificar la motivación de hacer un blog.\n/_notebooks: Lugar donde se deben guardar los notebooks (.ipynb) con la convención de nomenclatura **YYYY-MM-DD-*.ipynb**.\n/_posts: Lugar donde se deben guardar los archivos markdown (.md) con la convención de nomenclatura **YYYY-MM-DD-*.md**.\n/_word: Lugar donde se deben guardar los archivos word (.docx) con la convención de nomenclatura **YYYY-MM-DD-*.docx**.\n\n\nNote: fastpages usa nbdev para impulsar el proceso de conversión de Jupyter Notebooks en publicaciones de blog. Cuando guardas un notebook en la carpeta /_notebooks de tu repositorio, GitHub Actions aplica nbdev a esos notebooks automáticamente. El mismo proceso ocurre cuando guarda documentos de Word o markdown en el directorio _word o_posts, respectivamente."
  },
  {
    "objectID": "posts/2021/2021-08-20-fastpages.html#jupyter-notebooks-y-fastpages",
    "href": "posts/2021/2021-08-20-fastpages.html#jupyter-notebooks-y-fastpages",
    "title": "Fastpages",
    "section": "",
    "text": "En esta parte, se muestran características especiales que fastpages proporciona para los notebooks. También puede escribir las publicaciones de su blog con documentos de Word o makdown.\n\n\nLa primera celda de su Jupyter Notebook o markdown contiene información preliminar. El tema principal son los metadatos que pueden activar/desactivar opciones en su Notebook. Tiene el formato siguiente:\n# Title\n&gt; Awesome summary\n\n- toc: true\n- branch: master\n- badges: true\n- comments: true\n- author: Hamel Husain & Jeremy Howard\n- categories: [fastpages, jupyter]\nTodas las configuraciones anteriores están habilitadas en esta publicación, ¡para que pueda ver cómo se ven!\n\nEl campo de resumen (precedido por &gt;) se mostrará debajo de su título y también lo utilizarán las redes sociales para mostrar la descripción de su página.\ntoc: establecer esto en true generará automáticamente una tabla de contenido\nbadges: establecer esto en true mostrará los enlaces de Google Colab y GitHub en la publicación de su blog.\ncomments: establecer esto en true habilitará los comentarios. Consulte estas instrucciones para obtener más detalles.\nautor: esto mostrará los nombres de los autores.\ncategories: permitirá que su publicación sea categorizada en una página de “Etiquetas”, donde los lectores pueden navegar por su publicación por categorías.\n\nMarkdown Front Matters tiene un formato similar al de los notebooks. Las diferencias entre los dos se puede ver en el siguiente link.\n\n\n\ncoloque una marca # collapse-hide en la parte superior de cualquier celda si desea ocultar esa celda de forma predeterminada, pero déle al lector la opción de mostrarla:\n\n#hide\n!pip install pandas altair\n\nRequirement already satisfied: pandas in /home/fralfaro/.cache/pypoetry/virtualenvs/rise-example-Z8sxMVFk-py3.8/lib/python3.8/site-packages (1.3.1)\nRequirement already satisfied: altair in /home/fralfaro/.cache/pypoetry/virtualenvs/rise-example-Z8sxMVFk-py3.8/lib/python3.8/site-packages (4.1.0)\nRequirement already satisfied: pytz&gt;=2017.3 in /home/fralfaro/.cache/pypoetry/virtualenvs/rise-example-Z8sxMVFk-py3.8/lib/python3.8/site-packages (from pandas) (2021.1)\nRequirement already satisfied: numpy&gt;=1.17.3 in /home/fralfaro/.cache/pypoetry/virtualenvs/rise-example-Z8sxMVFk-py3.8/lib/python3.8/site-packages (from pandas) (1.21.1)\nRequirement already satisfied: python-dateutil&gt;=2.7.3 in /home/fralfaro/.cache/pypoetry/virtualenvs/rise-example-Z8sxMVFk-py3.8/lib/python3.8/site-packages (from pandas) (2.8.2)\nRequirement already satisfied: six&gt;=1.5 in /home/fralfaro/.cache/pypoetry/virtualenvs/rise-example-Z8sxMVFk-py3.8/lib/python3.8/site-packages (from python-dateutil&gt;=2.7.3-&gt;pandas) (1.16.0)\nRequirement already satisfied: jinja2 in /home/fralfaro/.cache/pypoetry/virtualenvs/rise-example-Z8sxMVFk-py3.8/lib/python3.8/site-packages (from altair) (3.0.1)\nRequirement already satisfied: toolz in /home/fralfaro/.cache/pypoetry/virtualenvs/rise-example-Z8sxMVFk-py3.8/lib/python3.8/site-packages (from altair) (0.11.1)\nRequirement already satisfied: entrypoints in /home/fralfaro/.cache/pypoetry/virtualenvs/rise-example-Z8sxMVFk-py3.8/lib/python3.8/site-packages (from altair) (0.3)\nRequirement already satisfied: jsonschema in /home/fralfaro/.cache/pypoetry/virtualenvs/rise-example-Z8sxMVFk-py3.8/lib/python3.8/site-packages (from altair) (3.2.0)\nRequirement already satisfied: MarkupSafe&gt;=2.0 in /home/fralfaro/.cache/pypoetry/virtualenvs/rise-example-Z8sxMVFk-py3.8/lib/python3.8/site-packages (from jinja2-&gt;altair) (2.0.1)\nRequirement already satisfied: attrs&gt;=17.4.0 in /home/fralfaro/.cache/pypoetry/virtualenvs/rise-example-Z8sxMVFk-py3.8/lib/python3.8/site-packages (from jsonschema-&gt;altair) (21.2.0)\nRequirement already satisfied: setuptools in /home/fralfaro/.cache/pypoetry/virtualenvs/rise-example-Z8sxMVFk-py3.8/lib/python3.8/site-packages (from jsonschema-&gt;altair) (57.1.0)\nRequirement already satisfied: pyrsistent&gt;=0.14.0 in /home/fralfaro/.cache/pypoetry/virtualenvs/rise-example-Z8sxMVFk-py3.8/lib/python3.8/site-packages (from jsonschema-&gt;altair) (0.18.0)\nWARNING: You are using pip version 21.1.3; however, version 21.2.2 is available.\nYou should consider upgrading via the '/home/fralfaro/.cache/pypoetry/virtualenvs/rise-example-Z8sxMVFk-py3.8/bin/python -m pip install --upgrade pip' command.\n\n\n\n#collapse-hide\nimport pandas as pd\nimport altair as alt\n\ncoloque una marca # collapse-show en la parte superior de cualquier celda si desea mostrar esa celda de forma predeterminada, pero déle al lector la opción de ocultarla:\n\n#collapse-show\ncars = 'https://vega.github.io/vega-datasets/data/cars.json'\nmovies = 'https://vega.github.io/vega-datasets/data/movies.json'\nsp500 = 'https://vega.github.io/vega-datasets/data/sp500.csv'\nstocks = 'https://vega.github.io/vega-datasets/data/stocks.csv'\nflights = 'https://vega.github.io/vega-datasets/data/flights-5k.json'\n\nSi desea ocultar las celdas por completo (no solo contraerlas), lea estas instrucciones.\n\n# hide\ndf = pd.read_json(movies) # load movies data\ndf.columns = [x.replace(' ', '_') for x in df.columns.values]\ngenres = df['Major_Genre'].unique() # get unique field values\ngenres = list(filter(lambda d: d is not None, genres)) # filter out None values\ngenres.sort() # sort alphabetically\n\n\n\n\nPuede mostrar tablas de la forma habitual en su blog:\n\n# display table with pandas\ndf[['Title', 'Worldwide_Gross', \n    'Production_Budget', 'IMDB_Rating']].head()\n\n\n\n\n\n\n\n\n\nTitle\nWorldwide_Gross\nProduction_Budget\nIMDB_Rating\n\n\n\n\n0\nThe Land Girls\n146083.0\n8000000.0\n6.1\n\n\n1\nFirst Love, Last Rites\n10876.0\n300000.0\n6.9\n\n\n2\nI Married a Strange Person\n203134.0\n250000.0\n6.8\n\n\n3\nLet's Talk About Sex\n373615.0\n300000.0\nNaN\n\n\n4\nSlam\n1087521.0\n1000000.0\n3.4\n\n\n\n\n\n\n\n\n\n\n\n¡Las visualizaciones interactivas realizadas con Altair siguen siendo interactivas!\nDejamos esta celda de abajo sin ocultar para que pueda disfrutar de una vista previa del resaltado de sintaxis en páginas rápidas, que utiliza el tema de drácula.\n\n#collapse-hide\n# select a point for which to provide details-on-demand\nlabel = alt.selection_single(\n    encodings=['x'], # limit selection to x-axis value\n    on='mouseover',  # select on mouseover events\n    nearest=True,    # select data point nearest the cursor\n    empty='none'     # empty selection includes no data points\n)\n\n# define our base line chart of stock prices\nbase = alt.Chart().mark_line().encode(\n    alt.X('date:T'),\n    alt.Y('price:Q', scale=alt.Scale(type='log')),\n    alt.Color('symbol:N')\n)\n\nalt.layer(\n    base, # base line chart\n    \n    # add a rule mark to serve as a guide line\n    alt.Chart().mark_rule(color='#aaa').encode(\n        x='date:T'\n    ).transform_filter(label),\n    \n    # add circle marks for selected time points, hide unselected points\n    base.mark_circle().encode(\n        opacity=alt.condition(label, alt.value(1), alt.value(0))\n    ).add_selection(label),\n\n    # add white stroked text to provide a legible background for labels\n    base.mark_text(align='left', dx=5, dy=-5, stroke='white', strokeWidth=2).encode(\n        text='price:Q'\n    ).transform_filter(label),\n\n    # add text labels for stock prices\n    base.mark_text(align='left', dx=5, dy=-5).encode(\n        text='price:Q'\n    ).transform_filter(label),\n    \n    data=stocks\n).properties(\n    width=500,\n    height=400\n)"
  },
  {
    "objectID": "posts/2021/2021-08-20-fastpages.html#otras-características",
    "href": "posts/2021/2021-08-20-fastpages.html#otras-características",
    "title": "Fastpages",
    "section": "",
    "text": "Al escribir Le doy a esta publicación dos :+1:! Se mostrará esto:\nLe doy a esta publicación dos :+1:!\n\n\nPuede incluir imágenes de rebajas con leyenda (caption) como este:\n![](https://www.fast.ai/images/fastai_paper/show_batch.png \"Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/\")\n\nPor supuesto, la leyenda es opcional.\n\n\n\nSi escribe &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 mostrará esto:\n\ntwitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20\n\n\n\n\nSi escribe &gt; youtube: https://youtu.be/XfoYk_Z5AkI mostrará esto:\n\nyoutube: https://youtu.be/XfoYk_Z5AkI\n\n\n\n\nPuedes ejecutar algunas cajas con mensajes de nota, información o warning!. A continuación se muestran algunos ejemplos:\n&gt; Warning: There will be no second warning! &gt; Warning: There will be no second warning!\n&gt; Important: Pay attention! It's important.\n\nImportant: Pay attention! It’s important.\n\n&gt; Tip: This is my tip.\n\nTip: This is my tip.\n\n&gt; Note: Take note of this.\n\nNote: Take note of this."
  },
  {
    "objectID": "posts/2021/2021-08-20-fastpages.html#referencias",
    "href": "posts/2021/2021-08-20-fastpages.html#referencias",
    "title": "Fastpages",
    "section": "",
    "text": "Repositorio: fastpages\nnbdev: notebooks a posts\nFastai: foro"
  },
  {
    "objectID": "posts/2022/2021-07-15-tdd.html",
    "href": "posts/2022/2021-07-15-tdd.html",
    "title": "Test Driven Development",
    "section": "",
    "text": "Esta sección busca dar señales de cómo abordar el desarrollo de software para Data Science usando Test Driven Development, una técnica ampliamente usada en otros rubros de la programación.\n\n\nEn palabras simples, el desarrollo guiado por pruebas pone las pruebas en el corazón de nuestro trabajo. En su forma más simple consiste en un proceso iterativo de 3 fases:\n\n\nRed: Escribe un test que ponga a prueba una nueva funcionalidad y asegurate de que el test falla\nGreen: Escribe el código mínimo necesario para pasar ese test\nRefactor: Refactoriza de ser necesario\n\n\n\n\nA modo de ejemplo, vamos a testear la función paridad, que determina si un número natural es par o no.\nLo primero que se debe hacer es crear el test, para ello se ocupará la librería pytest.\n\nNota: No es necesario conocer previamente la librería pytest para entender el ejemplo.\n\n@pytest.mark.parametrize(\n    \"number, expected\",\n    [\n        (2, 'par'),\n])\ndef test_paridad(number, expected):\n    assert paridad(number) == expected\nEl test nos dice que si el input es el número 2, la función paridad devuelve el output 'par'. Cómo aún no hemos escrito la función, el test fallará (fase red).\n========= test session starts ============================================ \nplatform linux -- Python 3.8.10, pytest-6.2.4, py-1.10.0, pluggy-0.13.1\nrootdir: /home/fralfaro/PycharmProjects/ds_blog\nplugins: anyio-3.3.0\ncollected 1 item                                                                                                                                                                          \n\ntemp/test_funcion.py F                                              [100%]\n========= 1 failed in 0.14s  ===============================================\nAhora, se escribe la función paridad (fase green):\ndef paridad(n:int)-&gt;str:\n    \"\"\"\n    Determina si un numero natural es par o no.\n    \n    :param n: numero entero\n    :return: 'par' si es el numero es par; 'impar' en otro caso\n    \"\"\"\n    return 'par' if n%2==0 else 'impar'\nVolvemos a correr el test:\n========= test session starts ============================================ \nplatform linux -- Python 3.8.10, pytest-6.2.4, py-1.10.0, pluggy-0.13.1\nrootdir: /home/fralfaro/PycharmProjects/ds_blog\nplugins: anyio-3.3.0\ncollected 1 item                                                                                                                                                                          \n\ntemp/test_funcion.py .                                              [100%]\n========= 1 passed in 0.06s  ===============================================\nHemos cometido un descuido a proposito, no hemos testeado el caso si el número fuese impar, por lo cual reescribimos el test (fase refactor)\n@pytest.mark.parametrize(\n    \"number, expected\",\n    [\n        (2, 'par'),\n        (3, 'impar'),\n])\ndef test_paridad(number, expected):\n    assert paridad(number) == expected\ny corremos nuevamente los test:\n========= test session starts ============================================ \nplatform linux -- Python 3.8.10, pytest-6.2.4, py-1.10.0, pluggy-0.13.1\nrootdir: /home/fralfaro/PycharmProjects/ds_blog\nplugins: anyio-3.3.0\ncollected 2 items                                                                                                                                                                          \n\ntemp/test_funcion.py ..                                              [100%]\n========= 2 passed in 0.06s  ===============================================\nListo, nuestra función paridad ha sido testeado correctamente!.\n\n\n\n\nExisten varias razones por las que uno debería usar TDD. Entre ellas podemos encontrar: - Formular bien nuestros pensamientos mediante la escritura de un test significativo antes de ponernos a solucionar el problema nos ayuda a clarificar los límites del problema y cómo podemos resolverlo. Con el tiempo esto ayuda a obtener un diseño modular y reusable del código. - Escribir tests ayuda la forma en que escribimos código, haciéndolo más legible a otros. Sin embargo, no es un acto de altruismo, la mayoría de las veces ese otro es tu futuro yo. - Verifica que el código funciona de la manera que se espera, y lo hace de forma automática. - Te permite realizar refactoring con la certeza de que no has roto nada. - Los tests escritos sirven como documentación para otros desarrolladores. - Es una práctica requerida en metodologías de desarrollo de software agile.\n\n\n\nEl 2008, Nagappan, Maximilien, Bhat y Williams publicaron el paper llamado Realizing Quality Improvement Through Test Driven Development - Results and Experiences of Four Industrial Teams, en donde estudiaron 4 equipos de trabajo (3 de Microsoft y 1 de IBM), con proyectos que variaban entre las 6000 lineas de código hasta las 155k. Estas son parte de sus conclusiones:\n\nTodos los equipos demostraron una baja considerable en la densidad de defectos: 40% para el equipo de IBM, y entre 60-90% para los equipos de Microsoft.\n\nComo todo en la vida, nada es gratis:\n\nIncremento del tiempo de desarrollo varía entre un 15% a 35%.\n\nSin embargo\n\nDesde un punto de vista de eficacia este incremento en tiempo de desarrollo se compensa por los costos de mantención reducidos debido al incremento en calidad.\n\nAdemás, es importante escribir tests junto con la implementación en pequeñas iteraciones. George y Williams encontraron que escribir tests después de que la aplicación está mas o menos lista hace que se testee menos porque los desarrolladores piensan en menos casos, y además la aplicación se vuelve menos testeable. Otra conclusión interesante del estudio de George y Williams es que un 79% de los desarrolladores experimentaron que el uso de TDD conlleva a un diseño más simple.\n\n\n\nNo, pero puedes usarlo casi siempre. El análisis exploratorio es un caso en que el uso de TDD no hace sentido. Una vez que tenemos definido el problema a solucionar y un mejor entendimiento del problema podemos aterrizar nuestras ideas a la implementación vía testing.\n\n\n\nAcá listamos algunas librerías de TDD en Python: - unittest: Módulo dentro de la librería estándar de Python. Permite realizar tests unitarios, de integración y end to end. - doctest: Permite realizar test de la documentación del código (ejemplos: Numpy o Pandas). - pytest: Librería de testing ampliamente usada en proyectos nuevos de Python. - nose: Librería que extiende unittest para hacerlo más simple. - coverage: Herramienta para medir la cobertura de código de los proyectos. - tox: Herramienta para facilitar el test de una librería en diferentes versiones e intérpretes de Python. - hypothesis: Librería para escribir tests vía reglas que ayuda a encontrar casos borde. - behave: Permite utilizar Behavior Driven Development, un proceso de desarrollo derivado del TDD.\n\n\n\n\nRealizing Quality Improvement Through Test Driven Development - Results and Experiences of Four Industrial Teams, es una buena lectura, sobretodo los consejos que dan en las conclusiones.\nGoogle Testing Blog: Poseen varios artículos sobre cómo abordar problemas tipo, buenas prácticas de diseño para generar código testeable, entre otros. En particular destaca la serie Testing on the Toilet.\nCualquier artículo de Martin Fowler sobre testing, empezando por éste\nDesign Patterns: Los patrones de diseño de software tienen en consideración que el código sea testeable."
  },
  {
    "objectID": "posts/2022/2021-07-15-tdd.html#introducción",
    "href": "posts/2022/2021-07-15-tdd.html#introducción",
    "title": "Test Driven Development",
    "section": "",
    "text": "Esta sección busca dar señales de cómo abordar el desarrollo de software para Data Science usando Test Driven Development, una técnica ampliamente usada en otros rubros de la programación.\n\n\nEn palabras simples, el desarrollo guiado por pruebas pone las pruebas en el corazón de nuestro trabajo. En su forma más simple consiste en un proceso iterativo de 3 fases:\n\n\nRed: Escribe un test que ponga a prueba una nueva funcionalidad y asegurate de que el test falla\nGreen: Escribe el código mínimo necesario para pasar ese test\nRefactor: Refactoriza de ser necesario\n\n\n\n\nA modo de ejemplo, vamos a testear la función paridad, que determina si un número natural es par o no.\nLo primero que se debe hacer es crear el test, para ello se ocupará la librería pytest.\n\nNota: No es necesario conocer previamente la librería pytest para entender el ejemplo.\n\n@pytest.mark.parametrize(\n    \"number, expected\",\n    [\n        (2, 'par'),\n])\ndef test_paridad(number, expected):\n    assert paridad(number) == expected\nEl test nos dice que si el input es el número 2, la función paridad devuelve el output 'par'. Cómo aún no hemos escrito la función, el test fallará (fase red).\n========= test session starts ============================================ \nplatform linux -- Python 3.8.10, pytest-6.2.4, py-1.10.0, pluggy-0.13.1\nrootdir: /home/fralfaro/PycharmProjects/ds_blog\nplugins: anyio-3.3.0\ncollected 1 item                                                                                                                                                                          \n\ntemp/test_funcion.py F                                              [100%]\n========= 1 failed in 0.14s  ===============================================\nAhora, se escribe la función paridad (fase green):\ndef paridad(n:int)-&gt;str:\n    \"\"\"\n    Determina si un numero natural es par o no.\n    \n    :param n: numero entero\n    :return: 'par' si es el numero es par; 'impar' en otro caso\n    \"\"\"\n    return 'par' if n%2==0 else 'impar'\nVolvemos a correr el test:\n========= test session starts ============================================ \nplatform linux -- Python 3.8.10, pytest-6.2.4, py-1.10.0, pluggy-0.13.1\nrootdir: /home/fralfaro/PycharmProjects/ds_blog\nplugins: anyio-3.3.0\ncollected 1 item                                                                                                                                                                          \n\ntemp/test_funcion.py .                                              [100%]\n========= 1 passed in 0.06s  ===============================================\nHemos cometido un descuido a proposito, no hemos testeado el caso si el número fuese impar, por lo cual reescribimos el test (fase refactor)\n@pytest.mark.parametrize(\n    \"number, expected\",\n    [\n        (2, 'par'),\n        (3, 'impar'),\n])\ndef test_paridad(number, expected):\n    assert paridad(number) == expected\ny corremos nuevamente los test:\n========= test session starts ============================================ \nplatform linux -- Python 3.8.10, pytest-6.2.4, py-1.10.0, pluggy-0.13.1\nrootdir: /home/fralfaro/PycharmProjects/ds_blog\nplugins: anyio-3.3.0\ncollected 2 items                                                                                                                                                                          \n\ntemp/test_funcion.py ..                                              [100%]\n========= 2 passed in 0.06s  ===============================================\nListo, nuestra función paridad ha sido testeado correctamente!."
  },
  {
    "objectID": "posts/2022/2021-07-15-tdd.html#porqué-debería-usarlo",
    "href": "posts/2022/2021-07-15-tdd.html#porqué-debería-usarlo",
    "title": "Test Driven Development",
    "section": "",
    "text": "Existen varias razones por las que uno debería usar TDD. Entre ellas podemos encontrar: - Formular bien nuestros pensamientos mediante la escritura de un test significativo antes de ponernos a solucionar el problema nos ayuda a clarificar los límites del problema y cómo podemos resolverlo. Con el tiempo esto ayuda a obtener un diseño modular y reusable del código. - Escribir tests ayuda la forma en que escribimos código, haciéndolo más legible a otros. Sin embargo, no es un acto de altruismo, la mayoría de las veces ese otro es tu futuro yo. - Verifica que el código funciona de la manera que se espera, y lo hace de forma automática. - Te permite realizar refactoring con la certeza de que no has roto nada. - Los tests escritos sirven como documentación para otros desarrolladores. - Es una práctica requerida en metodologías de desarrollo de software agile."
  },
  {
    "objectID": "posts/2022/2021-07-15-tdd.html#evidencia-empírica",
    "href": "posts/2022/2021-07-15-tdd.html#evidencia-empírica",
    "title": "Test Driven Development",
    "section": "",
    "text": "El 2008, Nagappan, Maximilien, Bhat y Williams publicaron el paper llamado Realizing Quality Improvement Through Test Driven Development - Results and Experiences of Four Industrial Teams, en donde estudiaron 4 equipos de trabajo (3 de Microsoft y 1 de IBM), con proyectos que variaban entre las 6000 lineas de código hasta las 155k. Estas son parte de sus conclusiones:\n\nTodos los equipos demostraron una baja considerable en la densidad de defectos: 40% para el equipo de IBM, y entre 60-90% para los equipos de Microsoft.\n\nComo todo en la vida, nada es gratis:\n\nIncremento del tiempo de desarrollo varía entre un 15% a 35%.\n\nSin embargo\n\nDesde un punto de vista de eficacia este incremento en tiempo de desarrollo se compensa por los costos de mantención reducidos debido al incremento en calidad.\n\nAdemás, es importante escribir tests junto con la implementación en pequeñas iteraciones. George y Williams encontraron que escribir tests después de que la aplicación está mas o menos lista hace que se testee menos porque los desarrolladores piensan en menos casos, y además la aplicación se vuelve menos testeable. Otra conclusión interesante del estudio de George y Williams es que un 79% de los desarrolladores experimentaron que el uso de TDD conlleva a un diseño más simple."
  },
  {
    "objectID": "posts/2022/2021-07-15-tdd.html#puedo-usar-tdd-siempre",
    "href": "posts/2022/2021-07-15-tdd.html#puedo-usar-tdd-siempre",
    "title": "Test Driven Development",
    "section": "",
    "text": "No, pero puedes usarlo casi siempre. El análisis exploratorio es un caso en que el uso de TDD no hace sentido. Una vez que tenemos definido el problema a solucionar y un mejor entendimiento del problema podemos aterrizar nuestras ideas a la implementación vía testing."
  },
  {
    "objectID": "posts/2022/2021-07-15-tdd.html#librerías-disponibles",
    "href": "posts/2022/2021-07-15-tdd.html#librerías-disponibles",
    "title": "Test Driven Development",
    "section": "",
    "text": "Acá listamos algunas librerías de TDD en Python: - unittest: Módulo dentro de la librería estándar de Python. Permite realizar tests unitarios, de integración y end to end. - doctest: Permite realizar test de la documentación del código (ejemplos: Numpy o Pandas). - pytest: Librería de testing ampliamente usada en proyectos nuevos de Python. - nose: Librería que extiende unittest para hacerlo más simple. - coverage: Herramienta para medir la cobertura de código de los proyectos. - tox: Herramienta para facilitar el test de una librería en diferentes versiones e intérpretes de Python. - hypothesis: Librería para escribir tests vía reglas que ayuda a encontrar casos borde. - behave: Permite utilizar Behavior Driven Development, un proceso de desarrollo derivado del TDD."
  },
  {
    "objectID": "posts/2022/2021-07-15-tdd.html#referencias",
    "href": "posts/2022/2021-07-15-tdd.html#referencias",
    "title": "Test Driven Development",
    "section": "",
    "text": "Realizing Quality Improvement Through Test Driven Development - Results and Experiences of Four Industrial Teams, es una buena lectura, sobretodo los consejos que dan en las conclusiones.\nGoogle Testing Blog: Poseen varios artículos sobre cómo abordar problemas tipo, buenas prácticas de diseño para generar código testeable, entre otros. En particular destaca la serie Testing on the Toilet.\nCualquier artículo de Martin Fowler sobre testing, empezando por éste\nDesign Patterns: Los patrones de diseño de software tienen en consideración que el código sea testeable."
  },
  {
    "objectID": "posts/2022/2022-10-12-implicit.html",
    "href": "posts/2022/2022-10-12-implicit.html",
    "title": "Collaborative Filtering",
    "section": "",
    "text": "Recommendation System: User-Based Collaborative Filtering o Filtrado Colaborativo Basado en el Usuario es un tipo de algoritmo de sistema de recomendación que utiliza la similitud del usuario para hacer recomendaciones de productos.\nEn este tutorial, hablaremos sobre * ¿Qué es el filtrado colaborativo basado en usuarios (usuario-usuario)? * ¿Cómo crear una matriz usuario-producto? * ¿Cómo procesar los datos para el filtrado colaborativo basado en el usuario? * ¿Cómo identificar usuarios similares? * ¿Cómo reducir el grupo de elementos? * ¿Cómo clasificar los artículos para la recomendación? * ¿Cómo predecir la puntuación de calificación?\n\nNota: Para entender a profundidad este algoritmo, se recomienda leer el documento oficial.\n\n\n\n\nEn primer lugar, comprendamos cómo funciona el filtrado colaborativo basado en usuarios.\nEl filtrado colaborativo basado en el usuario hace recomendaciones basadas en las interacciones del usuario con el producto en el pasado. La suposición detrás del algoritmo es que a usuarios similares les gustan productos similares.\nEl algoritmo de filtrado colaborativo basado en el usuario generalmente tiene los siguientes pasos:\n\nEncuentre usuarios similares en función de las interacciones con elementos comunes.\nIdentifique los elementos con una calificación alta por parte de usuarios similares pero que no han sido expuestos al usuario activo de interés.\nCalcular la puntuación media ponderada de cada elemento.\nClasifique los elementos según la puntuación y elija los n mejores elementos para recomendar.\n\n\n\n\nimage.png\n\n\nEste gráfico ilustra cómo funciona el filtrado colaborativo basado en elementos mediante un ejemplo simplificado. * A la Sra. Blond le gustan las manzanas, las sandías y las piñas. A la Sra. Black le gusta la sandía y la piña. A la Sra. Púrpura le gustan las sandías y las uvas. * Debido a que a la Sra. Black y la Sra. Purple les gustan tanto las sandías como las piñas, consideramos que las sandías y las piñas son artículos similares. * Dado que a la Sra. Púrpura le gustan las sandías y aún no ha estado expuesta a la piña, el sistema de recomendación recomienda la piña a la Sra. Púrpura.\n\n\n\nEn el primer paso, importaremos las bibliotecas de Python pandas, numpy y scipy.stats. Estas tres bibliotecas son para procesamiento de datos y cálculos.\nTambién importamos seaborn para la visualización y cosine_similarity para calcular el puntaje de similitud.\n\n# Data processing\nimport pandas as pd\nimport numpy as np\nimport scipy.stats\n\n# Visualization\nimport seaborn as sns\n\n# Similarity\nfrom sklearn.metrics.pairwise import cosine_similarity\n\nimport warnings\nwarnings.simplefilter('ignore')\n\n\n\n\nEste tutorial utiliza el conjunto de datos movielens. Este conjunto de datos contiene calificaciones reales de usuarios de películas.\nSeguiremos los pasos a continuación para obtener los conjuntos de datos: 1. Vaya a https://grouplens.org/datasets/movielens/ 2. Descargue el conjunto de datos de 100k con el nombre de archivo “ml-latest-small.zip” 3. Descomprima “ml-latest-small.zip” 4. Copie la carpeta “ml-latest-small” en la carpeta de su proyecto\nHay varios conjuntos de datos en la carpeta 100k movielens. Para este tutorial, utilizaremos dos clasificaciones y películas. Ahora vamos a leer en los datos de calificación (ratings.csv).\n\n# Read in data\nratings=pd.read_csv('data/ml-latest-small/ratings.csv')\n\n# Take a look at the data\nratings.head()\n\n\n\n\n\n\n\n\n\nuserId\nmovieId\nrating\ntimestamp\n\n\n\n\n0\n1\n1\n4.0\n964982703\n\n\n1\n1\n3\n4.0\n964981247\n\n\n2\n1\n6\n4.0\n964982224\n\n\n3\n1\n47\n5.0\n964983815\n\n\n4\n1\n50\n5.0\n964982931\n\n\n\n\n\n\n\n\nHay cuatro columnas en el conjunto de datos de calificaciones:\n\nuserID\nmovieID\nrating\ntimestamp\n\nEl conjunto de datos tiene más de 100 000 registros y no falta ningún dato.\n\n# Get the dataset information\nratings.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 100836 entries, 0 to 100835\nData columns (total 4 columns):\n #   Column     Non-Null Count   Dtype  \n---  ------     --------------   -----  \n 0   userId     100836 non-null  int64  \n 1   movieId    100836 non-null  int64  \n 2   rating     100836 non-null  float64\n 3   timestamp  100836 non-null  int64  \ndtypes: float64(1), int64(3)\nmemory usage: 3.1 MB\n\n\nLas calificaciones de 100k son de 610 usuarios en 9724 películas. La calificación tiene diez valores únicos de 0.5 a 5.\n\n# Number of users\nprint('The ratings dataset has', ratings['userId'].nunique(), 'unique users')\n\n# Number of movies\nprint('The ratings dataset has', ratings['movieId'].nunique(), 'unique movies')\n\n# Number of ratings\nprint('The ratings dataset has', ratings['rating'].nunique(), 'unique ratings')\n\n# List of unique ratings\nprint('The unique ratings are', sorted(ratings['rating'].unique()))\n\nThe ratings dataset has 610 unique users\nThe ratings dataset has 9724 unique movies\nThe ratings dataset has 10 unique ratings\nThe unique ratings are [0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0, 4.5, 5.0]\n\n\nA continuación, leamos los datos de las películas para obtener los nombres de las películas (movies.csv).\nEl conjunto de datos de películas tiene: * movieId * title * genres\n\n# Read in data\nmovies = pd.read_csv('data/ml-latest-small/movies.csv')\n\n# Take a look at the data\nmovies.head()\n\n\n\n\n\n\n\n\n\nmovieId\ntitle\ngenres\n\n\n\n\n0\n1\nToy Story (1995)\nAdventure|Animation|Children|Comedy|Fantasy\n\n\n1\n2\nJumanji (1995)\nAdventure|Children|Fantasy\n\n\n2\n3\nGrumpier Old Men (1995)\nComedy|Romance\n\n\n3\n4\nWaiting to Exhale (1995)\nComedy|Drama|Romance\n\n\n4\n5\nFather of the Bride Part II (1995)\nComedy\n\n\n\n\n\n\n\n\nUsando movieID como clave coincidente, agregamos información de la película al conjunto de datos de calificación y lo llamamos df. ¡Así que ahora tenemos el título de la película y la calificación de la película en el mismo conjunto de datos!\n\n# Merge ratings and movies datasets\ndf = pd.merge(ratings, movies, on='movieId', how='inner')\n\n# Take a look at the data\ndf.head()\n\n\n\n\n\n\n\n\n\nuserId\nmovieId\nrating\ntimestamp\ntitle\ngenres\n\n\n\n\n0\n1\n1\n4.0\n964982703\nToy Story (1995)\nAdventure|Animation|Children|Comedy|Fantasy\n\n\n1\n5\n1\n4.0\n847434962\nToy Story (1995)\nAdventure|Animation|Children|Comedy|Fantasy\n\n\n2\n7\n1\n4.5\n1106635946\nToy Story (1995)\nAdventure|Animation|Children|Comedy|Fantasy\n\n\n3\n15\n1\n2.5\n1510577970\nToy Story (1995)\nAdventure|Animation|Children|Comedy|Fantasy\n\n\n4\n17\n1\n4.5\n1305696483\nToy Story (1995)\nAdventure|Animation|Children|Comedy|Fantasy\n\n\n\n\n\n\n\n\n\n\n\nDebemos filtrar las películas y mantener solo aquellas con más de 100 calificaciones para el análisis. Esto es para que el cálculo sea manejable por la memoria de Google Colab.\nPara hacerlo, primero agrupamos las películas por título, contamos el número de calificaciones y mantenemos solo las películas con más de 100 calificaciones.\nLas calificaciones promedio de las películas también se calculan.\nDesde la salida .info(), podemos ver que quedan 134 películas.\n\n# Aggregate by movie\nagg_ratings = df.groupby('title').agg(mean_rating = ('rating', 'mean'),\n                                                number_of_ratings = ('rating', 'count')).reset_index()\n\n# Keep the movies with over 100 ratings\nagg_ratings_GT100 = agg_ratings[agg_ratings['number_of_ratings']&gt;100]\nagg_ratings_GT100.info()  \n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nInt64Index: 134 entries, 74 to 9615\nData columns (total 3 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   title              134 non-null    object \n 1   mean_rating        134 non-null    float64\n 2   number_of_ratings  134 non-null    int64  \ndtypes: float64(1), int64(1), object(1)\nmemory usage: 4.2+ KB\n\n\nVeamos cuáles son las películas más populares y sus calificaciones.\n\n# Check popular movies\nagg_ratings_GT100.sort_values(by='number_of_ratings', ascending=False).head()\n\n\n\n\n\n\n\n\n\ntitle\nmean_rating\nnumber_of_ratings\n\n\n\n\n3158\nForrest Gump (1994)\n4.164134\n329\n\n\n7593\nShawshank Redemption, The (1994)\n4.429022\n317\n\n\n6865\nPulp Fiction (1994)\n4.197068\n307\n\n\n7680\nSilence of the Lambs, The (1991)\n4.161290\n279\n\n\n5512\nMatrix, The (1999)\n4.192446\n278\n\n\n\n\n\n\n\n\nA continuación, usemos un jointplot para verificar la correlación entre la calificación promedio y el número de calificaciones.\nPodemos ver una tendencia ascendente en el diagrama de dispersión, que muestra que las películas populares obtienen calificaciones más altas.\nLa distribución de calificación promedio muestra que la mayoría de las películas en el conjunto de datos tienen una calificación promedio de alrededor de 4.\nEl número de distribución de calificaciones muestra que la mayoría de las películas tienen menos de 150 calificaciones.\n\n# Visulization\nsns.jointplot(\n    x='mean_rating',\n    y='number_of_ratings',\n    data=agg_ratings_GT100,\n    color='gray'\n)\n\n\n\n\n\n\n\n\nPara mantener solo las 134 películas con más de 100 calificaciones, debemos unir la película con el dataframe del nivel de calificación del usuario.\nhow='inner' y on='title' aseguran que solo se incluyan las películas con más de 100 calificaciones.\n\n# Merge data\ndf_GT100 = pd.merge(df, agg_ratings_GT100[['title']], on='title', how='inner')\ndf_GT100.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nInt64Index: 19788 entries, 0 to 19787\nData columns (total 6 columns):\n #   Column     Non-Null Count  Dtype  \n---  ------     --------------  -----  \n 0   userId     19788 non-null  int64  \n 1   movieId    19788 non-null  int64  \n 2   rating     19788 non-null  float64\n 3   timestamp  19788 non-null  int64  \n 4   title      19788 non-null  object \n 5   genres     19788 non-null  object \ndtypes: float64(1), int64(3), object(2)\nmemory usage: 1.1+ MB\n\n\nDespués de filtrar las películas con más de 100 calificaciones, tenemos 597 usuarios que calificaron 134 películas.\n\n# Number of users\nprint('The ratings dataset has', df_GT100['userId'].nunique(), 'unique users')\n\n# Number of movies\nprint('The ratings dataset has', df_GT100['movieId'].nunique(), 'unique movies')\n\n# Number of ratings\nprint('The ratings dataset has', df_GT100['rating'].nunique(), 'unique ratings')\n\n# List of unique ratings\nprint('The unique ratings are', sorted(df_GT100['rating'].unique()))\n\nThe ratings dataset has 597 unique users\nThe ratings dataset has 134 unique movies\nThe ratings dataset has 10 unique ratings\nThe unique ratings are [0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0, 4.5, 5.0]\n\n\n\n\n\nTransformaremos el conjunto de datos en un formato de matriz. Las filas de la matriz son usuarios y las columnas de la matriz son películas. El valor de la matriz es la calificación de usuario de la película si hay una calificación. De lo contrario, muestra NaN.\n\n# Create user-item matrix\nmatrix = df_GT100.pivot_table(index='userId', columns='title', values='rating')\nmatrix.head()\n\n\n\n\n\n\n\n\ntitle\n2001: A Space Odyssey (1968)\nAce Ventura: Pet Detective (1994)\nAladdin (1992)\nAlien (1979)\nAliens (1986)\nAmelie (Fabuleux destin d'Amélie Poulain, Le) (2001)\nAmerican Beauty (1999)\nAmerican History X (1998)\nAmerican Pie (1999)\nApocalypse Now (1979)\n...\nTrue Lies (1994)\nTruman Show, The (1998)\nTwelve Monkeys (a.k.a. 12 Monkeys) (1995)\nTwister (1996)\nUp (2009)\nUsual Suspects, The (1995)\nWALL·E (2008)\nWaterworld (1995)\nWilly Wonka & the Chocolate Factory (1971)\nX-Men (2000)\n\n\nuserId\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\nNaN\nNaN\nNaN\n4.0\nNaN\nNaN\n5.0\n5.0\nNaN\n4.0\n...\nNaN\nNaN\nNaN\n3.0\nNaN\n5.0\nNaN\nNaN\n5.0\n5.0\n\n\n2\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n3\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4\nNaN\nNaN\n4.0\nNaN\nNaN\nNaN\n5.0\nNaN\nNaN\nNaN\n...\nNaN\nNaN\n2.0\nNaN\nNaN\nNaN\nNaN\nNaN\n4.0\nNaN\n\n\n5\nNaN\n3.0\n4.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\n2.0\nNaN\nNaN\nNaN\nNaN\n4.0\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n5 rows × 134 columns\n\n\n\n\n\n\n\nDado que algunas personas tienden a dar una calificación más alta que otras, normalizamos la calificación extrayendo la calificación promedio de cada usuario.\nDespués de la normalización, las películas con una calificación inferior a la calificación promedio del usuario obtienen un valor negativo y las películas con una calificación superior a la calificación promedio del usuario obtienen un valor positivo.\n\n# Normalize user-item matrix\nmatrix_norm = matrix.subtract(matrix.mean(axis=1), axis = 'rows')\nmatrix_norm.head()\n\n\n\n\n\n\n\n\ntitle\n2001: A Space Odyssey (1968)\nAce Ventura: Pet Detective (1994)\nAladdin (1992)\nAlien (1979)\nAliens (1986)\nAmelie (Fabuleux destin d'Amélie Poulain, Le) (2001)\nAmerican Beauty (1999)\nAmerican History X (1998)\nAmerican Pie (1999)\nApocalypse Now (1979)\n...\nTrue Lies (1994)\nTruman Show, The (1998)\nTwelve Monkeys (a.k.a. 12 Monkeys) (1995)\nTwister (1996)\nUp (2009)\nUsual Suspects, The (1995)\nWALL·E (2008)\nWaterworld (1995)\nWilly Wonka & the Chocolate Factory (1971)\nX-Men (2000)\n\n\nuserId\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\nNaN\nNaN\nNaN\n-0.392857\nNaN\nNaN\n0.607143\n0.607143\nNaN\n-0.392857\n...\nNaN\nNaN\nNaN\n-1.392857\nNaN\n0.607143\nNaN\nNaN\n0.607143\n0.607143\n\n\n2\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n3\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4\nNaN\nNaN\n0.617647\nNaN\nNaN\nNaN\n1.617647\nNaN\nNaN\nNaN\n...\nNaN\nNaN\n-1.382353\nNaN\nNaN\nNaN\nNaN\nNaN\n0.617647\nNaN\n\n\n5\nNaN\n-0.461538\n0.538462\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\n-1.461538\nNaN\nNaN\nNaN\nNaN\n0.538462\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n5 rows × 134 columns\n\n\n\n\n\n\n\nHay diferentes formas de medir las similitudes. La correlación de Pearson y la similitud del coseno son dos métodos ampliamente utilizados.\nEn este tutorial, calcularemos la matriz de similitud del usuario utilizando la correlación de Pearson.\n\n# User similarity matrix using Pearson correlation\nuser_similarity = matrix_norm.T.corr()\nuser_similarity.head()\n\n\n\n\n\n\n\n\nuserId\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n...\n601\n602\n603\n604\n605\n606\n607\n608\n609\n610\n\n\nuserId\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\n1.000000\nNaN\nNaN\n0.391797\n0.180151\n-0.439941\n-0.029894\n0.464277\n1.0\n-0.037987\n...\n0.091574\n0.254514\n0.101482\n-0.500000\n0.780020\n0.303854\n-0.012077\n0.242309\n-0.175412\n0.071553\n\n\n2\nNaN\n1.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n1.000000\n...\n-0.583333\nNaN\n-1.000000\nNaN\nNaN\n0.583333\nNaN\n-0.229416\nNaN\n0.765641\n\n\n3\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4\n0.391797\nNaN\nNaN\n1.000000\n-0.394823\n0.421927\n0.704669\n0.055442\nNaN\n0.360399\n...\n-0.239325\n0.562500\n0.162301\n-0.158114\n0.905134\n0.021898\n-0.020659\n-0.286872\nNaN\n-0.050868\n\n\n5\n0.180151\nNaN\nNaN\n-0.394823\n1.000000\n-0.006888\n0.328889\n0.030168\nNaN\n-0.777714\n...\n0.000000\n0.231642\n0.131108\n0.068621\n-0.245026\n0.377341\n0.228218\n0.263139\n0.384111\n0.040582\n\n\n\n\n5 rows × 597 columns\n\n\n\n\nAquellos que estén interesados en usar la similitud del coseno pueden consultar este código. Dado que cosine_similarity no toma valores perdidos, necesitamos imputar los valores perdidos con 0 antes del cálculo.\n\n# User similarity matrix using cosine similarity\nuser_similarity_cosine = cosine_similarity(matrix_norm.fillna(0))\nuser_similarity_cosine\n\narray([[ 1.        ,  0.        ,  0.        , ...,  0.14893867,\n        -0.06003146,  0.04528224],\n       [ 0.        ,  1.        ,  0.        , ..., -0.04485403,\n        -0.25197632,  0.18886414],\n       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n         0.        ,  0.        ],\n       ...,\n       [ 0.14893867, -0.04485403,  0.        , ...,  1.        ,\n         0.14734568,  0.07931015],\n       [-0.06003146, -0.25197632,  0.        , ...,  0.14734568,\n         1.        , -0.14276787],\n       [ 0.04528224,  0.18886414,  0.        , ...,  0.07931015,\n        -0.14276787,  1.        ]])\n\n\nAhora usemos el ID de usuario 1 como ejemplo para ilustrar cómo encontrar usuarios similares.\nPrimero debemos excluir el ID de usuario 1 de la lista de usuarios similares y decidir el número de usuarios similares.\n\n# Pick a user ID\npicked_userid = 1\n\n# Remove picked user ID from the candidate list\nuser_similarity.drop(index=picked_userid, inplace=True)\n\n# Take a look at the data\nuser_similarity.head()\n\n\n\n\n\n\n\n\nuserId\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n...\n601\n602\n603\n604\n605\n606\n607\n608\n609\n610\n\n\nuserId\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2\nNaN\n1.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n1.000000\n...\n-0.583333\nNaN\n-1.000000\nNaN\nNaN\n0.583333\nNaN\n-0.229416\nNaN\n0.765641\n\n\n3\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4\n0.391797\nNaN\nNaN\n1.000000\n-0.394823\n0.421927\n0.704669\n0.055442\nNaN\n0.360399\n...\n-0.239325\n0.562500\n0.162301\n-0.158114\n0.905134\n0.021898\n-0.020659\n-0.286872\nNaN\n-0.050868\n\n\n5\n0.180151\nNaN\nNaN\n-0.394823\n1.000000\n-0.006888\n0.328889\n0.030168\nNaN\n-0.777714\n...\n0.000000\n0.231642\n0.131108\n0.068621\n-0.245026\n0.377341\n0.228218\n0.263139\n0.384111\n0.040582\n\n\n6\n-0.439941\nNaN\nNaN\n0.421927\n-0.006888\n1.000000\n0.000000\n-0.127385\nNaN\n0.957427\n...\n-0.292770\n-0.030599\n-0.123983\n-0.176327\n0.063861\n-0.468008\n0.541386\n-0.337129\n0.158255\n-0.030567\n\n\n\n\n5 rows × 597 columns\n\n\n\n\nEn la matriz de similitud del usuario, los valores varían de -1 a 1, donde -1 significa la preferencia de película opuesta y 1 significa la misma preferencia de película.\nn = 10 significa que nos gustaría elegir los 10 usuarios más similares para el ID de usuario 1.\nEl filtrado colaborativo basado en usuarios hace recomendaciones basadas en usuarios con gustos similares, por lo que debemos establecer un umbral positivo. Aquí configuramos user_similarity_threshold en 0,3, lo que significa que un usuario debe tener un coeficiente de correlación de Pearson de al menos 0,3 para ser considerado como un usuario similar.\nDespués de establecer la cantidad de usuarios similares y el umbral de similitud, clasificamos el valor de similitud del usuario del más alto al más bajo, luego imprimimos la ID de los usuarios más similares y el valor de correlación de Pearson.\n\n# Number of similar users\nn = 10\n\n# User similarity threashold\nuser_similarity_threshold = 0.3\n\n# Get top n similar users\nsimilar_users = user_similarity[user_similarity[picked_userid]&gt;user_similarity_threshold][picked_userid].sort_values(ascending=False)[:n]\n\n# Print out top n similar users\nprint(f'The similar users for user {picked_userid} are', similar_users)\n\nThe similar users for user 1 are userId\n108    1.000000\n9      1.000000\n550    1.000000\n598    1.000000\n502    1.000000\n401    0.942809\n511    0.925820\n366    0.872872\n154    0.866025\n595    0.866025\nName: 1, dtype: float64\n\n\n\n\n\nReduciremos el grupo de artículos haciendo lo siguiente:\n\nElimine las películas que ha visto el usuario de destino (ID de usuario 1 en este ejemplo).\nGuarde solo las películas que otros usuarios similares hayan visto.\n\nPara eliminar las películas vistas por el usuario objetivo, mantenemos solo la fila para userId=1 en la matriz de elementos de usuario y eliminamos los elementos con valores faltantes.\n\n# Movies that the target user has watched\npicked_userid_watched = matrix_norm[matrix_norm.index == picked_userid].dropna(axis=1, how='all')\npicked_userid_watched\n\n\n\n\n\n\n\n\ntitle\nAlien (1979)\nAmerican Beauty (1999)\nAmerican History X (1998)\nApocalypse Now (1979)\nBack to the Future (1985)\nBatman (1989)\nBig Lebowski, The (1998)\nBraveheart (1995)\nClear and Present Danger (1994)\nClerks (1994)\n...\nStar Wars: Episode IV - A New Hope (1977)\nStar Wars: Episode V - The Empire Strikes Back (1980)\nStar Wars: Episode VI - Return of the Jedi (1983)\nStargate (1994)\nTerminator, The (1984)\nToy Story (1995)\nTwister (1996)\nUsual Suspects, The (1995)\nWilly Wonka & the Chocolate Factory (1971)\nX-Men (2000)\n\n\nuserId\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\n-0.392857\n0.607143\n0.607143\n-0.392857\n0.607143\n-0.392857\n0.607143\n-0.392857\n-0.392857\n-1.392857\n...\n0.607143\n0.607143\n0.607143\n-1.392857\n0.607143\n-0.392857\n-1.392857\n0.607143\n0.607143\n0.607143\n\n\n\n\n1 rows × 56 columns\n\n\n\n\nPara mantener solo las películas de los usuarios similares, mantenemos los ID de usuario en las 10 listas de usuarios similares principales y eliminamos la película con todos los valores faltantes. Todo valor faltante para una película significa que ninguno de los usuarios similares ha visto la película.\n\n# Movies that similar users watched. Remove movies that none of the similar users have watched\nsimilar_user_movies = matrix_norm[matrix_norm.index.isin(similar_users.index)].dropna(axis=1, how='all')\nsimilar_user_movies\n\n\n\n\n\n\n\n\ntitle\nAladdin (1992)\nAlien (1979)\nAmelie (Fabuleux destin d'Amélie Poulain, Le) (2001)\nBack to the Future (1985)\nBatman Begins (2005)\nBeautiful Mind, A (2001)\nBeauty and the Beast (1991)\nBlade Runner (1982)\nBourne Identity, The (2002)\nBraveheart (1995)\n...\nShrek (2001)\nSilence of the Lambs, The (1991)\nSpider-Man (2002)\nStar Wars: Episode I - The Phantom Menace (1999)\nTerminator 2: Judgment Day (1991)\nTitanic (1997)\nToy Story (1995)\nUp (2009)\nUsual Suspects, The (1995)\nWALL·E (2008)\n\n\nuserId\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n9\nNaN\nNaN\nNaN\n0.333333\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n108\nNaN\nNaN\n0.466667\n0.466667\nNaN\n0.466667\nNaN\n0.466667\nNaN\nNaN\n...\nNaN\nNaN\n0.466667\nNaN\nNaN\n-0.533333\nNaN\nNaN\nNaN\nNaN\n\n\n154\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n0.214286\nNaN\nNaN\n\n\n366\nNaN\nNaN\nNaN\nNaN\n-0.205882\nNaN\nNaN\nNaN\nNaN\n-0.205882\n...\nNaN\nNaN\nNaN\nNaN\n-0.205882\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n401\n-0.382353\nNaN\nNaN\nNaN\nNaN\nNaN\n-0.382353\nNaN\nNaN\nNaN\n...\n0.117647\nNaN\nNaN\nNaN\nNaN\nNaN\n0.117647\n0.617647\nNaN\n0.617647\n\n\n502\nNaN\n-0.375\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n511\nNaN\nNaN\n-0.653846\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\n-1.153846\n-0.653846\nNaN\nNaN\nNaN\n-0.153846\nNaN\nNaN\n\n\n550\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n-0.277778\n0.222222\nNaN\n-0.277778\n\n\n595\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\n-0.333333\nNaN\nNaN\nNaN\nNaN\n0.666667\nNaN\n\n\n598\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n0.888889\nNaN\n...\n-2.111111\n-2.611111\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n10 rows × 62 columns\n\n\n\n\nA continuación, eliminaremos las películas que el usuario ID 1 vio de la lista de películas de usuarios similares. errors='ignore' elimina columnas si existen sin dar un mensaje de error.\n\n# Remove the watched movie from the movie list\nsimilar_user_movies.drop(picked_userid_watched.columns,axis=1, inplace=True, errors='ignore')\n\n# Take a look at the data\nsimilar_user_movies\n\n\n\n\n\n\n\n\ntitle\nAladdin (1992)\nAmelie (Fabuleux destin d'Amélie Poulain, Le) (2001)\nBatman Begins (2005)\nBeautiful Mind, A (2001)\nBeauty and the Beast (1991)\nBlade Runner (1982)\nBourne Identity, The (2002)\nBreakfast Club, The (1985)\nCatch Me If You Can (2002)\nDark Knight, The (2008)\n...\nMonsters, Inc. (2001)\nOcean's Eleven (2001)\nPirates of the Caribbean: The Curse of the Black Pearl (2003)\nShawshank Redemption, The (1994)\nShrek (2001)\nSpider-Man (2002)\nTerminator 2: Judgment Day (1991)\nTitanic (1997)\nUp (2009)\nWALL·E (2008)\n\n\nuserId\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n9\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n108\nNaN\n0.466667\nNaN\n0.466667\nNaN\n0.466667\nNaN\n-0.533333\n0.466667\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\n0.466667\nNaN\n-0.533333\nNaN\nNaN\n\n\n154\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n0.214286\nNaN\n\n\n366\nNaN\nNaN\n-0.205882\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n-0.205882\n...\nNaN\nNaN\n-0.205882\nNaN\nNaN\nNaN\n-0.205882\nNaN\nNaN\nNaN\n\n\n401\n-0.382353\nNaN\nNaN\nNaN\n-0.382353\nNaN\nNaN\nNaN\nNaN\nNaN\n...\n0.117647\nNaN\n0.117647\nNaN\n0.117647\nNaN\nNaN\nNaN\n0.617647\n0.617647\n\n\n502\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\n0.125000\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n511\nNaN\n-0.653846\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\n0.346154\nNaN\n-1.153846\nNaN\nNaN\n-0.153846\nNaN\n\n\n550\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n-0.277778\n-0.277778\n...\nNaN\nNaN\nNaN\n0.222222\nNaN\nNaN\nNaN\nNaN\n0.222222\n-0.277778\n\n\n595\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n598\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n0.888889\nNaN\nNaN\nNaN\n...\nNaN\n0.888889\nNaN\nNaN\n-2.111111\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n10 rows × 38 columns\n\n\n\n\n\n\n\nDecidiremos qué película recomendar al usuario objetivo. Los elementos recomendados están determinados por el promedio ponderado del puntaje de similitud del usuario y la calificación de la película. Las calificaciones de las películas están ponderadas por las puntuaciones de similitud, por lo que los usuarios con mayor similitud obtienen una mayor ponderación.\nEste código recorre los elementos y los usuarios para obtener la puntuación del elemento, clasificar la puntuación de mayor a menor y elegir las 10 mejores películas para recomendar al ID de usuario 1.\n\n# A dictionary to store item scores\nitem_score = {}\n\n# Loop through items\nfor i in similar_user_movies.columns:\n  # Get the ratings for movie i\n  movie_rating = similar_user_movies[i]\n  # Create a variable to store the score\n  total = 0\n  # Create a variable to store the number of scores\n  count = 0\n  # Loop through similar users\n  for u in similar_users.index:\n    # If the movie has rating\n    if pd.isna(movie_rating[u]) == False:\n      # Score is the sum of user similarity score multiply by the movie rating\n      score = similar_users[u] * movie_rating[u]\n      # Add the score to the total score for the movie so far\n      total += score\n      # Add 1 to the count\n      count +=1\n  # Get the average score for the item\n  item_score[i] = total / count\n\n# Convert dictionary to pandas dataframe\nitem_score = pd.DataFrame(item_score.items(), columns=['movie', 'movie_score'])\n    \n# Sort the movies by score\nranked_item_score = item_score.sort_values(by='movie_score', ascending=False)\n\n# Select top m movies\nm = 10\nranked_item_score.head(m)\n\n\n\n\n\n\n\n\n\nmovie\nmovie_score\n\n\n\n\n16\nHarry Potter and the Chamber of Secrets (2002)\n1.888889\n\n\n13\nEternal Sunshine of the Spotless Mind (2004)\n1.888889\n\n\n6\nBourne Identity, The (2002)\n0.888889\n\n\n29\nOcean's Eleven (2001)\n0.888889\n\n\n18\nInception (2010)\n0.587491\n\n\n3\nBeautiful Mind, A (2001)\n0.466667\n\n\n5\nBlade Runner (1982)\n0.466667\n\n\n12\nDonnie Darko (2001)\n0.466667\n\n\n10\nDeparted, The (2006)\n0.256727\n\n\n31\nShawshank Redemption, The (1994)\n0.222566\n\n\n\n\n\n\n\n\n\n\n\nSi el objetivo es elegir los elementos recomendados, basta con tener el rango de los elementos. Sin embargo, si el objetivo es predecir la calificación del usuario, debemos sumar la calificación promedio de la película del usuario a la calificación de la película.\n\n# Average rating for the picked user\navg_rating = matrix[matrix.index == picked_userid].T.mean()[picked_userid]\n\n# Print the average movie rating for user 1\nprint(f'The average movie rating for user {picked_userid} is {avg_rating:.2f}')\n\nThe average movie rating for user 1 is 4.39\n\n\nLa calificación promedio de la película para el usuario 1 es 4.39, por lo que agregamos 4.39 nuevamente a la calificación de la película.\n\n# Calcuate the predicted rating\nranked_item_score['predicted_rating'] = ranked_item_score['movie_score'] + avg_rating\n\n# Take a look at the data\nranked_item_score.head(m)\n\n\n\n\n\n\n\n\n\nmovie\nmovie_score\npredicted_rating\n\n\n\n\n16\nHarry Potter and the Chamber of Secrets (2002)\n1.888889\n6.281746\n\n\n13\nEternal Sunshine of the Spotless Mind (2004)\n1.888889\n6.281746\n\n\n6\nBourne Identity, The (2002)\n0.888889\n5.281746\n\n\n29\nOcean's Eleven (2001)\n0.888889\n5.281746\n\n\n18\nInception (2010)\n0.587491\n4.980348\n\n\n3\nBeautiful Mind, A (2001)\n0.466667\n4.859524\n\n\n5\nBlade Runner (1982)\n0.466667\n4.859524\n\n\n12\nDonnie Darko (2001)\n0.466667\n4.859524\n\n\n10\nDeparted, The (2006)\n0.256727\n4.649584\n\n\n31\nShawshank Redemption, The (1994)\n0.222566\n4.615423\n\n\n\n\n\n\n\n\nPodemos ver que las 10 mejores películas recomendadas tienen calificaciones pronosticadas superiores a 4.5.\n\n\n\nEn este tutorial, analizamos cómo crear un sistema de recomendación de filtrado colaborativo basado en el usuario. Aprendiste * ¿Qué es el filtrado colaborativo basado en usuarios (usuario-usuario)? * ¿Cómo crear una matriz usuario-producto? * ¿Cómo procesar los datos para el filtrado colaborativo basado en el usuario? * ¿Cómo identificar usuarios similares? * ¿Cómo reducir el grupo de elementos? * ¿Cómo clasificar los artículos para la recomendación? * ¿Cómo predecir la puntuación de calificación?\n\n\n\n\nUser-Based Collaborative Filtering.\nUser-Based Collaborative Filtering In Python | Machine Learning.\nCollaborative Filtering : Data Science Concepts."
  },
  {
    "objectID": "posts/2022/2022-10-12-implicit.html#introducción",
    "href": "posts/2022/2022-10-12-implicit.html#introducción",
    "title": "Collaborative Filtering",
    "section": "",
    "text": "Recommendation System: User-Based Collaborative Filtering o Filtrado Colaborativo Basado en el Usuario es un tipo de algoritmo de sistema de recomendación que utiliza la similitud del usuario para hacer recomendaciones de productos.\nEn este tutorial, hablaremos sobre * ¿Qué es el filtrado colaborativo basado en usuarios (usuario-usuario)? * ¿Cómo crear una matriz usuario-producto? * ¿Cómo procesar los datos para el filtrado colaborativo basado en el usuario? * ¿Cómo identificar usuarios similares? * ¿Cómo reducir el grupo de elementos? * ¿Cómo clasificar los artículos para la recomendación? * ¿Cómo predecir la puntuación de calificación?\n\nNota: Para entender a profundidad este algoritmo, se recomienda leer el documento oficial."
  },
  {
    "objectID": "posts/2022/2022-10-12-implicit.html#algoritmo",
    "href": "posts/2022/2022-10-12-implicit.html#algoritmo",
    "title": "Collaborative Filtering",
    "section": "",
    "text": "En primer lugar, comprendamos cómo funciona el filtrado colaborativo basado en usuarios.\nEl filtrado colaborativo basado en el usuario hace recomendaciones basadas en las interacciones del usuario con el producto en el pasado. La suposición detrás del algoritmo es que a usuarios similares les gustan productos similares.\nEl algoritmo de filtrado colaborativo basado en el usuario generalmente tiene los siguientes pasos:\n\nEncuentre usuarios similares en función de las interacciones con elementos comunes.\nIdentifique los elementos con una calificación alta por parte de usuarios similares pero que no han sido expuestos al usuario activo de interés.\nCalcular la puntuación media ponderada de cada elemento.\nClasifique los elementos según la puntuación y elija los n mejores elementos para recomendar.\n\n\n\n\nimage.png\n\n\nEste gráfico ilustra cómo funciona el filtrado colaborativo basado en elementos mediante un ejemplo simplificado. * A la Sra. Blond le gustan las manzanas, las sandías y las piñas. A la Sra. Black le gusta la sandía y la piña. A la Sra. Púrpura le gustan las sandías y las uvas. * Debido a que a la Sra. Black y la Sra. Purple les gustan tanto las sandías como las piñas, consideramos que las sandías y las piñas son artículos similares. * Dado que a la Sra. Púrpura le gustan las sandías y aún no ha estado expuesta a la piña, el sistema de recomendación recomienda la piña a la Sra. Púrpura."
  },
  {
    "objectID": "posts/2022/2022-10-12-implicit.html#importar-librerías",
    "href": "posts/2022/2022-10-12-implicit.html#importar-librerías",
    "title": "Collaborative Filtering",
    "section": "",
    "text": "En el primer paso, importaremos las bibliotecas de Python pandas, numpy y scipy.stats. Estas tres bibliotecas son para procesamiento de datos y cálculos.\nTambién importamos seaborn para la visualización y cosine_similarity para calcular el puntaje de similitud.\n\n# Data processing\nimport pandas as pd\nimport numpy as np\nimport scipy.stats\n\n# Visualization\nimport seaborn as sns\n\n# Similarity\nfrom sklearn.metrics.pairwise import cosine_similarity\n\nimport warnings\nwarnings.simplefilter('ignore')"
  },
  {
    "objectID": "posts/2022/2022-10-12-implicit.html#descargar-y-leer-datos",
    "href": "posts/2022/2022-10-12-implicit.html#descargar-y-leer-datos",
    "title": "Collaborative Filtering",
    "section": "",
    "text": "Este tutorial utiliza el conjunto de datos movielens. Este conjunto de datos contiene calificaciones reales de usuarios de películas.\nSeguiremos los pasos a continuación para obtener los conjuntos de datos: 1. Vaya a https://grouplens.org/datasets/movielens/ 2. Descargue el conjunto de datos de 100k con el nombre de archivo “ml-latest-small.zip” 3. Descomprima “ml-latest-small.zip” 4. Copie la carpeta “ml-latest-small” en la carpeta de su proyecto\nHay varios conjuntos de datos en la carpeta 100k movielens. Para este tutorial, utilizaremos dos clasificaciones y películas. Ahora vamos a leer en los datos de calificación (ratings.csv).\n\n# Read in data\nratings=pd.read_csv('data/ml-latest-small/ratings.csv')\n\n# Take a look at the data\nratings.head()\n\n\n\n\n\n\n\n\n\nuserId\nmovieId\nrating\ntimestamp\n\n\n\n\n0\n1\n1\n4.0\n964982703\n\n\n1\n1\n3\n4.0\n964981247\n\n\n2\n1\n6\n4.0\n964982224\n\n\n3\n1\n47\n5.0\n964983815\n\n\n4\n1\n50\n5.0\n964982931\n\n\n\n\n\n\n\n\nHay cuatro columnas en el conjunto de datos de calificaciones:\n\nuserID\nmovieID\nrating\ntimestamp\n\nEl conjunto de datos tiene más de 100 000 registros y no falta ningún dato.\n\n# Get the dataset information\nratings.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 100836 entries, 0 to 100835\nData columns (total 4 columns):\n #   Column     Non-Null Count   Dtype  \n---  ------     --------------   -----  \n 0   userId     100836 non-null  int64  \n 1   movieId    100836 non-null  int64  \n 2   rating     100836 non-null  float64\n 3   timestamp  100836 non-null  int64  \ndtypes: float64(1), int64(3)\nmemory usage: 3.1 MB\n\n\nLas calificaciones de 100k son de 610 usuarios en 9724 películas. La calificación tiene diez valores únicos de 0.5 a 5.\n\n# Number of users\nprint('The ratings dataset has', ratings['userId'].nunique(), 'unique users')\n\n# Number of movies\nprint('The ratings dataset has', ratings['movieId'].nunique(), 'unique movies')\n\n# Number of ratings\nprint('The ratings dataset has', ratings['rating'].nunique(), 'unique ratings')\n\n# List of unique ratings\nprint('The unique ratings are', sorted(ratings['rating'].unique()))\n\nThe ratings dataset has 610 unique users\nThe ratings dataset has 9724 unique movies\nThe ratings dataset has 10 unique ratings\nThe unique ratings are [0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0, 4.5, 5.0]\n\n\nA continuación, leamos los datos de las películas para obtener los nombres de las películas (movies.csv).\nEl conjunto de datos de películas tiene: * movieId * title * genres\n\n# Read in data\nmovies = pd.read_csv('data/ml-latest-small/movies.csv')\n\n# Take a look at the data\nmovies.head()\n\n\n\n\n\n\n\n\n\nmovieId\ntitle\ngenres\n\n\n\n\n0\n1\nToy Story (1995)\nAdventure|Animation|Children|Comedy|Fantasy\n\n\n1\n2\nJumanji (1995)\nAdventure|Children|Fantasy\n\n\n2\n3\nGrumpier Old Men (1995)\nComedy|Romance\n\n\n3\n4\nWaiting to Exhale (1995)\nComedy|Drama|Romance\n\n\n4\n5\nFather of the Bride Part II (1995)\nComedy\n\n\n\n\n\n\n\n\nUsando movieID como clave coincidente, agregamos información de la película al conjunto de datos de calificación y lo llamamos df. ¡Así que ahora tenemos el título de la película y la calificación de la película en el mismo conjunto de datos!\n\n# Merge ratings and movies datasets\ndf = pd.merge(ratings, movies, on='movieId', how='inner')\n\n# Take a look at the data\ndf.head()\n\n\n\n\n\n\n\n\n\nuserId\nmovieId\nrating\ntimestamp\ntitle\ngenres\n\n\n\n\n0\n1\n1\n4.0\n964982703\nToy Story (1995)\nAdventure|Animation|Children|Comedy|Fantasy\n\n\n1\n5\n1\n4.0\n847434962\nToy Story (1995)\nAdventure|Animation|Children|Comedy|Fantasy\n\n\n2\n7\n1\n4.5\n1106635946\nToy Story (1995)\nAdventure|Animation|Children|Comedy|Fantasy\n\n\n3\n15\n1\n2.5\n1510577970\nToy Story (1995)\nAdventure|Animation|Children|Comedy|Fantasy\n\n\n4\n17\n1\n4.5\n1305696483\nToy Story (1995)\nAdventure|Animation|Children|Comedy|Fantasy"
  },
  {
    "objectID": "posts/2022/2022-10-12-implicit.html#análisis-exploratorio-de-datos",
    "href": "posts/2022/2022-10-12-implicit.html#análisis-exploratorio-de-datos",
    "title": "Collaborative Filtering",
    "section": "",
    "text": "Debemos filtrar las películas y mantener solo aquellas con más de 100 calificaciones para el análisis. Esto es para que el cálculo sea manejable por la memoria de Google Colab.\nPara hacerlo, primero agrupamos las películas por título, contamos el número de calificaciones y mantenemos solo las películas con más de 100 calificaciones.\nLas calificaciones promedio de las películas también se calculan.\nDesde la salida .info(), podemos ver que quedan 134 películas.\n\n# Aggregate by movie\nagg_ratings = df.groupby('title').agg(mean_rating = ('rating', 'mean'),\n                                                number_of_ratings = ('rating', 'count')).reset_index()\n\n# Keep the movies with over 100 ratings\nagg_ratings_GT100 = agg_ratings[agg_ratings['number_of_ratings']&gt;100]\nagg_ratings_GT100.info()  \n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nInt64Index: 134 entries, 74 to 9615\nData columns (total 3 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   title              134 non-null    object \n 1   mean_rating        134 non-null    float64\n 2   number_of_ratings  134 non-null    int64  \ndtypes: float64(1), int64(1), object(1)\nmemory usage: 4.2+ KB\n\n\nVeamos cuáles son las películas más populares y sus calificaciones.\n\n# Check popular movies\nagg_ratings_GT100.sort_values(by='number_of_ratings', ascending=False).head()\n\n\n\n\n\n\n\n\n\ntitle\nmean_rating\nnumber_of_ratings\n\n\n\n\n3158\nForrest Gump (1994)\n4.164134\n329\n\n\n7593\nShawshank Redemption, The (1994)\n4.429022\n317\n\n\n6865\nPulp Fiction (1994)\n4.197068\n307\n\n\n7680\nSilence of the Lambs, The (1991)\n4.161290\n279\n\n\n5512\nMatrix, The (1999)\n4.192446\n278\n\n\n\n\n\n\n\n\nA continuación, usemos un jointplot para verificar la correlación entre la calificación promedio y el número de calificaciones.\nPodemos ver una tendencia ascendente en el diagrama de dispersión, que muestra que las películas populares obtienen calificaciones más altas.\nLa distribución de calificación promedio muestra que la mayoría de las películas en el conjunto de datos tienen una calificación promedio de alrededor de 4.\nEl número de distribución de calificaciones muestra que la mayoría de las películas tienen menos de 150 calificaciones.\n\n# Visulization\nsns.jointplot(\n    x='mean_rating',\n    y='number_of_ratings',\n    data=agg_ratings_GT100,\n    color='gray'\n)\n\n\n\n\n\n\n\n\nPara mantener solo las 134 películas con más de 100 calificaciones, debemos unir la película con el dataframe del nivel de calificación del usuario.\nhow='inner' y on='title' aseguran que solo se incluyan las películas con más de 100 calificaciones.\n\n# Merge data\ndf_GT100 = pd.merge(df, agg_ratings_GT100[['title']], on='title', how='inner')\ndf_GT100.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nInt64Index: 19788 entries, 0 to 19787\nData columns (total 6 columns):\n #   Column     Non-Null Count  Dtype  \n---  ------     --------------  -----  \n 0   userId     19788 non-null  int64  \n 1   movieId    19788 non-null  int64  \n 2   rating     19788 non-null  float64\n 3   timestamp  19788 non-null  int64  \n 4   title      19788 non-null  object \n 5   genres     19788 non-null  object \ndtypes: float64(1), int64(3), object(2)\nmemory usage: 1.1+ MB\n\n\nDespués de filtrar las películas con más de 100 calificaciones, tenemos 597 usuarios que calificaron 134 películas.\n\n# Number of users\nprint('The ratings dataset has', df_GT100['userId'].nunique(), 'unique users')\n\n# Number of movies\nprint('The ratings dataset has', df_GT100['movieId'].nunique(), 'unique movies')\n\n# Number of ratings\nprint('The ratings dataset has', df_GT100['rating'].nunique(), 'unique ratings')\n\n# List of unique ratings\nprint('The unique ratings are', sorted(df_GT100['rating'].unique()))\n\nThe ratings dataset has 597 unique users\nThe ratings dataset has 134 unique movies\nThe ratings dataset has 10 unique ratings\nThe unique ratings are [0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0, 4.5, 5.0]"
  },
  {
    "objectID": "posts/2022/2022-10-12-implicit.html#matriz-películas-de-usuario",
    "href": "posts/2022/2022-10-12-implicit.html#matriz-películas-de-usuario",
    "title": "Collaborative Filtering",
    "section": "",
    "text": "Transformaremos el conjunto de datos en un formato de matriz. Las filas de la matriz son usuarios y las columnas de la matriz son películas. El valor de la matriz es la calificación de usuario de la película si hay una calificación. De lo contrario, muestra NaN.\n\n# Create user-item matrix\nmatrix = df_GT100.pivot_table(index='userId', columns='title', values='rating')\nmatrix.head()\n\n\n\n\n\n\n\n\ntitle\n2001: A Space Odyssey (1968)\nAce Ventura: Pet Detective (1994)\nAladdin (1992)\nAlien (1979)\nAliens (1986)\nAmelie (Fabuleux destin d'Amélie Poulain, Le) (2001)\nAmerican Beauty (1999)\nAmerican History X (1998)\nAmerican Pie (1999)\nApocalypse Now (1979)\n...\nTrue Lies (1994)\nTruman Show, The (1998)\nTwelve Monkeys (a.k.a. 12 Monkeys) (1995)\nTwister (1996)\nUp (2009)\nUsual Suspects, The (1995)\nWALL·E (2008)\nWaterworld (1995)\nWilly Wonka & the Chocolate Factory (1971)\nX-Men (2000)\n\n\nuserId\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\nNaN\nNaN\nNaN\n4.0\nNaN\nNaN\n5.0\n5.0\nNaN\n4.0\n...\nNaN\nNaN\nNaN\n3.0\nNaN\n5.0\nNaN\nNaN\n5.0\n5.0\n\n\n2\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n3\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4\nNaN\nNaN\n4.0\nNaN\nNaN\nNaN\n5.0\nNaN\nNaN\nNaN\n...\nNaN\nNaN\n2.0\nNaN\nNaN\nNaN\nNaN\nNaN\n4.0\nNaN\n\n\n5\nNaN\n3.0\n4.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\n2.0\nNaN\nNaN\nNaN\nNaN\n4.0\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n5 rows × 134 columns"
  },
  {
    "objectID": "posts/2022/2022-10-12-implicit.html#normalización-de-datos",
    "href": "posts/2022/2022-10-12-implicit.html#normalización-de-datos",
    "title": "Collaborative Filtering",
    "section": "",
    "text": "Dado que algunas personas tienden a dar una calificación más alta que otras, normalizamos la calificación extrayendo la calificación promedio de cada usuario.\nDespués de la normalización, las películas con una calificación inferior a la calificación promedio del usuario obtienen un valor negativo y las películas con una calificación superior a la calificación promedio del usuario obtienen un valor positivo.\n\n# Normalize user-item matrix\nmatrix_norm = matrix.subtract(matrix.mean(axis=1), axis = 'rows')\nmatrix_norm.head()\n\n\n\n\n\n\n\n\ntitle\n2001: A Space Odyssey (1968)\nAce Ventura: Pet Detective (1994)\nAladdin (1992)\nAlien (1979)\nAliens (1986)\nAmelie (Fabuleux destin d'Amélie Poulain, Le) (2001)\nAmerican Beauty (1999)\nAmerican History X (1998)\nAmerican Pie (1999)\nApocalypse Now (1979)\n...\nTrue Lies (1994)\nTruman Show, The (1998)\nTwelve Monkeys (a.k.a. 12 Monkeys) (1995)\nTwister (1996)\nUp (2009)\nUsual Suspects, The (1995)\nWALL·E (2008)\nWaterworld (1995)\nWilly Wonka & the Chocolate Factory (1971)\nX-Men (2000)\n\n\nuserId\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\nNaN\nNaN\nNaN\n-0.392857\nNaN\nNaN\n0.607143\n0.607143\nNaN\n-0.392857\n...\nNaN\nNaN\nNaN\n-1.392857\nNaN\n0.607143\nNaN\nNaN\n0.607143\n0.607143\n\n\n2\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n3\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4\nNaN\nNaN\n0.617647\nNaN\nNaN\nNaN\n1.617647\nNaN\nNaN\nNaN\n...\nNaN\nNaN\n-1.382353\nNaN\nNaN\nNaN\nNaN\nNaN\n0.617647\nNaN\n\n\n5\nNaN\n-0.461538\n0.538462\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\n-1.461538\nNaN\nNaN\nNaN\nNaN\n0.538462\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n5 rows × 134 columns"
  },
  {
    "objectID": "posts/2022/2022-10-12-implicit.html#identifique-usuarios-similares",
    "href": "posts/2022/2022-10-12-implicit.html#identifique-usuarios-similares",
    "title": "Collaborative Filtering",
    "section": "",
    "text": "Hay diferentes formas de medir las similitudes. La correlación de Pearson y la similitud del coseno son dos métodos ampliamente utilizados.\nEn este tutorial, calcularemos la matriz de similitud del usuario utilizando la correlación de Pearson.\n\n# User similarity matrix using Pearson correlation\nuser_similarity = matrix_norm.T.corr()\nuser_similarity.head()\n\n\n\n\n\n\n\n\nuserId\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n...\n601\n602\n603\n604\n605\n606\n607\n608\n609\n610\n\n\nuserId\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\n1.000000\nNaN\nNaN\n0.391797\n0.180151\n-0.439941\n-0.029894\n0.464277\n1.0\n-0.037987\n...\n0.091574\n0.254514\n0.101482\n-0.500000\n0.780020\n0.303854\n-0.012077\n0.242309\n-0.175412\n0.071553\n\n\n2\nNaN\n1.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n1.000000\n...\n-0.583333\nNaN\n-1.000000\nNaN\nNaN\n0.583333\nNaN\n-0.229416\nNaN\n0.765641\n\n\n3\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4\n0.391797\nNaN\nNaN\n1.000000\n-0.394823\n0.421927\n0.704669\n0.055442\nNaN\n0.360399\n...\n-0.239325\n0.562500\n0.162301\n-0.158114\n0.905134\n0.021898\n-0.020659\n-0.286872\nNaN\n-0.050868\n\n\n5\n0.180151\nNaN\nNaN\n-0.394823\n1.000000\n-0.006888\n0.328889\n0.030168\nNaN\n-0.777714\n...\n0.000000\n0.231642\n0.131108\n0.068621\n-0.245026\n0.377341\n0.228218\n0.263139\n0.384111\n0.040582\n\n\n\n\n5 rows × 597 columns\n\n\n\n\nAquellos que estén interesados en usar la similitud del coseno pueden consultar este código. Dado que cosine_similarity no toma valores perdidos, necesitamos imputar los valores perdidos con 0 antes del cálculo.\n\n# User similarity matrix using cosine similarity\nuser_similarity_cosine = cosine_similarity(matrix_norm.fillna(0))\nuser_similarity_cosine\n\narray([[ 1.        ,  0.        ,  0.        , ...,  0.14893867,\n        -0.06003146,  0.04528224],\n       [ 0.        ,  1.        ,  0.        , ..., -0.04485403,\n        -0.25197632,  0.18886414],\n       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n         0.        ,  0.        ],\n       ...,\n       [ 0.14893867, -0.04485403,  0.        , ...,  1.        ,\n         0.14734568,  0.07931015],\n       [-0.06003146, -0.25197632,  0.        , ...,  0.14734568,\n         1.        , -0.14276787],\n       [ 0.04528224,  0.18886414,  0.        , ...,  0.07931015,\n        -0.14276787,  1.        ]])\n\n\nAhora usemos el ID de usuario 1 como ejemplo para ilustrar cómo encontrar usuarios similares.\nPrimero debemos excluir el ID de usuario 1 de la lista de usuarios similares y decidir el número de usuarios similares.\n\n# Pick a user ID\npicked_userid = 1\n\n# Remove picked user ID from the candidate list\nuser_similarity.drop(index=picked_userid, inplace=True)\n\n# Take a look at the data\nuser_similarity.head()\n\n\n\n\n\n\n\n\nuserId\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n...\n601\n602\n603\n604\n605\n606\n607\n608\n609\n610\n\n\nuserId\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2\nNaN\n1.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n1.000000\n...\n-0.583333\nNaN\n-1.000000\nNaN\nNaN\n0.583333\nNaN\n-0.229416\nNaN\n0.765641\n\n\n3\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4\n0.391797\nNaN\nNaN\n1.000000\n-0.394823\n0.421927\n0.704669\n0.055442\nNaN\n0.360399\n...\n-0.239325\n0.562500\n0.162301\n-0.158114\n0.905134\n0.021898\n-0.020659\n-0.286872\nNaN\n-0.050868\n\n\n5\n0.180151\nNaN\nNaN\n-0.394823\n1.000000\n-0.006888\n0.328889\n0.030168\nNaN\n-0.777714\n...\n0.000000\n0.231642\n0.131108\n0.068621\n-0.245026\n0.377341\n0.228218\n0.263139\n0.384111\n0.040582\n\n\n6\n-0.439941\nNaN\nNaN\n0.421927\n-0.006888\n1.000000\n0.000000\n-0.127385\nNaN\n0.957427\n...\n-0.292770\n-0.030599\n-0.123983\n-0.176327\n0.063861\n-0.468008\n0.541386\n-0.337129\n0.158255\n-0.030567\n\n\n\n\n5 rows × 597 columns\n\n\n\n\nEn la matriz de similitud del usuario, los valores varían de -1 a 1, donde -1 significa la preferencia de película opuesta y 1 significa la misma preferencia de película.\nn = 10 significa que nos gustaría elegir los 10 usuarios más similares para el ID de usuario 1.\nEl filtrado colaborativo basado en usuarios hace recomendaciones basadas en usuarios con gustos similares, por lo que debemos establecer un umbral positivo. Aquí configuramos user_similarity_threshold en 0,3, lo que significa que un usuario debe tener un coeficiente de correlación de Pearson de al menos 0,3 para ser considerado como un usuario similar.\nDespués de establecer la cantidad de usuarios similares y el umbral de similitud, clasificamos el valor de similitud del usuario del más alto al más bajo, luego imprimimos la ID de los usuarios más similares y el valor de correlación de Pearson.\n\n# Number of similar users\nn = 10\n\n# User similarity threashold\nuser_similarity_threshold = 0.3\n\n# Get top n similar users\nsimilar_users = user_similarity[user_similarity[picked_userid]&gt;user_similarity_threshold][picked_userid].sort_values(ascending=False)[:n]\n\n# Print out top n similar users\nprint(f'The similar users for user {picked_userid} are', similar_users)\n\nThe similar users for user 1 are userId\n108    1.000000\n9      1.000000\n550    1.000000\n598    1.000000\n502    1.000000\n401    0.942809\n511    0.925820\n366    0.872872\n154    0.866025\n595    0.866025\nName: 1, dtype: float64"
  },
  {
    "objectID": "posts/2022/2022-10-12-implicit.html#restringir-el-grupo-de-artículos",
    "href": "posts/2022/2022-10-12-implicit.html#restringir-el-grupo-de-artículos",
    "title": "Collaborative Filtering",
    "section": "",
    "text": "Reduciremos el grupo de artículos haciendo lo siguiente:\n\nElimine las películas que ha visto el usuario de destino (ID de usuario 1 en este ejemplo).\nGuarde solo las películas que otros usuarios similares hayan visto.\n\nPara eliminar las películas vistas por el usuario objetivo, mantenemos solo la fila para userId=1 en la matriz de elementos de usuario y eliminamos los elementos con valores faltantes.\n\n# Movies that the target user has watched\npicked_userid_watched = matrix_norm[matrix_norm.index == picked_userid].dropna(axis=1, how='all')\npicked_userid_watched\n\n\n\n\n\n\n\n\ntitle\nAlien (1979)\nAmerican Beauty (1999)\nAmerican History X (1998)\nApocalypse Now (1979)\nBack to the Future (1985)\nBatman (1989)\nBig Lebowski, The (1998)\nBraveheart (1995)\nClear and Present Danger (1994)\nClerks (1994)\n...\nStar Wars: Episode IV - A New Hope (1977)\nStar Wars: Episode V - The Empire Strikes Back (1980)\nStar Wars: Episode VI - Return of the Jedi (1983)\nStargate (1994)\nTerminator, The (1984)\nToy Story (1995)\nTwister (1996)\nUsual Suspects, The (1995)\nWilly Wonka & the Chocolate Factory (1971)\nX-Men (2000)\n\n\nuserId\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\n-0.392857\n0.607143\n0.607143\n-0.392857\n0.607143\n-0.392857\n0.607143\n-0.392857\n-0.392857\n-1.392857\n...\n0.607143\n0.607143\n0.607143\n-1.392857\n0.607143\n-0.392857\n-1.392857\n0.607143\n0.607143\n0.607143\n\n\n\n\n1 rows × 56 columns\n\n\n\n\nPara mantener solo las películas de los usuarios similares, mantenemos los ID de usuario en las 10 listas de usuarios similares principales y eliminamos la película con todos los valores faltantes. Todo valor faltante para una película significa que ninguno de los usuarios similares ha visto la película.\n\n# Movies that similar users watched. Remove movies that none of the similar users have watched\nsimilar_user_movies = matrix_norm[matrix_norm.index.isin(similar_users.index)].dropna(axis=1, how='all')\nsimilar_user_movies\n\n\n\n\n\n\n\n\ntitle\nAladdin (1992)\nAlien (1979)\nAmelie (Fabuleux destin d'Amélie Poulain, Le) (2001)\nBack to the Future (1985)\nBatman Begins (2005)\nBeautiful Mind, A (2001)\nBeauty and the Beast (1991)\nBlade Runner (1982)\nBourne Identity, The (2002)\nBraveheart (1995)\n...\nShrek (2001)\nSilence of the Lambs, The (1991)\nSpider-Man (2002)\nStar Wars: Episode I - The Phantom Menace (1999)\nTerminator 2: Judgment Day (1991)\nTitanic (1997)\nToy Story (1995)\nUp (2009)\nUsual Suspects, The (1995)\nWALL·E (2008)\n\n\nuserId\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n9\nNaN\nNaN\nNaN\n0.333333\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n108\nNaN\nNaN\n0.466667\n0.466667\nNaN\n0.466667\nNaN\n0.466667\nNaN\nNaN\n...\nNaN\nNaN\n0.466667\nNaN\nNaN\n-0.533333\nNaN\nNaN\nNaN\nNaN\n\n\n154\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n0.214286\nNaN\nNaN\n\n\n366\nNaN\nNaN\nNaN\nNaN\n-0.205882\nNaN\nNaN\nNaN\nNaN\n-0.205882\n...\nNaN\nNaN\nNaN\nNaN\n-0.205882\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n401\n-0.382353\nNaN\nNaN\nNaN\nNaN\nNaN\n-0.382353\nNaN\nNaN\nNaN\n...\n0.117647\nNaN\nNaN\nNaN\nNaN\nNaN\n0.117647\n0.617647\nNaN\n0.617647\n\n\n502\nNaN\n-0.375\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n511\nNaN\nNaN\n-0.653846\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\n-1.153846\n-0.653846\nNaN\nNaN\nNaN\n-0.153846\nNaN\nNaN\n\n\n550\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n-0.277778\n0.222222\nNaN\n-0.277778\n\n\n595\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\n-0.333333\nNaN\nNaN\nNaN\nNaN\n0.666667\nNaN\n\n\n598\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n0.888889\nNaN\n...\n-2.111111\n-2.611111\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n10 rows × 62 columns\n\n\n\n\nA continuación, eliminaremos las películas que el usuario ID 1 vio de la lista de películas de usuarios similares. errors='ignore' elimina columnas si existen sin dar un mensaje de error.\n\n# Remove the watched movie from the movie list\nsimilar_user_movies.drop(picked_userid_watched.columns,axis=1, inplace=True, errors='ignore')\n\n# Take a look at the data\nsimilar_user_movies\n\n\n\n\n\n\n\n\ntitle\nAladdin (1992)\nAmelie (Fabuleux destin d'Amélie Poulain, Le) (2001)\nBatman Begins (2005)\nBeautiful Mind, A (2001)\nBeauty and the Beast (1991)\nBlade Runner (1982)\nBourne Identity, The (2002)\nBreakfast Club, The (1985)\nCatch Me If You Can (2002)\nDark Knight, The (2008)\n...\nMonsters, Inc. (2001)\nOcean's Eleven (2001)\nPirates of the Caribbean: The Curse of the Black Pearl (2003)\nShawshank Redemption, The (1994)\nShrek (2001)\nSpider-Man (2002)\nTerminator 2: Judgment Day (1991)\nTitanic (1997)\nUp (2009)\nWALL·E (2008)\n\n\nuserId\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n9\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n108\nNaN\n0.466667\nNaN\n0.466667\nNaN\n0.466667\nNaN\n-0.533333\n0.466667\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\n0.466667\nNaN\n-0.533333\nNaN\nNaN\n\n\n154\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n0.214286\nNaN\n\n\n366\nNaN\nNaN\n-0.205882\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n-0.205882\n...\nNaN\nNaN\n-0.205882\nNaN\nNaN\nNaN\n-0.205882\nNaN\nNaN\nNaN\n\n\n401\n-0.382353\nNaN\nNaN\nNaN\n-0.382353\nNaN\nNaN\nNaN\nNaN\nNaN\n...\n0.117647\nNaN\n0.117647\nNaN\n0.117647\nNaN\nNaN\nNaN\n0.617647\n0.617647\n\n\n502\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\n0.125000\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n511\nNaN\n-0.653846\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\n0.346154\nNaN\n-1.153846\nNaN\nNaN\n-0.153846\nNaN\n\n\n550\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n-0.277778\n-0.277778\n...\nNaN\nNaN\nNaN\n0.222222\nNaN\nNaN\nNaN\nNaN\n0.222222\n-0.277778\n\n\n595\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n598\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n0.888889\nNaN\nNaN\nNaN\n...\nNaN\n0.888889\nNaN\nNaN\n-2.111111\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n10 rows × 38 columns"
  },
  {
    "objectID": "posts/2022/2022-10-12-implicit.html#recomendar-artículos",
    "href": "posts/2022/2022-10-12-implicit.html#recomendar-artículos",
    "title": "Collaborative Filtering",
    "section": "",
    "text": "Decidiremos qué película recomendar al usuario objetivo. Los elementos recomendados están determinados por el promedio ponderado del puntaje de similitud del usuario y la calificación de la película. Las calificaciones de las películas están ponderadas por las puntuaciones de similitud, por lo que los usuarios con mayor similitud obtienen una mayor ponderación.\nEste código recorre los elementos y los usuarios para obtener la puntuación del elemento, clasificar la puntuación de mayor a menor y elegir las 10 mejores películas para recomendar al ID de usuario 1.\n\n# A dictionary to store item scores\nitem_score = {}\n\n# Loop through items\nfor i in similar_user_movies.columns:\n  # Get the ratings for movie i\n  movie_rating = similar_user_movies[i]\n  # Create a variable to store the score\n  total = 0\n  # Create a variable to store the number of scores\n  count = 0\n  # Loop through similar users\n  for u in similar_users.index:\n    # If the movie has rating\n    if pd.isna(movie_rating[u]) == False:\n      # Score is the sum of user similarity score multiply by the movie rating\n      score = similar_users[u] * movie_rating[u]\n      # Add the score to the total score for the movie so far\n      total += score\n      # Add 1 to the count\n      count +=1\n  # Get the average score for the item\n  item_score[i] = total / count\n\n# Convert dictionary to pandas dataframe\nitem_score = pd.DataFrame(item_score.items(), columns=['movie', 'movie_score'])\n    \n# Sort the movies by score\nranked_item_score = item_score.sort_values(by='movie_score', ascending=False)\n\n# Select top m movies\nm = 10\nranked_item_score.head(m)\n\n\n\n\n\n\n\n\n\nmovie\nmovie_score\n\n\n\n\n16\nHarry Potter and the Chamber of Secrets (2002)\n1.888889\n\n\n13\nEternal Sunshine of the Spotless Mind (2004)\n1.888889\n\n\n6\nBourne Identity, The (2002)\n0.888889\n\n\n29\nOcean's Eleven (2001)\n0.888889\n\n\n18\nInception (2010)\n0.587491\n\n\n3\nBeautiful Mind, A (2001)\n0.466667\n\n\n5\nBlade Runner (1982)\n0.466667\n\n\n12\nDonnie Darko (2001)\n0.466667\n\n\n10\nDeparted, The (2006)\n0.256727\n\n\n31\nShawshank Redemption, The (1994)\n0.222566"
  },
  {
    "objectID": "posts/2022/2022-10-12-implicit.html#predecir-puntuaciones-opcional",
    "href": "posts/2022/2022-10-12-implicit.html#predecir-puntuaciones-opcional",
    "title": "Collaborative Filtering",
    "section": "",
    "text": "Si el objetivo es elegir los elementos recomendados, basta con tener el rango de los elementos. Sin embargo, si el objetivo es predecir la calificación del usuario, debemos sumar la calificación promedio de la película del usuario a la calificación de la película.\n\n# Average rating for the picked user\navg_rating = matrix[matrix.index == picked_userid].T.mean()[picked_userid]\n\n# Print the average movie rating for user 1\nprint(f'The average movie rating for user {picked_userid} is {avg_rating:.2f}')\n\nThe average movie rating for user 1 is 4.39\n\n\nLa calificación promedio de la película para el usuario 1 es 4.39, por lo que agregamos 4.39 nuevamente a la calificación de la película.\n\n# Calcuate the predicted rating\nranked_item_score['predicted_rating'] = ranked_item_score['movie_score'] + avg_rating\n\n# Take a look at the data\nranked_item_score.head(m)\n\n\n\n\n\n\n\n\n\nmovie\nmovie_score\npredicted_rating\n\n\n\n\n16\nHarry Potter and the Chamber of Secrets (2002)\n1.888889\n6.281746\n\n\n13\nEternal Sunshine of the Spotless Mind (2004)\n1.888889\n6.281746\n\n\n6\nBourne Identity, The (2002)\n0.888889\n5.281746\n\n\n29\nOcean's Eleven (2001)\n0.888889\n5.281746\n\n\n18\nInception (2010)\n0.587491\n4.980348\n\n\n3\nBeautiful Mind, A (2001)\n0.466667\n4.859524\n\n\n5\nBlade Runner (1982)\n0.466667\n4.859524\n\n\n12\nDonnie Darko (2001)\n0.466667\n4.859524\n\n\n10\nDeparted, The (2006)\n0.256727\n4.649584\n\n\n31\nShawshank Redemption, The (1994)\n0.222566\n4.615423\n\n\n\n\n\n\n\n\nPodemos ver que las 10 mejores películas recomendadas tienen calificaciones pronosticadas superiores a 4.5."
  },
  {
    "objectID": "posts/2022/2022-10-12-implicit.html#resumen",
    "href": "posts/2022/2022-10-12-implicit.html#resumen",
    "title": "Collaborative Filtering",
    "section": "",
    "text": "En este tutorial, analizamos cómo crear un sistema de recomendación de filtrado colaborativo basado en el usuario. Aprendiste * ¿Qué es el filtrado colaborativo basado en usuarios (usuario-usuario)? * ¿Cómo crear una matriz usuario-producto? * ¿Cómo procesar los datos para el filtrado colaborativo basado en el usuario? * ¿Cómo identificar usuarios similares? * ¿Cómo reducir el grupo de elementos? * ¿Cómo clasificar los artículos para la recomendación? * ¿Cómo predecir la puntuación de calificación?"
  },
  {
    "objectID": "posts/2022/2022-10-12-implicit.html#referencias",
    "href": "posts/2022/2022-10-12-implicit.html#referencias",
    "title": "Collaborative Filtering",
    "section": "",
    "text": "User-Based Collaborative Filtering.\nUser-Based Collaborative Filtering In Python | Machine Learning.\nCollaborative Filtering : Data Science Concepts."
  }
]